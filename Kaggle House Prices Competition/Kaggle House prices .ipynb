{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle House pricing dataset \n",
    "## trying to predict Saleprice using(Linear Regression,Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras #to make a neural net\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows',1000)\n",
    "# pd.set_option('display.max_columns',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>706</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>8</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>978</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>486</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>216</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>655</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig LandSlope Neighborhood Condition1  \\\n",
       "0         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "1         Lvl    AllPub       FR2       Gtl      Veenker      Feedr   \n",
       "2         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "3         Lvl    AllPub    Corner       Gtl      Crawfor       Norm   \n",
       "4         Lvl    AllPub       FR2       Gtl      NoRidge       Norm   \n",
       "\n",
       "  Condition2 BldgType HouseStyle  OverallQual  OverallCond  YearBuilt  \\\n",
       "0       Norm     1Fam     2Story            7            5       2003   \n",
       "1       Norm     1Fam     1Story            6            8       1976   \n",
       "2       Norm     1Fam     2Story            7            5       2001   \n",
       "3       Norm     1Fam     2Story            7            5       1915   \n",
       "4       Norm     1Fam     2Story            8            5       2000   \n",
       "\n",
       "   YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType  \\\n",
       "0          2003     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "1          1976     Gable  CompShg     MetalSd     MetalSd       None   \n",
       "2          2002     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "3          1970     Gable  CompShg     Wd Sdng     Wd Shng       None   \n",
       "4          2000     Gable  CompShg     VinylSd     VinylSd    BrkFace   \n",
       "\n",
       "   MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure  \\\n",
       "0       196.0        Gd        TA      PConc       Gd       TA           No   \n",
       "1         0.0        TA        TA     CBlock       Gd       TA           Gd   \n",
       "2       162.0        Gd        TA      PConc       Gd       TA           Mn   \n",
       "3         0.0        TA        TA     BrkTil       TA       Gd           No   \n",
       "4       350.0        Gd        TA      PConc       Gd       TA           Av   \n",
       "\n",
       "  BsmtFinType1  BsmtFinSF1 BsmtFinType2  BsmtFinSF2  BsmtUnfSF  TotalBsmtSF  \\\n",
       "0          GLQ         706          Unf           0        150          856   \n",
       "1          ALQ         978          Unf           0        284         1262   \n",
       "2          GLQ         486          Unf           0        434          920   \n",
       "3          ALQ         216          Unf           0        540          756   \n",
       "4          GLQ         655          Unf           0        490         1145   \n",
       "\n",
       "  Heating HeatingQC CentralAir Electrical  1stFlrSF  2ndFlrSF  LowQualFinSF  \\\n",
       "0    GasA        Ex          Y      SBrkr       856       854             0   \n",
       "1    GasA        Ex          Y      SBrkr      1262         0             0   \n",
       "2    GasA        Ex          Y      SBrkr       920       866             0   \n",
       "3    GasA        Gd          Y      SBrkr       961       756             0   \n",
       "4    GasA        Ex          Y      SBrkr      1145      1053             0   \n",
       "\n",
       "   GrLivArea  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  \\\n",
       "0       1710             1             0         2         1             3   \n",
       "1       1262             0             1         2         0             3   \n",
       "2       1786             1             0         2         1             3   \n",
       "3       1717             1             0         1         0             3   \n",
       "4       2198             1             0         2         1             4   \n",
       "\n",
       "   KitchenAbvGr KitchenQual  TotRmsAbvGrd Functional  Fireplaces FireplaceQu  \\\n",
       "0             1          Gd             8        Typ           0         NaN   \n",
       "1             1          TA             6        Typ           1          TA   \n",
       "2             1          Gd             6        Typ           1          TA   \n",
       "3             1          Gd             7        Typ           1          Gd   \n",
       "4             1          Gd             9        Typ           1          TA   \n",
       "\n",
       "  GarageType  GarageYrBlt GarageFinish  GarageCars  GarageArea GarageQual  \\\n",
       "0     Attchd       2003.0          RFn           2         548         TA   \n",
       "1     Attchd       1976.0          RFn           2         460         TA   \n",
       "2     Attchd       2001.0          RFn           2         608         TA   \n",
       "3     Detchd       1998.0          Unf           3         642         TA   \n",
       "4     Attchd       2000.0          RFn           3         836         TA   \n",
       "\n",
       "  GarageCond PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  \\\n",
       "0         TA          Y           0           61              0          0   \n",
       "1         TA          Y         298            0              0          0   \n",
       "2         TA          Y           0           42              0          0   \n",
       "3         TA          Y           0           35            272          0   \n",
       "4         TA          Y         192           84              0          0   \n",
       "\n",
       "   ScreenPorch  PoolArea PoolQC Fence MiscFeature  MiscVal  MoSold  YrSold  \\\n",
       "0            0         0    NaN   NaN         NaN        0       2    2008   \n",
       "1            0         0    NaN   NaN         NaN        0       5    2007   \n",
       "2            0         0    NaN   NaN         NaN        0       9    2008   \n",
       "3            0         0    NaN   NaN         NaN        0       2    2006   \n",
       "4            0         0    NaN   NaN         NaN        0      12    2008   \n",
       "\n",
       "  SaleType SaleCondition  SalePrice  \n",
       "0       WD        Normal     208500  \n",
       "1       WD        Normal     181500  \n",
       "2       WD        Normal     223500  \n",
       "3       WD       Abnorml     140000  \n",
       "4       WD        Normal     250000  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 81)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "BsmtQual           37\n",
       "BsmtCond           37\n",
       "BsmtExposure       38\n",
       "BsmtFinType1       37\n",
       "BsmtFinSF1          0\n",
       "BsmtFinType2       38\n",
       "BsmtFinSF2          0\n",
       "BsmtUnfSF           0\n",
       "TotalBsmtSF         0\n",
       "Heating             0\n",
       "HeatingQC           0\n",
       "CentralAir          0\n",
       "Electrical          1\n",
       "1stFlrSF            0\n",
       "2ndFlrSF            0\n",
       "LowQualFinSF        0\n",
       "GrLivArea           0\n",
       "BsmtFullBath        0\n",
       "BsmtHalfBath        0\n",
       "FullBath            0\n",
       "HalfBath            0\n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "SalePrice           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first we will see if there is any nan values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean our data\n",
    "df['LotFrontage']=df['LotFrontage'].fillna(df['LotFrontage'].mean())\n",
    "df.drop(['Alley'],axis=1,inplace=True)\n",
    "df['BsmtCond']=df['BsmtCond'].fillna(df['BsmtCond'].mode()[0])\n",
    "df['BsmtQual']=df['BsmtQual'].fillna(df['BsmtQual'].mode()[0])\n",
    "df['FireplaceQu']=df['FireplaceQu'].fillna(df['FireplaceQu'].mode()[0])\n",
    "df['GarageType']=df['GarageType'].fillna(df['GarageType'].mode()[0])\n",
    "df.drop(['GarageYrBlt'],axis=1,inplace=True)\n",
    "df['GarageFinish']=df['GarageFinish'].fillna(df['GarageFinish'].mode()[0])\n",
    "df['GarageQual']=df['GarageQual'].fillna(df['GarageQual'].mode()[0])\n",
    "df['GarageCond']=df['GarageCond'].fillna(df['GarageCond'].mode()[0])\n",
    "df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)\n",
    "df.drop(['Id'],axis=1,inplace=True)\n",
    "df['MasVnrType']=df['MasVnrType'].fillna(df['MasVnrType'].mode()[0])\n",
    "df['MasVnrArea']=df['MasVnrArea'].fillna(df['MasVnrArea'].mode()[0])\n",
    "df['BsmtExposure']=df['BsmtExposure'].fillna(df['BsmtExposure'].mode()[0])\n",
    "df['BsmtFinType2']=df['BsmtFinType2'].fillna(df['BsmtFinType2'].mode()[0])\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 75)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 80)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import test data and do the same \n",
    "test_df=pd.read_csv('test.csv')\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            4\n",
       "LotFrontage       227\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1352\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           2\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         1\n",
       "Exterior2nd         1\n",
       "MasVnrType         16\n",
       "MasVnrArea         15\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "BsmtQual           44\n",
       "BsmtCond           45\n",
       "BsmtExposure       44\n",
       "BsmtFinType1       42\n",
       "BsmtFinSF1          1\n",
       "BsmtFinType2       42\n",
       "BsmtFinSF2          1\n",
       "BsmtUnfSF           1\n",
       "TotalBsmtSF         1\n",
       "Heating             0\n",
       "HeatingQC           0\n",
       "CentralAir          0\n",
       "Electrical          0\n",
       "1stFlrSF            0\n",
       "2ndFlrSF            0\n",
       "LowQualFinSF        0\n",
       "GrLivArea           0\n",
       "BsmtFullBath        2\n",
       "BsmtHalfBath        2\n",
       "FullBath            0\n",
       "HalfBath            0\n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         1\n",
       "TotRmsAbvGrd        0\n",
       "Functional          2\n",
       "Fireplaces          0\n",
       "FireplaceQu       730\n",
       "GarageType         76\n",
       "GarageYrBlt        78\n",
       "GarageFinish       78\n",
       "GarageCars          1\n",
       "GarageArea          1\n",
       "GarageQual         78\n",
       "GarageCond         78\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1456\n",
       "Fence            1169\n",
       "MiscFeature      1408\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            1\n",
       "SaleCondition       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check null values\n",
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill Missing Values\n",
    "\n",
    "test_df['LotFrontage']=test_df['LotFrontage'].fillna(test_df['LotFrontage'].mean())\n",
    "test_df['MSZoning']=test_df['MSZoning'].fillna(test_df['MSZoning'].mode()[0])\n",
    "test_df.drop(['Alley'],axis=1,inplace=True)\n",
    "test_df['BsmtCond']=test_df['BsmtCond'].fillna(test_df['BsmtCond'].mode()[0])\n",
    "test_df['BsmtQual']=test_df['BsmtQual'].fillna(test_df['BsmtQual'].mode()[0])\n",
    "test_df['FireplaceQu']=test_df['FireplaceQu'].fillna(test_df['FireplaceQu'].mode()[0])\n",
    "test_df['GarageType']=test_df['GarageType'].fillna(test_df['GarageType'].mode()[0])\n",
    "test_df.drop(['GarageYrBlt'],axis=1,inplace=True)\n",
    "test_df['GarageFinish']=test_df['GarageFinish'].fillna(test_df['GarageFinish'].mode()[0])\n",
    "test_df['GarageQual']=test_df['GarageQual'].fillna(test_df['GarageQual'].mode()[0])\n",
    "test_df['GarageCond']=test_df['GarageCond'].fillna(test_df['GarageCond'].mode()[0])\n",
    "\n",
    "test_df.drop(['PoolQC','Fence','MiscFeature'],axis=1,inplace=True)\n",
    "test_df.drop(['Id'],axis=1,inplace=True)\n",
    "test_df['MasVnrType']=test_df['MasVnrType'].fillna(test_df['MasVnrType'].mode()[0])\n",
    "test_df['MasVnrArea']=test_df['MasVnrArea'].fillna(test_df['MasVnrArea'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f505e67c8>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#check nan values using heatmap from seaborn\n",
    "sns.heatmap(test_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['BsmtExposure']=test_df['BsmtExposure'].fillna(test_df['BsmtExposure'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['BsmtFinType2']=test_df['BsmtFinType2'].fillna(test_df['BsmtFinType2'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Utilities']=test_df['Utilities'].fillna(test_df['Utilities'].mode()[0])\n",
    "test_df['Exterior1st']=test_df['Exterior1st'].fillna(test_df['Exterior1st'].mode()[0])\n",
    "test_df['Exterior2nd']=test_df['Exterior2nd'].fillna(test_df['Exterior2nd'].mode()[0])\n",
    "test_df['BsmtFinType1']=test_df['BsmtFinType1'].fillna(test_df['BsmtFinType1'].mode()[0])\n",
    "test_df['BsmtFinSF1']=test_df['BsmtFinSF1'].fillna(test_df['BsmtFinSF1'].mean())\n",
    "test_df['BsmtFinSF2']=test_df['BsmtFinSF2'].fillna(test_df['BsmtFinSF2'].mean())\n",
    "test_df['BsmtUnfSF']=test_df['BsmtUnfSF'].fillna(test_df['BsmtUnfSF'].mean())\n",
    "test_df['TotalBsmtSF']=test_df['TotalBsmtSF'].fillna(test_df['TotalBsmtSF'].mean())\n",
    "test_df['BsmtFullBath']=test_df['BsmtFullBath'].fillna(test_df['BsmtFullBath'].mode()[0])\n",
    "test_df['BsmtHalfBath']=test_df['BsmtHalfBath'].fillna(test_df['BsmtHalfBath'].mode()[0])\n",
    "test_df['KitchenQual']=test_df['KitchenQual'].fillna(test_df['KitchenQual'].mode()[0])\n",
    "test_df['Functional']=test_df['Functional'].fillna(test_df['Functional'].mode()[0])\n",
    "test_df['GarageCars']=test_df['GarageCars'].fillna(test_df['GarageCars'].mean())\n",
    "test_df['GarageArea']=test_df['GarageArea'].fillna(test_df['GarageArea'].mean())\n",
    "test_df['SaleType']=test_df['SaleType'].fillna(test_df['SaleType'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 74)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we want to deal with the categorical data\n",
    "columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',\n",
    "         'Condition2','BldgType','Condition1','HouseStyle','SaleType',\n",
    "        'SaleCondition','ExterCond',\n",
    "         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n",
    "        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',\n",
    "         'CentralAir',\n",
    "         'Electrical','KitchenQual','Functional',\n",
    "         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_onehot_multcols(multcolumns):\n",
    "    df_final=final_df\n",
    "    i=0\n",
    "    for fields in multcolumns:\n",
    "        \n",
    "        print(fields)\n",
    "        df1=pd.get_dummies(final_df[fields],drop_first=True)\n",
    "        \n",
    "        final_df.drop([fields],axis=1,inplace=True)\n",
    "        if i==0:\n",
    "            df_final=df1.copy()\n",
    "        else:\n",
    "            \n",
    "            df_final=pd.concat([df_final,df1],axis=1)\n",
    "        i=i+1\n",
    "       \n",
    "        \n",
    "    df_final=pd.concat([final_df,df_final],axis=1)\n",
    "        \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anacondaaa\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#lets concatenate both test and train data\n",
    "final_df=pd.concat([df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 75)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning\n",
      "Street\n",
      "LotShape\n",
      "LandContour\n",
      "Utilities\n",
      "LotConfig\n",
      "LandSlope\n",
      "Neighborhood\n",
      "Condition2\n",
      "BldgType\n",
      "Condition1\n",
      "HouseStyle\n",
      "SaleType\n",
      "SaleCondition\n",
      "ExterCond\n",
      "ExterQual\n",
      "Foundation\n",
      "BsmtQual\n",
      "BsmtCond\n",
      "BsmtExposure\n",
      "BsmtFinType1\n",
      "BsmtFinType2\n",
      "RoofStyle\n",
      "RoofMatl\n",
      "Exterior1st\n",
      "Exterior2nd\n",
      "MasVnrType\n",
      "Heating\n",
      "HeatingQC\n",
      "CentralAir\n",
      "Electrical\n",
      "KitchenQual\n",
      "Functional\n",
      "FireplaceQu\n",
      "GarageType\n",
      "GarageFinish\n",
      "GarageQual\n",
      "GarageCond\n",
      "PavedDrive\n"
     ]
    }
   ],
   "source": [
    "final_df=category_onehot_multcols(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 235)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df =final_df.loc[:,~final_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2881, 175)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train=final_df.iloc[:1422,:]\n",
    "df_Test=final_df.iloc[1422:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>FV</th>\n",
       "      <th>RH</th>\n",
       "      <th>RL</th>\n",
       "      <th>RM</th>\n",
       "      <th>Pave</th>\n",
       "      <th>IR2</th>\n",
       "      <th>IR3</th>\n",
       "      <th>Reg</th>\n",
       "      <th>HLS</th>\n",
       "      <th>Low</th>\n",
       "      <th>Lvl</th>\n",
       "      <th>NoSeWa</th>\n",
       "      <th>CulDSac</th>\n",
       "      <th>FR2</th>\n",
       "      <th>FR3</th>\n",
       "      <th>Inside</th>\n",
       "      <th>Mod</th>\n",
       "      <th>Sev</th>\n",
       "      <th>Blueste</th>\n",
       "      <th>BrDale</th>\n",
       "      <th>BrkSide</th>\n",
       "      <th>ClearCr</th>\n",
       "      <th>CollgCr</th>\n",
       "      <th>Crawfor</th>\n",
       "      <th>Edwards</th>\n",
       "      <th>Gilbert</th>\n",
       "      <th>IDOTRR</th>\n",
       "      <th>MeadowV</th>\n",
       "      <th>Mitchel</th>\n",
       "      <th>NAmes</th>\n",
       "      <th>NPkVill</th>\n",
       "      <th>NWAmes</th>\n",
       "      <th>NoRidge</th>\n",
       "      <th>NridgHt</th>\n",
       "      <th>OldTown</th>\n",
       "      <th>SWISU</th>\n",
       "      <th>Sawyer</th>\n",
       "      <th>SawyerW</th>\n",
       "      <th>Somerst</th>\n",
       "      <th>StoneBr</th>\n",
       "      <th>Timber</th>\n",
       "      <th>Veenker</th>\n",
       "      <th>Feedr</th>\n",
       "      <th>Norm</th>\n",
       "      <th>PosA</th>\n",
       "      <th>PosN</th>\n",
       "      <th>RRAe</th>\n",
       "      <th>RRAn</th>\n",
       "      <th>RRNn</th>\n",
       "      <th>2fmCon</th>\n",
       "      <th>Duplex</th>\n",
       "      <th>Twnhs</th>\n",
       "      <th>TwnhsE</th>\n",
       "      <th>RRNe</th>\n",
       "      <th>1.5Unf</th>\n",
       "      <th>1Story</th>\n",
       "      <th>2.5Fin</th>\n",
       "      <th>2.5Unf</th>\n",
       "      <th>2Story</th>\n",
       "      <th>SFoyer</th>\n",
       "      <th>SLvl</th>\n",
       "      <th>CWD</th>\n",
       "      <th>Con</th>\n",
       "      <th>ConLD</th>\n",
       "      <th>ConLI</th>\n",
       "      <th>ConLw</th>\n",
       "      <th>New</th>\n",
       "      <th>Oth</th>\n",
       "      <th>WD</th>\n",
       "      <th>AdjLand</th>\n",
       "      <th>Alloca</th>\n",
       "      <th>Family</th>\n",
       "      <th>Normal</th>\n",
       "      <th>Partial</th>\n",
       "      <th>Fa</th>\n",
       "      <th>Gd</th>\n",
       "      <th>Po</th>\n",
       "      <th>TA</th>\n",
       "      <th>CBlock</th>\n",
       "      <th>PConc</th>\n",
       "      <th>Slab</th>\n",
       "      <th>Stone</th>\n",
       "      <th>Wood</th>\n",
       "      <th>Mn</th>\n",
       "      <th>No</th>\n",
       "      <th>BLQ</th>\n",
       "      <th>GLQ</th>\n",
       "      <th>LwQ</th>\n",
       "      <th>Rec</th>\n",
       "      <th>Unf</th>\n",
       "      <th>Gable</th>\n",
       "      <th>Gambrel</th>\n",
       "      <th>Hip</th>\n",
       "      <th>Mansard</th>\n",
       "      <th>Shed</th>\n",
       "      <th>CompShg</th>\n",
       "      <th>Membran</th>\n",
       "      <th>Metal</th>\n",
       "      <th>Roll</th>\n",
       "      <th>Tar&amp;Grv</th>\n",
       "      <th>WdShake</th>\n",
       "      <th>WdShngl</th>\n",
       "      <th>AsphShn</th>\n",
       "      <th>BrkComm</th>\n",
       "      <th>BrkFace</th>\n",
       "      <th>CemntBd</th>\n",
       "      <th>HdBoard</th>\n",
       "      <th>ImStucc</th>\n",
       "      <th>MetalSd</th>\n",
       "      <th>Plywood</th>\n",
       "      <th>Stucco</th>\n",
       "      <th>VinylSd</th>\n",
       "      <th>Wd Sdng</th>\n",
       "      <th>WdShing</th>\n",
       "      <th>Brk Cmn</th>\n",
       "      <th>CmentBd</th>\n",
       "      <th>Other</th>\n",
       "      <th>Wd Shng</th>\n",
       "      <th>None</th>\n",
       "      <th>GasW</th>\n",
       "      <th>Grav</th>\n",
       "      <th>OthW</th>\n",
       "      <th>Wall</th>\n",
       "      <th>Y</th>\n",
       "      <th>FuseF</th>\n",
       "      <th>FuseP</th>\n",
       "      <th>Mix</th>\n",
       "      <th>SBrkr</th>\n",
       "      <th>Maj2</th>\n",
       "      <th>Min1</th>\n",
       "      <th>Min2</th>\n",
       "      <th>Typ</th>\n",
       "      <th>Attchd</th>\n",
       "      <th>Basment</th>\n",
       "      <th>BuiltIn</th>\n",
       "      <th>CarPort</th>\n",
       "      <th>Detchd</th>\n",
       "      <th>RFn</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>468.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>730.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11622</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>882.0</td>\n",
       "      <td>140</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1329</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>923.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>406.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>312.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1329</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14267</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>108.0</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>393</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>928</td>\n",
       "      <td>701</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>791.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>482.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1629</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13830</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>928.0</td>\n",
       "      <td>212</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>926</td>\n",
       "      <td>678</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>602.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>470.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1604</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9978</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>926.0</td>\n",
       "      <td>360</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>506.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1280</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5005</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144</td>\n",
       "      <td>5</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n",
       "0       896         0          0             2       468.0       144.0   \n",
       "1      1329         0          0             3       923.0         0.0   \n",
       "2       928       701          0             3       791.0         0.0   \n",
       "3       926       678          0             3       602.0         0.0   \n",
       "4      1280         0          0             2       263.0         0.0   \n",
       "\n",
       "   BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  Fireplaces  FullBath  \\\n",
       "0           0.0           0.0      270.0              0           0         1   \n",
       "1           0.0           0.0      406.0              0           0         1   \n",
       "2           0.0           0.0      137.0              0           1         2   \n",
       "3           0.0           0.0      324.0              0           1         2   \n",
       "4           0.0           0.0     1017.0              0           0         2   \n",
       "\n",
       "   GarageArea  GarageCars  GrLivArea  HalfBath  KitchenAbvGr  LotArea  \\\n",
       "0       730.0         1.0        896         0             1    11622   \n",
       "1       312.0         1.0       1329         1             1    14267   \n",
       "2       482.0         2.0       1629         1             1    13830   \n",
       "3       470.0         2.0       1604         1             1     9978   \n",
       "4       506.0         2.0       1280         0             1     5005   \n",
       "\n",
       "   LotFrontage  LowQualFinSF  MSSubClass  MasVnrArea  MiscVal  MoSold  \\\n",
       "0         80.0             0          20         0.0        0       6   \n",
       "1         81.0             0          20       108.0    12500       6   \n",
       "2         74.0             0          60         0.0        0       3   \n",
       "3         78.0             0          60        20.0        0       6   \n",
       "4         43.0             0         120         0.0        0       1   \n",
       "\n",
       "   OpenPorchSF  OverallCond  OverallQual  PoolArea  SalePrice  ScreenPorch  \\\n",
       "0            0            6            5         0        NaN          120   \n",
       "1           36            6            6         0        NaN            0   \n",
       "2           34            5            5         0        NaN            0   \n",
       "3           36            6            6         0        NaN            0   \n",
       "4           82            5            8         0        NaN          144   \n",
       "\n",
       "   TotRmsAbvGrd  TotalBsmtSF  WoodDeckSF  YearBuilt  YearRemodAdd  YrSold  FV  \\\n",
       "0             5        882.0         140       1961          1961    2010   0   \n",
       "1             6       1329.0         393       1958          1958    2010   0   \n",
       "2             6        928.0         212       1997          1998    2010   0   \n",
       "3             7        926.0         360       1998          1998    2010   0   \n",
       "4             5       1280.0           0       1992          1992    2010   0   \n",
       "\n",
       "   RH  RL  RM  Pave  IR2  IR3  Reg  HLS  Low  Lvl  NoSeWa  CulDSac  FR2  FR3  \\\n",
       "0   1   0   0     1    0    0    1    0    0    1       0        0    0    0   \n",
       "1   0   1   0     1    0    0    0    0    0    1       0        0    0    0   \n",
       "2   0   1   0     1    0    0    0    0    0    1       0        0    0    0   \n",
       "3   0   1   0     1    0    0    0    0    0    1       0        0    0    0   \n",
       "4   0   1   0     1    0    0    0    1    0    0       0        0    0    0   \n",
       "\n",
       "   Inside  Mod  Sev  Blueste  BrDale  BrkSide  ClearCr  CollgCr  Crawfor  \\\n",
       "0       1    0    0        0       0        0        0        0        0   \n",
       "1       0    0    0        0       0        0        0        0        0   \n",
       "2       1    0    0        0       0        0        0        0        0   \n",
       "3       1    0    0        0       0        0        0        0        0   \n",
       "4       1    0    0        0       0        0        0        0        0   \n",
       "\n",
       "   Edwards  Gilbert  IDOTRR  MeadowV  Mitchel  NAmes  NPkVill  NWAmes  \\\n",
       "0        0        0       0        0        0      1        0       0   \n",
       "1        0        0       0        0        0      1        0       0   \n",
       "2        0        1       0        0        0      0        0       0   \n",
       "3        0        1       0        0        0      0        0       0   \n",
       "4        0        0       0        0        0      0        0       0   \n",
       "\n",
       "   NoRidge  NridgHt  OldTown  SWISU  Sawyer  SawyerW  Somerst  StoneBr  \\\n",
       "0        0        0        0      0       0        0        0        0   \n",
       "1        0        0        0      0       0        0        0        0   \n",
       "2        0        0        0      0       0        0        0        0   \n",
       "3        0        0        0      0       0        0        0        0   \n",
       "4        0        0        0      0       0        0        0        1   \n",
       "\n",
       "   Timber  Veenker  Feedr  Norm  PosA  PosN  RRAe  RRAn  RRNn  2fmCon  Duplex  \\\n",
       "0       0        0      0     1     0     0     0     0     0       0       0   \n",
       "1       0        0      0     1     0     0     0     0     0       0       0   \n",
       "2       0        0      0     1     0     0     0     0     0       0       0   \n",
       "3       0        0      0     1     0     0     0     0     0       0       0   \n",
       "4       0        0      0     1     0     0     0     0     0       0       0   \n",
       "\n",
       "   Twnhs  TwnhsE  RRNe  1.5Unf  1Story  2.5Fin  2.5Unf  2Story  SFoyer  SLvl  \\\n",
       "0      0       0     0       0       1       0       0       0       0     0   \n",
       "1      0       0     0       0       1       0       0       0       0     0   \n",
       "2      0       0     0       0       0       0       0       1       0     0   \n",
       "3      0       0     0       0       0       0       0       1       0     0   \n",
       "4      0       1     0       0       1       0       0       0       0     0   \n",
       "\n",
       "   CWD  Con  ConLD  ConLI  ConLw  New  Oth  WD  AdjLand  Alloca  Family  \\\n",
       "0    0    0      0      0      0    0    0   1        0       0       0   \n",
       "1    0    0      0      0      0    0    0   1        0       0       0   \n",
       "2    0    0      0      0      0    0    0   1        0       0       0   \n",
       "3    0    0      0      0      0    0    0   1        0       0       0   \n",
       "4    0    0      0      0      0    0    0   1        0       0       0   \n",
       "\n",
       "   Normal  Partial  Fa  Gd  Po  TA  CBlock  PConc  Slab  Stone  Wood  Mn  No  \\\n",
       "0       1        0   0   0   0   1       1      0     0      0     0   0   1   \n",
       "1       1        0   0   0   0   1       1      0     0      0     0   0   1   \n",
       "2       1        0   0   0   0   1       0      1     0      0     0   0   1   \n",
       "3       1        0   0   0   0   1       0      1     0      0     0   0   1   \n",
       "4       1        0   0   0   0   1       0      1     0      0     0   0   1   \n",
       "\n",
       "   BLQ  GLQ  LwQ  Rec  Unf  Gable  Gambrel  Hip  Mansard  Shed  CompShg  \\\n",
       "0    0    0    0    1    0      1        0    0        0     0        1   \n",
       "1    0    0    0    0    0      0        0    1        0     0        1   \n",
       "2    0    1    0    0    0      1        0    0        0     0        1   \n",
       "3    0    1    0    0    0      1        0    0        0     0        1   \n",
       "4    0    0    0    0    0      1        0    0        0     0        1   \n",
       "\n",
       "   Membran  Metal  Roll  Tar&Grv  WdShake  WdShngl  AsphShn  BrkComm  BrkFace  \\\n",
       "0        0      0     0        0        0        0        0        0        0   \n",
       "1        0      0     0        0        0        0        0        0        0   \n",
       "2        0      0     0        0        0        0        0        0        0   \n",
       "3        0      0     0        0        0        0        0        0        0   \n",
       "4        0      0     0        0        0        0        0        0        0   \n",
       "\n",
       "   CemntBd  HdBoard  ImStucc  MetalSd  Plywood  Stucco  VinylSd  Wd Sdng  \\\n",
       "0        0        0        0        0        0       0        1        0   \n",
       "1        0        0        0        0        0       0        0        1   \n",
       "2        0        0        0        0        0       0        1        0   \n",
       "3        0        0        0        0        0       0        1        0   \n",
       "4        0        1        0        0        0       0        0        0   \n",
       "\n",
       "   WdShing  Brk Cmn  CmentBd  Other  Wd Shng  None  GasW  Grav  OthW  Wall  Y  \\\n",
       "0        0        0        0      0        0     1     0     0     0     0  1   \n",
       "1        0        0        0      0        0     0     0     0     0     0  1   \n",
       "2        0        0        0      0        0     1     0     0     0     0  1   \n",
       "3        0        0        0      0        0     0     0     0     0     0  1   \n",
       "4        0        0        0      0        0     1     0     0     0     0  1   \n",
       "\n",
       "   FuseF  FuseP  Mix  SBrkr  Maj2  Min1  Min2  Typ  Attchd  Basment  BuiltIn  \\\n",
       "0      0      0    0      1     0     0     0    1       1        0        0   \n",
       "1      0      0    0      1     0     0     0    1       1        0        0   \n",
       "2      0      0    0      1     0     0     0    1       1        0        0   \n",
       "3      0      0    0      1     0     0     0    1       1        0        0   \n",
       "4      0      0    0      1     0     0     0    1       1        0        0   \n",
       "\n",
       "   CarPort  Detchd  RFn  P  \n",
       "0        0       0    0  0  \n",
       "1        0       0    0  0  \n",
       "2        0       0    0  0  \n",
       "3        0       0    0  0  \n",
       "4        0       0    1  0  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 175)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 175)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anacondaaa\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "df_Test.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df_Train.drop(['SalePrice'],axis=1)\n",
    "y_train=df_Train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First we will use Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use the Linear regression \n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LinearRegression()\n",
    "LR.fit(X_train,y_train)\n",
    "y_pred = LR.predict(df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128846.19384762, 169927.33528765, 184874.85549199, ...,\n",
       "       175475.61559544, 105014.76685818, 239800.93266636])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 174)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's try using Neural Network(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will use keras to make a Neural Network \n",
    "model = Sequential()\n",
    "model.add(Dense(174, activation='relu', input_shape=(174,)))\n",
    "model.add(keras.layers.Dense(32, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(optimizer='Adam',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anacondaaa\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1137 samples, validate on 285 samples\n",
      "Epoch 1/2000\n",
      "1137/1137 [==============================] - 0s 256us/step - loss: 35613509437.4670 - val_loss: 25508634480.2807\n",
      "Epoch 2/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 15207578175.9437 - val_loss: 6639646684.0702\n",
      "Epoch 3/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 8506450993.9842 - val_loss: 5379488291.9298\n",
      "Epoch 4/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 5371276583.8522 - val_loss: 4583253629.7544\n",
      "Epoch 5/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 4262246302.9587 - val_loss: 4310864213.3333\n",
      "Epoch 6/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 3728208905.6816 - val_loss: 4358513425.9649\n",
      "Epoch 7/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 3397703305.1187 - val_loss: 3938915072.0000\n",
      "Epoch 8/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 3032469605.0941 - val_loss: 3768562131.0877\n",
      "Epoch 9/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 2794031861.6429 - val_loss: 3677027628.9123\n",
      "Epoch 10/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 2538508934.3043 - val_loss: 3450222068.7719\n",
      "Epoch 11/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 2309436886.0088 - val_loss: 3309028410.3860\n",
      "Epoch 12/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 2091454655.0431 - val_loss: 3444568861.1930\n",
      "Epoch 13/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 1977073877.5585 - val_loss: 3191234537.5439\n",
      "Epoch 14/2000\n",
      "1137/1137 [==============================] - 0s 105us/step - loss: 1874940414.3676 - val_loss: 3255957027.9298\n",
      "Epoch 15/2000\n",
      "1137/1137 [==============================] - 0s 102us/step - loss: 1700247057.7872 - val_loss: 3407487038.8772\n",
      "Epoch 16/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 1670323678.9024 - val_loss: 3375235952.2807\n",
      "Epoch 17/2000\n",
      "1137/1137 [==============================] - 0s 100us/step - loss: 1630555291.6939 - val_loss: 3453173845.3333\n",
      "Epoch 18/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 1596592982.2339 - val_loss: 3534878807.5789\n",
      "Epoch 19/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 1592638525.4670 - val_loss: 3475582962.5263\n",
      "Epoch 20/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 1746080010.2445 - val_loss: 3433907518.8772\n",
      "Epoch 21/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 1646541768.9499 - val_loss: 3773330227.6491\n",
      "Epoch 22/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 1614611974.8672 - val_loss: 3454434286.0351\n",
      "Epoch 23/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 1573340593.9842 - val_loss: 3492087605.8947\n",
      "Epoch 24/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 1575757282.0545 - val_loss: 3465245639.8596\n",
      "Epoch 25/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 1573324428.2709 - val_loss: 3705139204.4912\n",
      "Epoch 26/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1627568166.6139 - val_loss: 3521851196.6316\n",
      "Epoch 27/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1568616637.0167 - val_loss: 3412069124.4912\n",
      "Epoch 28/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 1546247420.7916 - val_loss: 3422235021.4737\n",
      "Epoch 29/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 1560614638.4380 - val_loss: 3656737574.1754\n",
      "Epoch 30/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 1645021404.6508 - val_loss: 3821980732.6316\n",
      "Epoch 31/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 1592294633.9912 - val_loss: 3384513059.9298\n",
      "Epoch 32/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 1528246506.2726 - val_loss: 3394174886.1754\n",
      "Epoch 33/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 1614056813.8751 - val_loss: 4383013384.9825\n",
      "Epoch 34/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 1544205274.9622 - val_loss: 3659125685.8947\n",
      "Epoch 35/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 1577264006.0792 - val_loss: 3367130376.9825\n",
      "Epoch 36/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 1525633717.2489 - val_loss: 3429897162.1053\n",
      "Epoch 37/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 1498201960.0211 - val_loss: 3569765384.9825\n",
      "Epoch 38/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 1537207676.3975 - val_loss: 3420433567.4386\n",
      "Epoch 39/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 1527898474.2726 - val_loss: 3542437353.5439\n",
      "Epoch 40/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 1510014838.0369 - val_loss: 3377365712.8421\n",
      "Epoch 41/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 1500676826.2867 - val_loss: 3412050670.0351\n",
      "Epoch 42/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 1520474634.1319 - val_loss: 3512826692.4912\n",
      "Epoch 43/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 1496893018.9622 - val_loss: 3264399519.4386\n",
      "Epoch 44/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 1480779468.8338 - val_loss: 3293684356.4912\n",
      "Epoch 45/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 1603521571.8558 - val_loss: 3575642762.1053\n",
      "Epoch 46/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 1475180679.2049 - val_loss: 3460792270.5965\n",
      "Epoch 47/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 1496717455.7045 - val_loss: 3278558844.6316\n",
      "Epoch 48/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 1446004754.9129 - val_loss: 3571790587.5088\n",
      "Epoch 49/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 1449019085.6781 - val_loss: 3215884687.7193\n",
      "Epoch 50/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 1466148511.5778 - val_loss: 3293065786.3860\n",
      "Epoch 51/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 1531299206.5295 - val_loss: 3264706412.9123\n",
      "Epoch 52/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 1523067696.4644 - val_loss: 3194377848.1404\n",
      "Epoch 53/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 1458495386.7933 - val_loss: 3298928298.6667\n",
      "Epoch 54/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 1400185799.0361 - val_loss: 3242563676.0702\n",
      "Epoch 55/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 1452655644.4820 - val_loss: 3360041889.6842\n",
      "Epoch 56/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 1441798431.5778 - val_loss: 3164089548.3509\n",
      "Epoch 57/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 1450592163.9683 - val_loss: 3545352892.6316\n",
      "Epoch 58/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 1436826920.0774 - val_loss: 3277707468.3509\n",
      "Epoch 59/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 1389620800.5066 - val_loss: 3166700133.0526\n",
      "Epoch 60/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 1395071529.9912 - val_loss: 3130694411.2281\n",
      "Epoch 61/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 1396300621.3967 - val_loss: 3085840770.2456\n",
      "Epoch 62/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 1430193369.6113 - val_loss: 3024289118.3158\n",
      "Epoch 63/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 1363321086.9868 - val_loss: 3161423785.5439\n",
      "Epoch 64/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 1383131437.4811 - val_loss: 3056072293.0526\n",
      "Epoch 65/2000\n",
      "1137/1137 [==============================] - 0s 64us/step - loss: 1383786195.1944 - val_loss: 3068411535.7193\n",
      "Epoch 66/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 63us/step - loss: 1465755483.6939 - val_loss: 3543583435.2281\n",
      "Epoch 67/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1403590685.7203 - val_loss: 3649820317.1930\n",
      "Epoch 68/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 1392418256.9991 - val_loss: 3045954007.5789\n",
      "Epoch 69/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 1361950016.7880 - val_loss: 3064125744.2807\n",
      "Epoch 70/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 1354584648.0493 - val_loss: 3186801032.9825\n",
      "Epoch 71/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 1509335575.7537 - val_loss: 2866069643.2281\n",
      "Epoch 72/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1426928159.1838 - val_loss: 3039122993.4035\n",
      "Epoch 73/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 1394532138.5541 - val_loss: 3064134551.5789\n",
      "Epoch 74/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 1326903459.0114 - val_loss: 2898344842.1053\n",
      "Epoch 75/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 1332224374.2058 - val_loss: 2975581008.8421\n",
      "Epoch 76/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1361306783.8593 - val_loss: 3133580539.5088\n",
      "Epoch 77/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 1476341169.0836 - val_loss: 3620526679.5789\n",
      "Epoch 78/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 1666206058.4978 - val_loss: 2872641169.9649\n",
      "Epoch 79/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1448072345.5550 - val_loss: 3152242361.2632\n",
      "Epoch 80/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 1302528754.3782 - val_loss: 3003386671.1579\n",
      "Epoch 81/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 1311602168.9077 - val_loss: 2870474053.6140\n",
      "Epoch 82/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 1395311605.9807 - val_loss: 3391950329.2632\n",
      "Epoch 83/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1327446106.7933 - val_loss: 2915703415.0175\n",
      "Epoch 84/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1326513064.9217 - val_loss: 3035320989.1930\n",
      "Epoch 85/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1300894479.4230 - val_loss: 2959852577.6842\n",
      "Epoch 86/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1282396447.7467 - val_loss: 2898966578.5263\n",
      "Epoch 87/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1290733195.8769 - val_loss: 3063994736.2807\n",
      "Epoch 88/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1403869204.2076 - val_loss: 2812198714.3860\n",
      "Epoch 89/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 1304352317.4670 - val_loss: 3036648725.3333\n",
      "Epoch 90/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1273047095.6130 - val_loss: 2842758507.7895\n",
      "Epoch 91/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1283329289.2876 - val_loss: 2914236265.5439\n",
      "Epoch 92/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1281486850.3641 - val_loss: 2915746453.3333\n",
      "Epoch 93/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1340358504.1900 - val_loss: 3136001571.9298\n",
      "Epoch 94/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1250005648.6614 - val_loss: 2704675332.4912\n",
      "Epoch 95/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1253178763.9332 - val_loss: 2828596752.8421\n",
      "Epoch 96/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 1260989111.5567 - val_loss: 2816376525.4737\n",
      "Epoch 97/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 1259065261.3122 - val_loss: 2870870988.3509\n",
      "Epoch 98/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1300487345.4776 - val_loss: 2692418320.8421\n",
      "Epoch 99/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1334102901.7555 - val_loss: 2622193158.7368\n",
      "Epoch 100/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1232469342.2269 - val_loss: 2791573816.1404\n",
      "Epoch 101/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 1244780942.7476 - val_loss: 2679928995.9298\n",
      "Epoch 102/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 1286096629.0800 - val_loss: 2603026050.2456\n",
      "Epoch 103/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1290183551.2120 - val_loss: 3025454049.6842\n",
      "Epoch 104/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 1269084226.4767 - val_loss: 2656624317.7544\n",
      "Epoch 105/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1217903078.2762 - val_loss: 2904380165.6140\n",
      "Epoch 106/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1319449858.3078 - val_loss: 2528629298.5263\n",
      "Epoch 107/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1307835793.5620 - val_loss: 3163566318.0351\n",
      "Epoch 108/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1250834865.5339 - val_loss: 2634474668.9123\n",
      "Epoch 109/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1269083108.7564 - val_loss: 2690494961.4035\n",
      "Epoch 110/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1214366790.7546 - val_loss: 2706436160.0000\n",
      "Epoch 111/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1215555314.8285 - val_loss: 2702447856.2807\n",
      "Epoch 112/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 1227541827.9965 - val_loss: 2653739558.1754\n",
      "Epoch 113/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1265687494.9798 - val_loss: 2503598010.3860\n",
      "Epoch 114/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1197799535.2823 - val_loss: 2581137751.5789\n",
      "Epoch 115/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1174984877.9314 - val_loss: 2664451464.9825\n",
      "Epoch 116/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1220439087.7326 - val_loss: 2638993286.7368\n",
      "Epoch 117/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 1196523306.3852 - val_loss: 2612393949.1930\n",
      "Epoch 118/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1225650705.5057 - val_loss: 2481567337.5439\n",
      "Epoch 119/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1189443121.9842 - val_loss: 2586833154.2456\n",
      "Epoch 120/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1167459358.3113 - val_loss: 2450517091.9298\n",
      "Epoch 121/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1191853163.3421 - val_loss: 2728021483.7895\n",
      "Epoch 122/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1209479117.0589 - val_loss: 2460700768.5614\n",
      "Epoch 123/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1219846411.4266 - val_loss: 2615836713.5439\n",
      "Epoch 124/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1278455460.5312 - val_loss: 2433079976.4211\n",
      "Epoch 125/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1258676123.2436 - val_loss: 2621639292.6316\n",
      "Epoch 126/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1214762793.4846 - val_loss: 2502110078.8772\n",
      "Epoch 127/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1500970958.2410 - val_loss: 2718079119.7193\n",
      "Epoch 128/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 1250497882.6245 - val_loss: 2408698010.9474\n",
      "Epoch 129/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 1157533204.6579 - val_loss: 2433608458.1053\n",
      "Epoch 130/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 1167925020.5383 - val_loss: 2519418271.4386\n",
      "Epoch 131/2000\n",
      "1137/1137 [==============================] - 0s 103us/step - loss: 1179579398.9798 - val_loss: 2336388593.4035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 1193599658.6104 - val_loss: 2572864392.9825\n",
      "Epoch 133/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 1222366489.6675 - val_loss: 2436642610.5263\n",
      "Epoch 134/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 1167502175.7467 - val_loss: 2435369101.4737\n",
      "Epoch 135/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1150285068.2709 - val_loss: 2328166225.9649\n",
      "Epoch 136/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1125199518.2832 - val_loss: 2483835183.1579\n",
      "Epoch 137/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1107041057.3791 - val_loss: 2379433496.7018\n",
      "Epoch 138/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1128797887.7748 - val_loss: 2409143967.4386\n",
      "Epoch 139/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1290265225.2313 - val_loss: 2340444484.4912\n",
      "Epoch 140/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1182690141.2700 - val_loss: 2354655550.8772\n",
      "Epoch 141/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1296301427.8417 - val_loss: 2445586091.7895\n",
      "Epoch 142/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1103479632.6051 - val_loss: 2535656245.8947\n",
      "Epoch 143/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1171255995.7784 - val_loss: 2345963235.9298\n",
      "Epoch 144/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1144261209.7238 - val_loss: 2294834361.2632\n",
      "Epoch 145/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1083944431.7889 - val_loss: 2300435662.5965\n",
      "Epoch 146/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1148926450.6033 - val_loss: 2527968796.0702\n",
      "Epoch 147/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1102900669.0730 - val_loss: 2285757968.8421\n",
      "Epoch 148/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 1100151127.2471 - val_loss: 2448726651.5088\n",
      "Epoch 149/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1093505761.3791 - val_loss: 2234714581.3333\n",
      "Epoch 150/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1119398526.0862 - val_loss: 2252088723.0877\n",
      "Epoch 151/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1149906259.3632 - val_loss: 2401953329.4035\n",
      "Epoch 152/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1207141447.8804 - val_loss: 2461984243.6491\n",
      "Epoch 153/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1154346385.1680 - val_loss: 2237505018.3860\n",
      "Epoch 154/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 1062902248.3026 - val_loss: 2324097122.8070\n",
      "Epoch 155/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 1077961698.8426 - val_loss: 2312834675.6491\n",
      "Epoch 156/2000\n",
      "1137/1137 [==============================] - 0s 67us/step - loss: 1069723762.2093 - val_loss: 2311014716.6316\n",
      "Epoch 157/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 1181547836.1161 - val_loss: 2713686942.3158\n",
      "Epoch 158/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1099958844.6790 - val_loss: 2250349568.0000\n",
      "Epoch 159/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1093036637.2700 - val_loss: 2261013349.0526\n",
      "Epoch 160/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1053525224.0211 - val_loss: 2187054999.5789\n",
      "Epoch 161/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1148552286.0018 - val_loss: 2451761783.0175\n",
      "Epoch 162/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 1092942672.4925 - val_loss: 2182673349.6140\n",
      "Epoch 163/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1065836683.8206 - val_loss: 2250941754.3860\n",
      "Epoch 164/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1045910272.7318 - val_loss: 2267045632.5614\n",
      "Epoch 165/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 993737310.5646 - val_loss: 2479706451.6491\n",
      "Epoch 166/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1060434141.9736 - val_loss: 2107482984.4211\n",
      "Epoch 167/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 1080198350.0158 - val_loss: 2104314945.1228\n",
      "Epoch 168/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1144933965.7907 - val_loss: 2218149745.4035\n",
      "Epoch 169/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1137630549.6711 - val_loss: 2633564192.5614\n",
      "Epoch 170/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1179511388.9323 - val_loss: 2197370839.5789\n",
      "Epoch 171/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1051197345.8857 - val_loss: 2208507182.0351\n",
      "Epoch 172/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 1014574724.0528 - val_loss: 2101333045.8947\n",
      "Epoch 173/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1026398020.3624 - val_loss: 2099741336.7018\n",
      "Epoch 174/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 1075775814.4732 - val_loss: 2645304284.0702\n",
      "Epoch 175/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 1293466168.2885 - val_loss: 2103346056.9825\n",
      "Epoch 176/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 1128830990.9727 - val_loss: 2113588405.3333\n",
      "Epoch 177/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1006744386.9833 - val_loss: 1999354803.0877\n",
      "Epoch 178/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1020371540.5453 - val_loss: 2144907089.9649\n",
      "Epoch 179/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 1083383636.6016 - val_loss: 2114811656.9825\n",
      "Epoch 180/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 1087071108.3905 - val_loss: 2035252949.3333\n",
      "Epoch 181/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 959349707.3703 - val_loss: 2060828913.4035\n",
      "Epoch 182/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 956906443.3703 - val_loss: 1997943755.7895\n",
      "Epoch 183/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 978996939.6517 - val_loss: 1984255229.7544\n",
      "Epoch 184/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 976787999.4090 - val_loss: 2005604082.5263\n",
      "Epoch 185/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 977109214.6209 - val_loss: 2319038650.3860\n",
      "Epoch 186/2000\n",
      "1137/1137 [==============================] - 0s 67us/step - loss: 968595659.7643 - val_loss: 1973630705.4035\n",
      "Epoch 187/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 946107728.8303 - val_loss: 1968540170.1053\n",
      "Epoch 188/2000\n",
      "1137/1137 [==============================] - 0s 67us/step - loss: 916186860.8056 - val_loss: 1978369853.7544\n",
      "Epoch 189/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 975098456.5418 - val_loss: 2162306620.0702\n",
      "Epoch 190/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 964911141.6007 - val_loss: 2006539303.2982\n",
      "Epoch 191/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 916216946.6596 - val_loss: 1938739816.4211\n",
      "Epoch 192/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 916518061.6500 - val_loss: 2134423502.0351\n",
      "Epoch 193/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 955754860.8056 - val_loss: 1976441570.8070\n",
      "Epoch 194/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 1001671729.4776 - val_loss: 1994182650.3860\n",
      "Epoch 195/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 997430756.8690 - val_loss: 1891233066.1053\n",
      "Epoch 196/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 910462858.9200 - val_loss: 1974974028.3509\n",
      "Epoch 197/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 72us/step - loss: 898544807.8522 - val_loss: 1937372420.4912\n",
      "Epoch 198/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 895720872.6684 - val_loss: 1831959367.8596\n",
      "Epoch 199/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 899079161.4705 - val_loss: 1979853697.6842\n",
      "Epoch 200/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 908074929.4776 - val_loss: 1861760325.6140\n",
      "Epoch 201/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 1021331038.7335 - val_loss: 1916967305.5439\n",
      "Epoch 202/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 933030898.9974 - val_loss: 1937938464.5614\n",
      "Epoch 203/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 895527832.7669 - val_loss: 1775160602.3860\n",
      "Epoch 204/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 860605296.3237 - val_loss: 1855050674.5263\n",
      "Epoch 205/2000\n",
      "1137/1137 [==============================] - 0s 67us/step - loss: 953190237.6077 - val_loss: 1780410362.3860\n",
      "Epoch 206/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 910633857.6605 - val_loss: 1911461890.8070\n",
      "Epoch 207/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 868282462.7898 - val_loss: 1954743628.3509\n",
      "Epoch 208/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 966648876.9745 - val_loss: 1796665023.4386\n",
      "Epoch 209/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 901698481.8716 - val_loss: 1800625070.0351\n",
      "Epoch 210/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 872141985.9982 - val_loss: 1731637571.3684\n",
      "Epoch 211/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 880304848.4925 - val_loss: 1941162179.9298\n",
      "Epoch 212/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 853792793.7801 - val_loss: 1791341722.9474\n",
      "Epoch 213/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 845854531.8839 - val_loss: 1667442547.6491\n",
      "Epoch 214/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 888159092.6860 - val_loss: 1771291091.0877\n",
      "Epoch 215/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 1033550747.8628 - val_loss: 1837120669.1930\n",
      "Epoch 216/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 840364678.1917 - val_loss: 1764298391.0175\n",
      "Epoch 217/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 805291277.2841 - val_loss: 1757968128.5614\n",
      "Epoch 218/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 851665947.8628 - val_loss: 1701435486.3158\n",
      "Epoch 219/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 816223744.5629 - val_loss: 1671623020.9123\n",
      "Epoch 220/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 809885082.2586 - val_loss: 1641630149.0526\n",
      "Epoch 221/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 839058232.0915 - val_loss: 1621927621.6140\n",
      "Epoch 222/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 866433404.5101 - val_loss: 1670127997.7544\n",
      "Epoch 223/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 809768658.5752 - val_loss: 1580869913.8246\n",
      "Epoch 224/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 757840772.5031 - val_loss: 1608383761.4035\n",
      "Epoch 225/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 772595219.3632 - val_loss: 1601032241.9649\n",
      "Epoch 226/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 752233414.1917 - val_loss: 1628106181.0526\n",
      "Epoch 227/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 745578832.7458 - val_loss: 1586638071.0175\n",
      "Epoch 228/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 752307851.5110 - val_loss: 1562714130.5263\n",
      "Epoch 229/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 760845221.3193 - val_loss: 1644102331.5088\n",
      "Epoch 230/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 753075304.6684 - val_loss: 1544831322.9474\n",
      "Epoch 231/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 846173556.7986 - val_loss: 1736330493.7544\n",
      "Epoch 232/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 772342791.8241 - val_loss: 1624003099.5088\n",
      "Epoch 233/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 753302790.6983 - val_loss: 1513816879.1579\n",
      "Epoch 234/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 796595787.1451 - val_loss: 1661942617.8246\n",
      "Epoch 235/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 805010329.0484 - val_loss: 1610073835.7895\n",
      "Epoch 236/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 805840426.9481 - val_loss: 1473033556.7719\n",
      "Epoch 237/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 786505896.3870 - val_loss: 1535830574.0351\n",
      "Epoch 238/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 709186434.0545 - val_loss: 1459742224.8421\n",
      "Epoch 239/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 741628716.1302 - val_loss: 1612193799.8596\n",
      "Epoch 240/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 898836980.3483 - val_loss: 1489571590.1754\n",
      "Epoch 241/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 794332406.1495 - val_loss: 1499676829.1930\n",
      "Epoch 242/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 719184851.2507 - val_loss: 1482653428.2105\n",
      "Epoch 243/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 695309174.0932 - val_loss: 1436827288.1404\n",
      "Epoch 244/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 682699589.1785 - val_loss: 1465106372.4912\n",
      "Epoch 245/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 686707769.4142 - val_loss: 1422167975.2982\n",
      "Epoch 246/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 744455521.2946 - val_loss: 1698665046.4561\n",
      "Epoch 247/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 714482847.1557 - val_loss: 1403421436.6316\n",
      "Epoch 248/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 674589257.4846 - val_loss: 1598862493.7544\n",
      "Epoch 249/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 725688254.5365 - val_loss: 1395630784.0000\n",
      "Epoch 250/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 737277884.9604 - val_loss: 1561634831.7193\n",
      "Epoch 251/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 712004890.6245 - val_loss: 1391651947.7895\n",
      "Epoch 252/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 721254990.9727 - val_loss: 1521494368.0000\n",
      "Epoch 253/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 691224514.1953 - val_loss: 1448875363.9298\n",
      "Epoch 254/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 739629190.4732 - val_loss: 1339254121.5439\n",
      "Epoch 255/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 658603113.3720 - val_loss: 1470935765.3333\n",
      "Epoch 256/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 659300876.5242 - val_loss: 1365493164.3509\n",
      "Epoch 257/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 620076186.3430 - val_loss: 1412650305.6842\n",
      "Epoch 258/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 682648923.6376 - val_loss: 1320584191.4386\n",
      "Epoch 259/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 707415988.4609 - val_loss: 1440695424.0000\n",
      "Epoch 260/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 617363996.1442 - val_loss: 1314242348.9123\n",
      "Epoch 261/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 609116430.4380 - val_loss: 1392148396.9123\n",
      "Epoch 262/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 591457566.3958 - val_loss: 1354042491.5088\n",
      "Epoch 263/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 75us/step - loss: 584375497.8786 - val_loss: 1338491896.1404\n",
      "Epoch 264/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 625412686.6913 - val_loss: 1319488275.6491\n",
      "Epoch 265/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 593602300.5664 - val_loss: 1301890928.8421\n",
      "Epoch 266/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 602894388.9112 - val_loss: 1389724737.6842\n",
      "Epoch 267/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 582400203.8206 - val_loss: 1370878301.1930\n",
      "Epoch 268/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 633847380.7142 - val_loss: 1311203644.6316\n",
      "Epoch 269/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 585907159.7256 - val_loss: 1302248556.9123\n",
      "Epoch 270/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 568028713.1469 - val_loss: 1350042757.6140\n",
      "Epoch 271/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 657506283.0888 - val_loss: 1351275504.2807\n",
      "Epoch 272/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 614473601.3509 - val_loss: 1367482262.4561\n",
      "Epoch 273/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 579955085.6781 - val_loss: 1281864200.9825\n",
      "Epoch 274/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 531493861.7414 - val_loss: 1270043267.9298\n",
      "Epoch 275/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 564342019.8839 - val_loss: 1209570359.0175\n",
      "Epoch 276/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 609757576.1900 - val_loss: 1232209173.3333\n",
      "Epoch 277/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 557010320.8021 - val_loss: 1388225223.8596\n",
      "Epoch 278/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 564987921.4776 - val_loss: 1329876922.9474\n",
      "Epoch 279/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 586047292.7916 - val_loss: 1227644273.4035\n",
      "Epoch 280/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 544796352.7318 - val_loss: 1208532041.5439\n",
      "Epoch 281/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 593448520.3026 - val_loss: 1490214233.8246\n",
      "Epoch 282/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 715417136.4925 - val_loss: 1181251566.5965\n",
      "Epoch 283/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 621253559.3316 - val_loss: 1295152532.7719\n",
      "Epoch 284/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 591271083.9050 - val_loss: 1330259626.1053\n",
      "Epoch 285/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 568536365.6500 - val_loss: 1233389542.7368\n",
      "Epoch 286/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 542090775.7537 - val_loss: 1180199952.8421\n",
      "Epoch 287/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 510291459.0396 - val_loss: 1198828295.8596\n",
      "Epoch 288/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 512076902.0510 - val_loss: 1204757102.0351\n",
      "Epoch 289/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 545268311.3034 - val_loss: 1184122427.5088\n",
      "Epoch 290/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 521602994.4345 - val_loss: 1340175484.6316\n",
      "Epoch 291/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 512987681.9982 - val_loss: 1149747760.8421\n",
      "Epoch 292/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 510369733.3755 - val_loss: 1327996104.9825\n",
      "Epoch 293/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 636049072.0422 - val_loss: 1384130103.0175\n",
      "Epoch 294/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 585286897.9842 - val_loss: 1163216710.7368\n",
      "Epoch 295/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 498762940.0598 - val_loss: 1150593758.8772\n",
      "Epoch 296/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 496603510.5435 - val_loss: 1213161456.8421\n",
      "Epoch 297/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 477514165.5585 - val_loss: 1155956441.8246\n",
      "Epoch 298/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 506462135.6693 - val_loss: 1134570589.7544\n",
      "Epoch 299/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 481579371.6517 - val_loss: 1195820646.1754\n",
      "Epoch 300/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 511166165.4178 - val_loss: 1264318645.3333\n",
      "Epoch 301/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 523794861.7625 - val_loss: 1124353602.8070\n",
      "Epoch 302/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 536740314.2304 - val_loss: 1094836807.8596\n",
      "Epoch 303/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 479953641.2032 - val_loss: 1153985589.3333\n",
      "Epoch 304/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 456350843.8628 - val_loss: 1121393710.0351\n",
      "Epoch 305/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 452386894.8320 - val_loss: 1256994234.3860\n",
      "Epoch 306/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 460603654.7546 - val_loss: 1133099820.3509\n",
      "Epoch 307/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 508053955.3210 - val_loss: 1077668839.2982\n",
      "Epoch 308/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 616466697.2876 - val_loss: 1325070128.2807\n",
      "Epoch 309/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 521070803.0114 - val_loss: 1105025163.2281\n",
      "Epoch 310/2000\n",
      "1137/1137 [==============================] - 0s 100us/step - loss: 471873040.3940 - val_loss: 1092030627.9298\n",
      "Epoch 311/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 472789159.1205 - val_loss: 1067403253.3333\n",
      "Epoch 312/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 447430472.8373 - val_loss: 1114252152.7018\n",
      "Epoch 313/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 496181143.1909 - val_loss: 1045967755.7895\n",
      "Epoch 314/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 485544866.1671 - val_loss: 1074158849.1228\n",
      "Epoch 315/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 473092671.4934 - val_loss: 1222619959.0175\n",
      "Epoch 316/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 548927321.3580 - val_loss: 1132026479.7193\n",
      "Epoch 317/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 610190427.6939 - val_loss: 1114428069.0526\n",
      "Epoch 318/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 639064332.3272 - val_loss: 1164060305.9649\n",
      "Epoch 319/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 589382602.8355 - val_loss: 1113790282.1053\n",
      "Epoch 320/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 484243151.1697 - val_loss: 1090883240.4211\n",
      "Epoch 321/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 454121449.1469 - val_loss: 1083387358.3158\n",
      "Epoch 322/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 443153642.9763 - val_loss: 1156213786.3860\n",
      "Epoch 323/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 433653163.6517 - val_loss: 1079538626.8070\n",
      "Epoch 324/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 457646104.5136 - val_loss: 1173123067.5088\n",
      "Epoch 325/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 438502481.6746 - val_loss: 1159956679.8596\n",
      "Epoch 326/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 447739809.8575 - val_loss: 1148120294.7368\n",
      "Epoch 327/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 450697384.0493 - val_loss: 1152032133.0526\n",
      "Epoch 328/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 458633167.6763 - val_loss: 1205648634.3860\n",
      "Epoch 329/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 75us/step - loss: 495474357.1363 - val_loss: 1122079660.3509\n",
      "Epoch 330/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 489415021.7062 - val_loss: 1129738073.2632\n",
      "Epoch 331/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 473978412.5242 - val_loss: 1157513657.8246\n",
      "Epoch 332/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 472582662.9798 - val_loss: 1140701716.2105\n",
      "Epoch 333/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 465962357.6429 - val_loss: 1096268972.9123\n",
      "Epoch 334/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 412390552.1759 - val_loss: 1174253408.5614\n",
      "Epoch 335/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 427256244.2076 - val_loss: 1191686804.2105\n",
      "Epoch 336/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 435121170.2938 - val_loss: 1102669352.4211\n",
      "Epoch 337/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 397174223.0009 - val_loss: 1096701178.3860\n",
      "Epoch 338/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 396027002.8215 - val_loss: 1086503248.8421\n",
      "Epoch 339/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 434391188.8830 - val_loss: 1097038237.1930\n",
      "Epoch 340/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 395574042.9340 - val_loss: 1054654937.2632\n",
      "Epoch 341/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 416300188.6227 - val_loss: 1149063481.2632\n",
      "Epoch 342/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 495278752.5910 - val_loss: 1198104334.5965\n",
      "Epoch 343/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 400030640.1266 - val_loss: 1126362664.4211\n",
      "Epoch 344/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 450831978.8074 - val_loss: 1097223307.2281\n",
      "Epoch 345/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 443942512.4081 - val_loss: 1065149125.0526\n",
      "Epoch 346/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 413137960.5558 - val_loss: 1228599399.8596\n",
      "Epoch 347/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 393907010.4204 - val_loss: 1095091310.0351\n",
      "Epoch 348/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 392535739.8909 - val_loss: 1030128629.8947\n",
      "Epoch 349/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 410977625.7238 - val_loss: 1145592814.5965\n",
      "Epoch 350/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 424943686.4450 - val_loss: 1080626470.1754\n",
      "Epoch 351/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 412315613.2419 - val_loss: 1204508719.1579\n",
      "Epoch 352/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 384656223.2120 - val_loss: 1114961882.3860\n",
      "Epoch 353/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 418882415.1697 - val_loss: 1141226868.2105\n",
      "Epoch 354/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 457961218.5048 - val_loss: 1216495526.7368\n",
      "Epoch 355/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 469832577.6887 - val_loss: 1068222910.8772\n",
      "Epoch 356/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 421096104.5840 - val_loss: 1346335564.9123\n",
      "Epoch 357/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 467340921.2172 - val_loss: 1552021947.5088\n",
      "Epoch 358/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 502298736.1548 - val_loss: 1365233392.8421\n",
      "Epoch 359/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 429860011.4266 - val_loss: 1349622600.9825\n",
      "Epoch 360/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 498195268.9252 - val_loss: 1166716025.2632\n",
      "Epoch 361/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 412292695.5004 - val_loss: 1148653985.1228\n",
      "Epoch 362/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 393794718.5365 - val_loss: 1080871805.1930\n",
      "Epoch 363/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 376204874.5822 - val_loss: 1077276620.9123\n",
      "Epoch 364/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 399000415.0994 - val_loss: 1141108198.1754\n",
      "Epoch 365/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 400849642.9481 - val_loss: 1183870168.1404\n",
      "Epoch 366/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 473138563.2647 - val_loss: 1311936363.7895\n",
      "Epoch 367/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 508609215.7748 - val_loss: 1144447082.1053\n",
      "Epoch 368/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 438925493.3333 - val_loss: 1187109251.3684\n",
      "Epoch 369/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 370368558.8602 - val_loss: 1138249154.8070\n",
      "Epoch 370/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 373330192.7599 - val_loss: 1304894632.9825\n",
      "Epoch 371/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 401728581.9384 - val_loss: 1225758362.9474\n",
      "Epoch 372/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 420147530.1319 - val_loss: 1356330032.8421\n",
      "Epoch 373/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 399738812.7916 - val_loss: 1037669075.6491\n",
      "Epoch 374/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 370315070.4802 - val_loss: 1039838373.6140\n",
      "Epoch 375/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 356362080.5629 - val_loss: 1067837528.1404\n",
      "Epoch 376/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 404027109.4318 - val_loss: 1140702972.6316\n",
      "Epoch 377/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 426166725.3474 - val_loss: 1157766393.2632\n",
      "Epoch 378/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 493840692.1794 - val_loss: 1462187944.9825\n",
      "Epoch 379/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 553555829.8118 - val_loss: 1165742502.7368\n",
      "Epoch 380/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 410072400.8021 - val_loss: 1116377541.0526\n",
      "Epoch 381/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 364674452.2076 - val_loss: 1098573809.9649\n",
      "Epoch 382/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 344921697.2665 - val_loss: 1070616175.7193\n",
      "Epoch 383/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 377443081.7942 - val_loss: 1069554868.2105\n",
      "Epoch 384/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 427781515.3703 - val_loss: 1084358803.0877\n",
      "Epoch 385/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 372908773.0941 - val_loss: 1025782177.6842\n",
      "Epoch 386/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 358779649.1539 - val_loss: 1064496697.8246\n",
      "Epoch 387/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 353786197.3052 - val_loss: 1059424076.9123\n",
      "Epoch 388/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 376223512.5136 - val_loss: 1021160948.2105\n",
      "Epoch 389/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 348983728.1829 - val_loss: 1082581475.3684\n",
      "Epoch 390/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 408639991.6130 - val_loss: 1434022854.7368\n",
      "Epoch 391/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 456611486.2832 - val_loss: 1093380716.3509\n",
      "Epoch 392/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 378489303.5567 - val_loss: 1085283687.8596\n",
      "Epoch 393/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 369375880.0211 - val_loss: 1077064817.9649\n",
      "Epoch 394/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 371155683.4055 - val_loss: 1072216251.5088\n",
      "Epoch 395/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 81us/step - loss: 361941311.9156 - val_loss: 1192335047.2982\n",
      "Epoch 396/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 446599902.6772 - val_loss: 1062927654.7368\n",
      "Epoch 397/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 527218794.5541 - val_loss: 1226975624.9825\n",
      "Epoch 398/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 434205846.3747 - val_loss: 1179134226.5263\n",
      "Epoch 399/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 354069829.5444 - val_loss: 1219775951.1579\n",
      "Epoch 400/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 385370979.9402 - val_loss: 1086452219.5088\n",
      "Epoch 401/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 352028973.7907 - val_loss: 1082073233.4035\n",
      "Epoch 402/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 358673958.6702 - val_loss: 1083913012.2105\n",
      "Epoch 403/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 343254633.9912 - val_loss: 1098578933.3333\n",
      "Epoch 404/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 328648875.0466 - val_loss: 1069129383.8596\n",
      "Epoch 405/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 351723887.6482 - val_loss: 1019494001.4035\n",
      "Epoch 406/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 343295766.5435 - val_loss: 1107707023.1579\n",
      "Epoch 407/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 328863446.3254 - val_loss: 1037718114.8070\n",
      "Epoch 408/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 321671646.9446 - val_loss: 1072205652.2105\n",
      "Epoch 409/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 347220000.1970 - val_loss: 1054401933.4737\n",
      "Epoch 410/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 354043548.3694 - val_loss: 1052300988.0702\n",
      "Epoch 411/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 343861631.7748 - val_loss: 1050319420.0702\n",
      "Epoch 412/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 346779183.9015 - val_loss: 1106547759.1579\n",
      "Epoch 413/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 396108286.2269 - val_loss: 1380571762.5263\n",
      "Epoch 414/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 402593925.1504 - val_loss: 1051963181.4737\n",
      "Epoch 415/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 388664989.6922 - val_loss: 1246078658.2456\n",
      "Epoch 416/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 400140034.5330 - val_loss: 1153423012.4912\n",
      "Epoch 417/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 325645744.2814 - val_loss: 1122004803.3684\n",
      "Epoch 418/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 332707965.3263 - val_loss: 1111164486.1754\n",
      "Epoch 419/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 321975967.3245 - val_loss: 1100609191.8596\n",
      "Epoch 420/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 379371785.7942 - val_loss: 1133684813.4737\n",
      "Epoch 421/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 346279771.6517 - val_loss: 1078966969.8246\n",
      "Epoch 422/2000\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 300438616.000 - 0s 73us/step - loss: 316388999.1909 - val_loss: 1112174264.7018\n",
      "Epoch 423/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 317593097.3157 - val_loss: 1069372595.0877\n",
      "Epoch 424/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 325740122.6174 - val_loss: 1072203865.2632\n",
      "Epoch 425/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 351595893.5866 - val_loss: 1046168354.2456\n",
      "Epoch 426/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 339834962.7441 - val_loss: 1017074726.1754\n",
      "Epoch 427/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 317985315.4617 - val_loss: 1088878688.0000\n",
      "Epoch 428/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 315602464.4503 - val_loss: 1055196354.2456\n",
      "Epoch 429/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 301877778.7722 - val_loss: 1060366464.0000\n",
      "Epoch 430/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 312882376.8654 - val_loss: 1053506764.3509\n",
      "Epoch 431/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 318135610.9059 - val_loss: 1076341975.0175\n",
      "Epoch 432/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 307370016.8725 - val_loss: 1122824648.4211\n",
      "Epoch 433/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 309138355.4055 - val_loss: 1098970336.0000\n",
      "Epoch 434/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 325178716.9604 - val_loss: 1346203560.4211\n",
      "Epoch 435/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 331840866.6737 - val_loss: 1109170157.4737\n",
      "Epoch 436/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 361386410.4556 - val_loss: 1413761176.7018\n",
      "Epoch 437/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 414671357.2700 - val_loss: 1223347375.1579\n",
      "Epoch 438/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 361023032.4573 - val_loss: 1554502845.7544\n",
      "Epoch 439/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 434560521.2876 - val_loss: 1054454707.6491\n",
      "Epoch 440/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 314921863.3738 - val_loss: 1137792346.9474\n",
      "Epoch 441/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 306967664.3237 - val_loss: 1063952939.2281\n",
      "Epoch 442/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 314368519.2331 - val_loss: 1217274586.3860\n",
      "Epoch 443/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 374479158.5295 - val_loss: 988179415.0175\n",
      "Epoch 444/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 370255525.1504 - val_loss: 1072859900.6316\n",
      "Epoch 445/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 295161778.9692 - val_loss: 1074969684.2105\n",
      "Epoch 446/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 307475267.1240 - val_loss: 1102065127.2982\n",
      "Epoch 447/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 294520191.2401 - val_loss: 1117127519.4386\n",
      "Epoch 448/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 289023292.1442 - val_loss: 1045371821.4737\n",
      "Epoch 449/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 307105498.4415 - val_loss: 1063283640.7018\n",
      "Epoch 450/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 296953217.9138 - val_loss: 1160497415.8596\n",
      "Epoch 451/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 350521126.2762 - val_loss: 1053788983.0175\n",
      "Epoch 452/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 319666516.6719 - val_loss: 1369175740.0702\n",
      "Epoch 453/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 438389633.5761 - val_loss: 1327077029.0526\n",
      "Epoch 454/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 448190477.7344 - val_loss: 1032761094.1754\n",
      "Epoch 455/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 425828752.0141 - val_loss: 1038281451.2281\n",
      "Epoch 456/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 328898652.5383 - val_loss: 1142375089.9649\n",
      "Epoch 457/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 328422721.7449 - val_loss: 1168260548.4912\n",
      "Epoch 458/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 314801106.1108 - val_loss: 1114616544.0000\n",
      "Epoch 459/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 291730993.5901 - val_loss: 998318474.6667\n",
      "Epoch 460/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 302441484.7071 - val_loss: 1182639165.1930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 461/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 303142062.7757 - val_loss: 1017661373.7544\n",
      "Epoch 462/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 326294027.3421 - val_loss: 1077003634.5263\n",
      "Epoch 463/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 299406436.1935 - val_loss: 1133639915.7895\n",
      "Epoch 464/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 293819003.2577 - val_loss: 1182889831.2982\n",
      "Epoch 465/2000\n",
      "1137/1137 [==============================] - 0s 105us/step - loss: 308273290.3149 - val_loss: 1231875036.6316\n",
      "Epoch 466/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 331142512.8725 - val_loss: 1230656897.6842\n",
      "Epoch 467/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 330117025.4354 - val_loss: 1109338558.8772\n",
      "Epoch 468/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 287956519.4582 - val_loss: 1142088334.0351\n",
      "Epoch 469/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 294213750.7124 - val_loss: 1043669637.6140\n",
      "Epoch 470/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 305587366.8672 - val_loss: 1322916437.3333\n",
      "Epoch 471/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 326773731.7713 - val_loss: 1163081324.3509\n",
      "Epoch 472/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 300331364.7845 - val_loss: 1334767802.9474\n",
      "Epoch 473/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 327955785.9068 - val_loss: 1069333461.3333\n",
      "Epoch 474/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 329328880.8443 - val_loss: 1087589879.0175\n",
      "Epoch 475/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 303478888.6684 - val_loss: 1107578981.0526\n",
      "Epoch 476/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 299236483.4195 - val_loss: 1122230495.4386\n",
      "Epoch 477/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 276155827.8698 - val_loss: 1045849328.8421\n",
      "Epoch 478/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 288683062.4732 - val_loss: 1068389198.0351\n",
      "Epoch 479/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 370576634.1460 - val_loss: 1066621053.7544\n",
      "Epoch 480/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 298818316.8901 - val_loss: 1063292170.6667\n",
      "Epoch 481/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 396287992.7388 - val_loss: 1138913015.0175\n",
      "Epoch 482/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 378859485.4952 - val_loss: 1109293779.6491\n",
      "Epoch 483/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 346214425.8786 - val_loss: 1063649644.9123\n",
      "Epoch 484/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 278521973.6218 - val_loss: 1161857812.2105\n",
      "Epoch 485/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 324492726.1777 - val_loss: 1126410091.2281\n",
      "Epoch 486/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 281221129.1891 - val_loss: 1082369306.9474\n",
      "Epoch 487/2000\n",
      "1137/1137 [==============================] - 0s 102us/step - loss: 278155560.1689 - val_loss: 1094914548.7719\n",
      "Epoch 488/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 284605514.9903 - val_loss: 1100127549.7544\n",
      "Epoch 489/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 272180691.7854 - val_loss: 1099346158.5965\n",
      "Epoch 490/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 297406288.1829 - val_loss: 1110922256.2807\n",
      "Epoch 491/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 335633785.3861 - val_loss: 1133876304.8421\n",
      "Epoch 492/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 276375374.7194 - val_loss: 1089912943.1579\n",
      "Epoch 493/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 336896050.9692 - val_loss: 1259568774.1754\n",
      "Epoch 494/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 350632903.4864 - val_loss: 1051338806.4561\n",
      "Epoch 495/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 295653588.7704 - val_loss: 1220747751.2982\n",
      "Epoch 496/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 337143291.3843 - val_loss: 1068390962.5263\n",
      "Epoch 497/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 279895438.5506 - val_loss: 1084436830.8772\n",
      "Epoch 498/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 281409441.1539 - val_loss: 1101305159.8596\n",
      "Epoch 499/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 351216894.0299 - val_loss: 1074123234.2456\n",
      "Epoch 500/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 318226860.4257 - val_loss: 1096942955.7895\n",
      "Epoch 501/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 294239093.6711 - val_loss: 1189039657.5439\n",
      "Epoch 502/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 360351070.8461 - val_loss: 1128997112.7018\n",
      "Epoch 503/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 313162661.0941 - val_loss: 1129449130.6667\n",
      "Epoch 504/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 359409243.6095 - val_loss: 1041484780.9123\n",
      "Epoch 505/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 320624516.1372 - val_loss: 1105611249.4035\n",
      "Epoch 506/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 279055267.9543 - val_loss: 1051284219.5088\n",
      "Epoch 507/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 306000133.4600 - val_loss: 1059348958.8772\n",
      "Epoch 508/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 277960210.0123 - val_loss: 1054461108.2105\n",
      "Epoch 509/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 266884622.7617 - val_loss: 1070833580.9123\n",
      "Epoch 510/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 261270932.9956 - val_loss: 1100490494.8772\n",
      "Epoch 511/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 302487192.7106 - val_loss: 1071339961.8246\n",
      "Epoch 512/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 260187260.4679 - val_loss: 1036331196.6316\n",
      "Epoch 513/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 261731710.2410 - val_loss: 1041382412.3509\n",
      "Epoch 514/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 280302620.1865 - val_loss: 1085597639.2982\n",
      "Epoch 515/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 262134028.1161 - val_loss: 1096704847.1579\n",
      "Epoch 516/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 276349361.3087 - val_loss: 1026978778.9474\n",
      "Epoch 517/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 253670385.4776 - val_loss: 1051766995.0877\n",
      "Epoch 518/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 309478016.6473 - val_loss: 1045977984.0000\n",
      "Epoch 519/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 258131774.3254 - val_loss: 1029234268.0702\n",
      "Epoch 520/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 294250529.4494 - val_loss: 1096722226.5263\n",
      "Epoch 521/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 267630661.9666 - val_loss: 1043364492.3509\n",
      "Epoch 522/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 283816996.3061 - val_loss: 1057439243.7895\n",
      "Epoch 523/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 330691579.5251 - val_loss: 1110884345.2632\n",
      "Epoch 524/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 283897536.9850 - val_loss: 1076865561.2632\n",
      "Epoch 525/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 264454476.3131 - val_loss: 1049373617.4035\n",
      "Epoch 526/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 274225451.8487 - val_loss: 1078839710.8772\n",
      "Epoch 527/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 73us/step - loss: 262405454.4943 - val_loss: 1148599233.6842\n",
      "Epoch 528/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 274657623.0079 - val_loss: 1033442053.6140\n",
      "Epoch 529/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 234907875.2084 - val_loss: 1066987658.6667\n",
      "Epoch 530/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 262425023.2612 - val_loss: 1151375109.6140\n",
      "Epoch 531/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 271543157.4459 - val_loss: 1080816316.0702\n",
      "Epoch 532/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 388898025.3720 - val_loss: 992573804.9123\n",
      "Epoch 533/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 338868600.2322 - val_loss: 1127196981.3333\n",
      "Epoch 534/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 271630116.1653 - val_loss: 1002963010.8070\n",
      "Epoch 535/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 272734987.3140 - val_loss: 1135444445.1930\n",
      "Epoch 536/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 316494113.9138 - val_loss: 1035898213.6140\n",
      "Epoch 537/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 304391674.3712 - val_loss: 1092204026.3860\n",
      "Epoch 538/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 272163663.4230 - val_loss: 1094947615.4386\n",
      "Epoch 539/2000\n",
      "1137/1137 [==============================] - 0s 100us/step - loss: 265424915.2507 - val_loss: 1222254035.6491\n",
      "Epoch 540/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 337933468.8338 - val_loss: 1537183013.0526\n",
      "Epoch 541/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 331085877.7133 - val_loss: 1204624462.0351\n",
      "Epoch 542/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 263032884.4046 - val_loss: 1058482257.4035\n",
      "Epoch 543/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 290502818.4485 - val_loss: 1077425154.8070\n",
      "Epoch 544/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 251572561.0836 - val_loss: 1130247943.8596\n",
      "Epoch 545/2000\n",
      "1137/1137 [==============================] - 0s 100us/step - loss: 273286349.6500 - val_loss: 1102796028.0702\n",
      "Epoch 546/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 288397505.1398 - val_loss: 1162316454.7368\n",
      "Epoch 547/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 287696621.4811 - val_loss: 1077916945.9649\n",
      "Epoch 548/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 243901173.2208 - val_loss: 1124333278.8772\n",
      "Epoch 549/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 248363050.4978 - val_loss: 1028783765.8947\n",
      "Epoch 550/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 241781561.8927 - val_loss: 1100869885.7544\n",
      "Epoch 551/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 237633850.4978 - val_loss: 1027230523.5088\n",
      "Epoch 552/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 229526377.2595 - val_loss: 1080240144.2807\n",
      "Epoch 553/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 278769578.0193 - val_loss: 1196551121.9649\n",
      "Epoch 554/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 259657573.5585 - val_loss: 1169417447.2982\n",
      "Epoch 555/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 252388062.0580 - val_loss: 1050485990.7368\n",
      "Epoch 556/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 250030941.7766 - val_loss: 1157813159.2982\n",
      "Epoch 557/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 274908574.1284 - val_loss: 1194768606.3158\n",
      "Epoch 558/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 257742344.9499 - val_loss: 1066295115.7895\n",
      "Epoch 559/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 243641499.1451 - val_loss: 1095859210.6667\n",
      "Epoch 560/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 258932807.2612 - val_loss: 1080883325.1930\n",
      "Epoch 561/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 252651581.0589 - val_loss: 1076442617.8246\n",
      "Epoch 562/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 226431870.3958 - val_loss: 1052677739.7895\n",
      "Epoch 563/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 252282528.9850 - val_loss: 1254748326.7368\n",
      "Epoch 564/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 302312095.8311 - val_loss: 1116864826.3860\n",
      "Epoch 565/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 274257129.6253 - val_loss: 1065854936.7018\n",
      "Epoch 566/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 278725271.8522 - val_loss: 1073879827.6491\n",
      "Epoch 567/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 239899795.3351 - val_loss: 1080219577.2632\n",
      "Epoch 568/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 260004139.9332 - val_loss: 1041764393.5439\n",
      "Epoch 569/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 258778395.8909 - val_loss: 1123894878.8772\n",
      "Epoch 570/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 245106602.5963 - val_loss: 1112035697.4035\n",
      "Epoch 571/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 272482831.2260 - val_loss: 1093268844.9123\n",
      "Epoch 572/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 357930116.0387 - val_loss: 1106020188.6316\n",
      "Epoch 573/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 318433263.7608 - val_loss: 1183499751.8596\n",
      "Epoch 574/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 268081084.4398 - val_loss: 1088320489.5439\n",
      "Epoch 575/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 243082443.3843 - val_loss: 1100050827.7895\n",
      "Epoch 576/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 232574672.9288 - val_loss: 1042732285.7544\n",
      "Epoch 577/2000\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 282159784.000 - 0s 89us/step - loss: 269954556.3835 - val_loss: 1101927670.4561\n",
      "Epoch 578/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 240519736.6262 - val_loss: 1100762036.2105\n",
      "Epoch 579/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 236356855.1205 - val_loss: 1113038213.0526\n",
      "Epoch 580/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 266293634.1671 - val_loss: 1135782159.7193\n",
      "Epoch 581/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 264321414.8531 - val_loss: 1065713972.7719\n",
      "Epoch 582/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 240609318.6420 - val_loss: 1102262538.6667\n",
      "Epoch 583/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 218538086.6280 - val_loss: 1081694178.2456\n",
      "Epoch 584/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 231776471.6130 - val_loss: 1067366694.1754\n",
      "Epoch 585/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 226806952.3729 - val_loss: 1060689272.7018\n",
      "Epoch 586/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 264092724.4890 - val_loss: 1100206937.2632\n",
      "Epoch 587/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 296630398.6772 - val_loss: 1369894051.3684\n",
      "Epoch 588/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 289844019.7291 - val_loss: 1069555372.3509\n",
      "Epoch 589/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 358198242.3923 - val_loss: 1354305109.3333\n",
      "Epoch 590/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 266878521.2383 - val_loss: 1097053373.7544\n",
      "Epoch 591/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 262274080.3799 - val_loss: 1062271582.8772\n",
      "Epoch 592/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 275531408.8021 - val_loss: 1418152815.1579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 256554094.1143 - val_loss: 1084649948.6316\n",
      "Epoch 594/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 227278051.8839 - val_loss: 1034236148.7719\n",
      "Epoch 595/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 214404567.9930 - val_loss: 1123995312.2807\n",
      "Epoch 596/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 230782848.4644 - val_loss: 1059602129.4035\n",
      "Epoch 597/2000\n",
      "1137/1137 [==============================] - 0s 103us/step - loss: 218067272.0352 - val_loss: 1084297041.9649\n",
      "Epoch 598/2000\n",
      "1137/1137 [==============================] - 0s 106us/step - loss: 223170071.2612 - val_loss: 1074485943.5789\n",
      "Epoch 599/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 224409542.5154 - val_loss: 1069599929.8246\n",
      "Epoch 600/2000\n",
      "1137/1137 [==============================] - 0s 105us/step - loss: 297681176.9780 - val_loss: 1086971079.2982\n",
      "Epoch 601/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 217746371.5251 - val_loss: 1056170088.9825\n",
      "Epoch 602/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 211794642.5893 - val_loss: 1061567269.0526\n",
      "Epoch 603/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 206386124.4538 - val_loss: 1156863280.2807\n",
      "Epoch 604/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 226991089.8012 - val_loss: 1084559933.1930\n",
      "Epoch 605/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 226808311.2471 - val_loss: 1145891226.3860\n",
      "Epoch 606/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 236487248.3799 - val_loss: 1095322004.2105\n",
      "Epoch 607/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 232057040.7177 - val_loss: 1088113103.1579\n",
      "Epoch 608/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 241794842.1460 - val_loss: 1063586996.2105\n",
      "Epoch 609/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 220236478.1073 - val_loss: 1039349711.7193\n",
      "Epoch 610/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 245182149.4178 - val_loss: 1063492284.0702\n",
      "Epoch 611/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 255533182.8179 - val_loss: 1036855261.1930\n",
      "Epoch 612/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 238724200.1618 - val_loss: 1042911962.3860\n",
      "Epoch 613/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 228813887.4653 - val_loss: 1152001967.7193\n",
      "Epoch 614/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 251676809.4424 - val_loss: 1089312831.4386\n",
      "Epoch 615/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 218345021.8892 - val_loss: 1121497685.8947\n",
      "Epoch 616/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 263402208.7036 - val_loss: 1073143333.0526\n",
      "Epoch 617/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 244129079.7397 - val_loss: 1134055157.8947\n",
      "Epoch 618/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 303021293.0026 - val_loss: 1132438350.5965\n",
      "Epoch 619/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 272228830.9868 - val_loss: 1131355764.7719\n",
      "Epoch 620/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 236820661.4459 - val_loss: 1030661594.3860\n",
      "Epoch 621/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 223147223.4160 - val_loss: 1050940071.2982\n",
      "Epoch 622/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 217840830.8602 - val_loss: 1060506825.5439\n",
      "Epoch 623/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 245568409.8646 - val_loss: 1158291968.5614\n",
      "Epoch 624/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 262781508.2779 - val_loss: 1204979124.7719\n",
      "Epoch 625/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 265217657.1187 - val_loss: 1122298096.2807\n",
      "Epoch 626/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 237681702.1917 - val_loss: 1106357380.4912\n",
      "Epoch 627/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 267779359.8593 - val_loss: 1104935260.6316\n",
      "Epoch 628/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 243832030.1425 - val_loss: 1037515782.7368\n",
      "Epoch 629/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 241403132.6649 - val_loss: 1067966075.5088\n",
      "Epoch 630/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 228295712.7458 - val_loss: 1136953302.4561\n",
      "Epoch 631/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 297059922.7722 - val_loss: 1073502118.1754\n",
      "Epoch 632/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 310955140.1091 - val_loss: 1313348903.2982\n",
      "Epoch 633/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 280221347.5180 - val_loss: 1230673370.9474\n",
      "Epoch 634/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 264387556.9252 - val_loss: 1158775850.6667\n",
      "Epoch 635/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 270974215.9367 - val_loss: 1115992016.2807\n",
      "Epoch 636/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 286249735.0501 - val_loss: 1088110631.8596\n",
      "Epoch 637/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 249417721.2032 - val_loss: 1124274777.8246\n",
      "Epoch 638/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 274075678.4802 - val_loss: 1127860969.5439\n",
      "Epoch 639/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 275301265.9701 - val_loss: 1130480588.3509\n",
      "Epoch 640/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 342546253.3685 - val_loss: 1167006648.1404\n",
      "Epoch 641/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 225898679.7397 - val_loss: 1193547916.9123\n",
      "Epoch 642/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 215361850.8355 - val_loss: 1098253891.3684\n",
      "Epoch 643/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 262006097.3087 - val_loss: 1125518880.0000\n",
      "Epoch 644/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 199760850.6174 - val_loss: 1132838224.2807\n",
      "Epoch 645/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 223656197.7274 - val_loss: 1128877543.8596\n",
      "Epoch 646/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 213968267.4828 - val_loss: 1074848851.6491\n",
      "Epoch 647/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 261615001.3017 - val_loss: 1243733088.0000\n",
      "Epoch 648/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 326413705.1187 - val_loss: 1129436623.7193\n",
      "Epoch 649/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 215702549.2067 - val_loss: 1139827998.3158\n",
      "Epoch 650/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 214911425.0976 - val_loss: 1096015715.3684\n",
      "Epoch 651/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 199472450.1812 - val_loss: 1115832746.1053\n",
      "Epoch 652/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 197814504.6403 - val_loss: 1093502596.4912\n",
      "Epoch 653/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 197971171.8417 - val_loss: 1060332724.2105\n",
      "Epoch 654/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 194383896.0633 - val_loss: 1111383901.1930\n",
      "Epoch 655/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 217958363.0185 - val_loss: 1145229533.1930\n",
      "Epoch 656/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 236734085.6148 - val_loss: 1111940694.4561\n",
      "Epoch 657/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 213406857.9912 - val_loss: 1132394757.6140\n",
      "Epoch 658/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 192940539.9050 - val_loss: 1134290307.3684\n",
      "Epoch 659/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 76us/step - loss: 204158425.8083 - val_loss: 1123497676.9123\n",
      "Epoch 660/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 189941937.4916 - val_loss: 1073912596.2105\n",
      "Epoch 661/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 219691110.5506 - val_loss: 1169360463.1579\n",
      "Epoch 662/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 242944444.5805 - val_loss: 1156944771.3684\n",
      "Epoch 663/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 236294478.4661 - val_loss: 1113548552.4211\n",
      "Epoch 664/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 200107069.5796 - val_loss: 1119012793.2632\n",
      "Epoch 665/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 210584903.5004 - val_loss: 1131200769.6842\n",
      "Epoch 666/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 201515170.2656 - val_loss: 1198715955.6491\n",
      "Epoch 667/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 214963466.7792 - val_loss: 1071987632.2807\n",
      "Epoch 668/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 245647464.3237 - val_loss: 1149521690.9474\n",
      "Epoch 669/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 224035018.3008 - val_loss: 1052925606.7368\n",
      "Epoch 670/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 212020066.7441 - val_loss: 1117709125.0526\n",
      "Epoch 671/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 210254533.9173 - val_loss: 1069865112.1404\n",
      "Epoch 672/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 215515790.7757 - val_loss: 1091780037.0526\n",
      "Epoch 673/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 248166930.0405 - val_loss: 1211214736.2807\n",
      "Epoch 674/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 222897806.3958 - val_loss: 1116107251.0877\n",
      "Epoch 675/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 234776946.2797 - val_loss: 1230771229.1930\n",
      "Epoch 676/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 281926626.1390 - val_loss: 1181987810.8070\n",
      "Epoch 677/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 257535552.1548 - val_loss: 1143429819.5088\n",
      "Epoch 678/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 200081717.1785 - val_loss: 1143041073.9649\n",
      "Epoch 679/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 215423168.5347 - val_loss: 1187575466.1053\n",
      "Epoch 680/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 274315496.3448 - val_loss: 1083248510.3158\n",
      "Epoch 681/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 202016868.7001 - val_loss: 1096954461.7544\n",
      "Epoch 682/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 199461624.3166 - val_loss: 1094104545.6842\n",
      "Epoch 683/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 213413061.9384 - val_loss: 1118270078.8772\n",
      "Epoch 684/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 266051370.1601 - val_loss: 1165176926.3158\n",
      "Epoch 685/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 199615358.7194 - val_loss: 1069931505.4035\n",
      "Epoch 686/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 199203176.7951 - val_loss: 1097544125.1930\n",
      "Epoch 687/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 216342706.9974 - val_loss: 1130253271.0175\n",
      "Epoch 688/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 207881080.4573 - val_loss: 1287194288.2807\n",
      "Epoch 689/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 221207416.5136 - val_loss: 1115248871.2982\n",
      "Epoch 690/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 195680506.7722 - val_loss: 1124717360.8421\n",
      "Epoch 691/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 218961105.9701 - val_loss: 1106267536.2807\n",
      "Epoch 692/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 231514508.2709 - val_loss: 1127533789.1930\n",
      "Epoch 693/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 232970014.6491 - val_loss: 1330294570.6667\n",
      "Epoch 694/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 296781328.5347 - val_loss: 1080937202.5263\n",
      "Epoch 695/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 282566970.6526 - val_loss: 1265273510.1754\n",
      "Epoch 696/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 262981909.5866 - val_loss: 1148464044.9123\n",
      "Epoch 697/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 219554106.4415 - val_loss: 1113502202.3860\n",
      "Epoch 698/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 189558224.0844 - val_loss: 1162383413.8947\n",
      "Epoch 699/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 180987812.0106 - val_loss: 1113354593.1228\n",
      "Epoch 700/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 197138866.0475 - val_loss: 1082849568.5614\n",
      "Epoch 701/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 179125991.9015 - val_loss: 1131515398.7368\n",
      "Epoch 702/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 179896436.8549 - val_loss: 1099453661.7544\n",
      "Epoch 703/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 224901205.4741 - val_loss: 1228149820.6316\n",
      "Epoch 704/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 215629241.2454 - val_loss: 1120001886.3158\n",
      "Epoch 705/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 201787050.2656 - val_loss: 1108430562.2456\n",
      "Epoch 706/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 196598421.9666 - val_loss: 1053622536.4211\n",
      "Epoch 707/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 189043950.4099 - val_loss: 1124706064.2807\n",
      "Epoch 708/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 207919200.5910 - val_loss: 1203985956.4912\n",
      "Epoch 709/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 185616777.2313 - val_loss: 1155754955.7895\n",
      "Epoch 710/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 205330468.7001 - val_loss: 1086872679.8596\n",
      "Epoch 711/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 182266869.9033 - val_loss: 1092562296.7018\n",
      "Epoch 712/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 176087975.0783 - val_loss: 1103755138.2456\n",
      "Epoch 713/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 180165050.4204 - val_loss: 1181827810.2456\n",
      "Epoch 714/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 213793652.9534 - val_loss: 1161440992.5614\n",
      "Epoch 715/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 205631102.5224 - val_loss: 1188390636.9123\n",
      "Epoch 716/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 211326185.8364 - val_loss: 1232390270.8772\n",
      "Epoch 717/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 232590884.5734 - val_loss: 1207091102.8772\n",
      "Epoch 718/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 213845150.5365 - val_loss: 1143693482.1053\n",
      "Epoch 719/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 218140336.4222 - val_loss: 1127815782.7368\n",
      "Epoch 720/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 185327943.6552 - val_loss: 1159593259.2281\n",
      "Epoch 721/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 173451673.4283 - val_loss: 1147451196.0702\n",
      "Epoch 722/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 192493577.8927 - val_loss: 1100311707.5088\n",
      "Epoch 723/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 211960807.1627 - val_loss: 1136049785.2632\n",
      "Epoch 724/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 201430957.2841 - val_loss: 1150938708.2105\n",
      "Epoch 725/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 75us/step - loss: 219739524.6297 - val_loss: 1116128862.3158\n",
      "Epoch 726/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 188523960.2603 - val_loss: 1066132835.3684\n",
      "Epoch 727/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 190056662.9094 - val_loss: 1140111692.9123\n",
      "Epoch 728/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 215960668.6931 - val_loss: 1153926125.4737\n",
      "Epoch 729/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 207210835.1803 - val_loss: 1075725408.5614\n",
      "Epoch 730/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 198149251.2929 - val_loss: 1117491888.2807\n",
      "Epoch 731/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 185369419.5954 - val_loss: 1179284083.6491\n",
      "Epoch 732/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 184156056.3518 - val_loss: 1148394693.0526\n",
      "Epoch 733/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 182156313.8857 - val_loss: 1145761978.3860\n",
      "Epoch 734/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 217167247.7045 - val_loss: 1087775938.2456\n",
      "Epoch 735/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 221587341.9173 - val_loss: 1161229930.1053\n",
      "Epoch 736/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 218861671.2471 - val_loss: 1107707656.4211\n",
      "Epoch 737/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 212285360.5066 - val_loss: 1183238634.1053\n",
      "Epoch 738/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 185382565.9666 - val_loss: 1136214962.5263\n",
      "Epoch 739/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 179075498.2586 - val_loss: 1105053292.9123\n",
      "Epoch 740/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 187806217.5690 - val_loss: 1120221545.5439\n",
      "Epoch 741/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 191805896.7106 - val_loss: 1090268613.6140\n",
      "Epoch 742/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 189472637.5655 - val_loss: 1154837579.2281\n",
      "Epoch 743/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 193271763.1944 - val_loss: 1171601923.3684\n",
      "Epoch 744/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 180003293.9455 - val_loss: 1122095466.1053\n",
      "Epoch 745/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 181801232.8162 - val_loss: 1135959965.1930\n",
      "Epoch 746/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 181543684.7704 - val_loss: 1122649973.3333\n",
      "Epoch 747/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 174477468.7916 - val_loss: 1101806801.9649\n",
      "Epoch 748/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 179127077.3052 - val_loss: 1057588023.5789\n",
      "Epoch 749/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 192472038.9235 - val_loss: 1162209978.3860\n",
      "Epoch 750/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 203754462.2691 - val_loss: 1341292980.2105\n",
      "Epoch 751/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 316313722.1812 - val_loss: 1311100132.4912\n",
      "Epoch 752/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 234877929.9349 - val_loss: 1155153538.8070\n",
      "Epoch 753/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 250465255.5427 - val_loss: 1086382412.3509\n",
      "Epoch 754/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 226802511.2401 - val_loss: 1050012198.1754\n",
      "Epoch 755/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 183883568.5066 - val_loss: 1117740248.1404\n",
      "Epoch 756/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 162008300.0176 - val_loss: 1086713988.4912\n",
      "Epoch 757/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 162767111.0501 - val_loss: 1101714371.3684\n",
      "Epoch 758/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 164350984.4573 - val_loss: 1129907988.7719\n",
      "Epoch 759/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 209268196.4398 - val_loss: 1081213780.2105\n",
      "Epoch 760/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 176735510.0369 - val_loss: 1068236588.3509\n",
      "Epoch 761/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 198586144.1970 - val_loss: 1119157568.5614\n",
      "Epoch 762/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 208304194.9974 - val_loss: 1305532856.7018\n",
      "Epoch 763/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 210981497.1469 - val_loss: 1109964455.8596\n",
      "Epoch 764/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 215143899.1310 - val_loss: 1114014885.6140\n",
      "Epoch 765/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 197328136.4151 - val_loss: 1177402888.9825\n",
      "Epoch 766/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 177413477.6711 - val_loss: 1171401537.6842\n",
      "Epoch 767/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 263172550.3465 - val_loss: 1087719815.8596\n",
      "Epoch 768/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 217737127.1205 - val_loss: 1197297327.1579\n",
      "Epoch 769/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 193944492.9323 - val_loss: 1238984343.5789\n",
      "Epoch 770/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 173920306.7441 - val_loss: 1149453866.6667\n",
      "Epoch 771/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 224328211.6306 - val_loss: 1256237126.7368\n",
      "Epoch 772/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 194801374.4521 - val_loss: 1145355785.5439\n",
      "Epoch 773/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 180242388.5453 - val_loss: 1080073962.1053\n",
      "Epoch 774/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 196564791.1205 - val_loss: 1298323732.2105\n",
      "Epoch 775/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 222272528.9288 - val_loss: 1123496527.1579\n",
      "Epoch 776/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 188334653.1996 - val_loss: 1150711552.5614\n",
      "Epoch 777/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 167102646.1354 - val_loss: 1147103054.5965\n",
      "Epoch 778/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 207637447.4442 - val_loss: 1082955035.5088\n",
      "Epoch 779/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 167949057.4705 - val_loss: 1125431972.4912\n",
      "Epoch 780/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 166267939.8980 - val_loss: 1118061036.3509\n",
      "Epoch 781/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 175044851.3492 - val_loss: 1100493636.4912\n",
      "Epoch 782/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 166255245.4459 - val_loss: 1214607916.3509\n",
      "Epoch 783/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 241326424.8232 - val_loss: 1183961846.4561\n",
      "Epoch 784/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 201704155.8487 - val_loss: 1101280848.2807\n",
      "Epoch 785/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 156254065.1961 - val_loss: 1124559263.4386\n",
      "Epoch 786/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 197363659.2155 - val_loss: 1405668846.0351\n",
      "Epoch 787/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 229751281.2806 - val_loss: 1129322882.2456\n",
      "Epoch 788/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 218053175.3105 - val_loss: 1143340455.2982\n",
      "Epoch 789/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 161590454.7405 - val_loss: 1188532734.8772\n",
      "Epoch 790/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 192262453.7414 - val_loss: 1207273549.4737\n",
      "Epoch 791/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 73us/step - loss: 194113033.0976 - val_loss: 1130467238.1754\n",
      "Epoch 792/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 180830528.3729 - val_loss: 1183518430.3158\n",
      "Epoch 793/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 152894491.6376 - val_loss: 1107293034.1053\n",
      "Epoch 794/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 158849027.2788 - val_loss: 1144019498.6667\n",
      "Epoch 795/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 186039525.6851 - val_loss: 1152644134.7368\n",
      "Epoch 796/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 159204142.0440 - val_loss: 1117488092.6316\n",
      "Epoch 797/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 189446492.4679 - val_loss: 1103764638.3158\n",
      "Epoch 798/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 165226674.2375 - val_loss: 1237698576.2807\n",
      "Epoch 799/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 198233516.8619 - val_loss: 1191622129.4035\n",
      "Epoch 800/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 166797102.8883 - val_loss: 1195473397.8947\n",
      "Epoch 801/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 170631133.6640 - val_loss: 1171998190.5965\n",
      "Epoch 802/2000\n",
      "1137/1137 [==============================] - 0s 67us/step - loss: 167768330.7581 - val_loss: 1115719329.6842\n",
      "Epoch 803/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 149735897.4565 - val_loss: 1125870793.5439\n",
      "Epoch 804/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 165064193.6887 - val_loss: 1100126479.1579\n",
      "Epoch 805/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 169018450.2375 - val_loss: 1099210693.0526\n",
      "Epoch 806/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 178579715.0536 - val_loss: 1224095111.8596\n",
      "Epoch 807/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 196976746.0827 - val_loss: 1200152122.9474\n",
      "Epoch 808/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 243821860.4468 - val_loss: 1075194428.6316\n",
      "Epoch 809/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 197040900.5594 - val_loss: 1089181097.5439\n",
      "Epoch 810/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 160452593.7661 - val_loss: 1118871423.4386\n",
      "Epoch 811/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 146539616.7106 - val_loss: 1156611552.0000\n",
      "Epoch 812/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 156249146.8777 - val_loss: 1090864427.2281\n",
      "Epoch 813/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 194922051.8698 - val_loss: 1203675005.1930\n",
      "Epoch 814/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 276935920.5207 - val_loss: 1056720073.5439\n",
      "Epoch 815/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 216975766.1354 - val_loss: 1269104654.5965\n",
      "Epoch 816/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 214103430.3887 - val_loss: 1112463985.4035\n",
      "Epoch 817/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 186929776.3377 - val_loss: 1178517422.0351\n",
      "Epoch 818/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 154620635.2155 - val_loss: 1165077974.4561\n",
      "Epoch 819/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 154212113.0554 - val_loss: 1210247944.4211\n",
      "Epoch 820/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 164323057.9912 - val_loss: 1086519014.1754\n",
      "Epoch 821/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 146936800.3588 - val_loss: 1225121156.4912\n",
      "Epoch 822/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 161056755.1240 - val_loss: 1123877344.5614\n",
      "Epoch 823/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 165343311.4230 - val_loss: 1307584824.1404\n",
      "Epoch 824/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 242291102.6772 - val_loss: 1172929546.1053\n",
      "Epoch 825/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 153737851.2366 - val_loss: 1110952679.2982\n",
      "Epoch 826/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 155573160.4433 - val_loss: 1076196436.2105\n",
      "Epoch 827/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 169292205.0167 - val_loss: 1088970326.4561\n",
      "Epoch 828/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 152206465.6675 - val_loss: 1188360524.9123\n",
      "Epoch 829/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 169899751.9648 - val_loss: 1096805931.2281\n",
      "Epoch 830/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 197331819.7502 - val_loss: 1325213570.2456\n",
      "Epoch 831/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 189183798.5928 - val_loss: 1101474877.1930\n",
      "Epoch 832/2000\n",
      "1137/1137 [==============================] - 0s 100us/step - loss: 167351146.4697 - val_loss: 1187371546.9474\n",
      "Epoch 833/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 166724666.8004 - val_loss: 1181234676.7719\n",
      "Epoch 834/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 173303787.4266 - val_loss: 1157100439.5789\n",
      "Epoch 835/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 268115676.4398 - val_loss: 1146125496.1404\n",
      "Epoch 836/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 228956820.6719 - val_loss: 1251104647.2982\n",
      "Epoch 837/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 192303187.2366 - val_loss: 1110732681.5439\n",
      "Epoch 838/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 201281316.9815 - val_loss: 1193904994.8070\n",
      "Epoch 839/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 205463039.1416 - val_loss: 1116990815.4386\n",
      "Epoch 840/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 174558465.8505 - val_loss: 1169891713.6842\n",
      "Epoch 841/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 156636214.0158 - val_loss: 1100687383.5789\n",
      "Epoch 842/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 152449276.2146 - val_loss: 1117336560.8421\n",
      "Epoch 843/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 168864416.1337 - val_loss: 1152222541.4737\n",
      "Epoch 844/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 185906498.7581 - val_loss: 1227902144.0000\n",
      "Epoch 845/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 227630899.8839 - val_loss: 1261716071.2982\n",
      "Epoch 846/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 338192942.4028 - val_loss: 1141498795.7895\n",
      "Epoch 847/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 239118223.5075 - val_loss: 1208284236.3509\n",
      "Epoch 848/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 177129397.0660 - val_loss: 1152411484.0702\n",
      "Epoch 849/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 162326658.3360 - val_loss: 1124211113.5439\n",
      "Epoch 850/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 140508677.9666 - val_loss: 1120783293.1930\n",
      "Epoch 851/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 157561157.7696 - val_loss: 1303081368.7018\n",
      "Epoch 852/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 189822210.4556 - val_loss: 1134878252.9123\n",
      "Epoch 853/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 162727295.2682 - val_loss: 1200369307.5088\n",
      "Epoch 854/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 170217737.6675 - val_loss: 1150587525.6140\n",
      "Epoch 855/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 168689371.7924 - val_loss: 1133894242.2456\n",
      "Epoch 856/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 162252169.5972 - val_loss: 1100819697.4035\n",
      "Epoch 857/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 74us/step - loss: 165941281.3650 - val_loss: 1100526468.4912\n",
      "Epoch 858/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 153276920.7810 - val_loss: 1082190668.3509\n",
      "Epoch 859/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 143042754.3008 - val_loss: 1094241729.6842\n",
      "Epoch 860/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 142622160.6192 - val_loss: 1121225864.4211\n",
      "Epoch 861/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 144911661.9173 - val_loss: 1097830531.3684\n",
      "Epoch 862/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 167484582.4028 - val_loss: 1104186432.5614\n",
      "Epoch 863/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 154463328.8091 - val_loss: 1125500702.8772\n",
      "Epoch 864/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 167664524.0950 - val_loss: 1206311257.8246\n",
      "Epoch 865/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 209485702.4872 - val_loss: 1134672913.9649\n",
      "Epoch 866/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 147911285.0554 - val_loss: 1160576453.0526\n",
      "Epoch 867/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 157393992.0493 - val_loss: 1169290567.2982\n",
      "Epoch 868/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 147135356.9675 - val_loss: 1122082691.9298\n",
      "Epoch 869/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 141672397.2982 - val_loss: 1093313663.4386\n",
      "Epoch 870/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 151669291.8909 - val_loss: 1145977870.0351\n",
      "Epoch 871/2000\n",
      "1137/1137 [==============================] - 0s 64us/step - loss: 166321576.9077 - val_loss: 1177102726.1754\n",
      "Epoch 872/2000\n",
      "1137/1137 [==============================] - 0s 63us/step - loss: 166970650.4134 - val_loss: 1077623477.3333\n",
      "Epoch 873/2000\n",
      "1137/1137 [==============================] - 0s 64us/step - loss: 177175948.3553 - val_loss: 1147552073.5439\n",
      "Epoch 874/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 194607313.7449 - val_loss: 1094098034.5263\n",
      "Epoch 875/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 162174187.6658 - val_loss: 1087018635.7895\n",
      "Epoch 876/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 143234956.1302 - val_loss: 1121458585.8246\n",
      "Epoch 877/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 146556333.0308 - val_loss: 1124486008.7018\n",
      "Epoch 878/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 138867545.1258 - val_loss: 1096569076.2105\n",
      "Epoch 879/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 140554475.6799 - val_loss: 1123403503.1579\n",
      "Epoch 880/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 139347211.1170 - val_loss: 1079001906.5263\n",
      "Epoch 881/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 132449470.4802 - val_loss: 1099338122.1053\n",
      "Epoch 882/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 162720849.2806 - val_loss: 1432041757.1930\n",
      "Epoch 883/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 256016346.6667 - val_loss: 1435366935.5789\n",
      "Epoch 884/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 237850782.0721 - val_loss: 1122161065.5439\n",
      "Epoch 885/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 173141424.8021 - val_loss: 1174512469.3333\n",
      "Epoch 886/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 148280634.0545 - val_loss: 1093353565.7544\n",
      "Epoch 887/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 151649515.8065 - val_loss: 1065156609.6842\n",
      "Epoch 888/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 137400613.8540 - val_loss: 1167763460.4912\n",
      "Epoch 889/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 152911689.7731 - val_loss: 1140076250.3860\n",
      "Epoch 890/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 167473831.1909 - val_loss: 1162950925.4737\n",
      "Epoch 891/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 167945304.0633 - val_loss: 1194868452.4912\n",
      "Epoch 892/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 145407566.4802 - val_loss: 1125139621.6140\n",
      "Epoch 893/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 145864046.5646 - val_loss: 1193873608.4211\n",
      "Epoch 894/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 131336743.6130 - val_loss: 1093026855.2982\n",
      "Epoch 895/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 134387062.4310 - val_loss: 1140461209.8246\n",
      "Epoch 896/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 158742017.0132 - val_loss: 1203444676.4912\n",
      "Epoch 897/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 136206145.6746 - val_loss: 1140078865.9649\n",
      "Epoch 898/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 136708334.0967 - val_loss: 1144569807.1579\n",
      "Epoch 899/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 135276753.9771 - val_loss: 1164871763.6491\n",
      "Epoch 900/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 166874723.1522 - val_loss: 1191711207.2982\n",
      "Epoch 901/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 214895434.4837 - val_loss: 1190683141.6140\n",
      "Epoch 902/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 231471260.1442 - val_loss: 1214313642.6667\n",
      "Epoch 903/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 332964866.2797 - val_loss: 998161077.3333\n",
      "Epoch 904/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 223133542.1917 - val_loss: 1156918021.0526\n",
      "Epoch 905/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 201617272.0915 - val_loss: 1114850015.4386\n",
      "Epoch 906/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 167727609.1609 - val_loss: 1134719123.6491\n",
      "Epoch 907/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 177327898.8355 - val_loss: 1144051811.9298\n",
      "Epoch 908/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 161184573.1363 - val_loss: 1219348370.5263\n",
      "Epoch 909/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 136162520.4714 - val_loss: 1153984896.5614\n",
      "Epoch 910/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 135586768.2322 - val_loss: 1176851373.4737\n",
      "Epoch 911/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 152312934.7828 - val_loss: 1136147447.0175\n",
      "Epoch 912/2000\n",
      "1137/1137 [==============================] - 0s 109us/step - loss: 133021093.5233 - val_loss: 1154200187.5088\n",
      "Epoch 913/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 149586771.8909 - val_loss: 1245823405.4737\n",
      "Epoch 914/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 150675107.4266 - val_loss: 1178536708.4912\n",
      "Epoch 915/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 165322974.6069 - val_loss: 1106789853.1930\n",
      "Epoch 916/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 152563940.7282 - val_loss: 1135912531.0877\n",
      "Epoch 917/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 146505541.8540 - val_loss: 1101968056.7018\n",
      "Epoch 918/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 140429226.6807 - val_loss: 1138711060.7719\n",
      "Epoch 919/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 134534346.9833 - val_loss: 1102288273.4035\n",
      "Epoch 920/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 143862932.2850 - val_loss: 1136180092.0702\n",
      "Epoch 921/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 203134319.5638 - val_loss: 1116887637.8947\n",
      "Epoch 922/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 242482926.4169 - val_loss: 1147860761.8246\n",
      "Epoch 923/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 80us/step - loss: 171596216.5136 - val_loss: 1230299633.9649\n",
      "Epoch 924/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 140035377.0765 - val_loss: 1158672951.5789\n",
      "Epoch 925/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 128142137.0554 - val_loss: 1106474735.1579\n",
      "Epoch 926/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 205469294.1847 - val_loss: 1479268235.7895\n",
      "Epoch 927/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 242622743.8663 - val_loss: 1140052889.8246\n",
      "Epoch 928/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 148858857.9349 - val_loss: 1182664168.9825\n",
      "Epoch 929/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 163306689.5409 - val_loss: 1120619718.7368\n",
      "Epoch 930/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 162986692.3272 - val_loss: 1154231631.7193\n",
      "Epoch 931/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 144343603.8980 - val_loss: 1193961603.3684\n",
      "Epoch 932/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 144567447.0923 - val_loss: 1125503393.1228\n",
      "Epoch 933/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 132799063.5778 - val_loss: 1138658947.3684\n",
      "Epoch 934/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 137603019.4547 - val_loss: 1199850696.4211\n",
      "Epoch 935/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 135101231.7748 - val_loss: 1148250854.7368\n",
      "Epoch 936/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 133467402.7230 - val_loss: 1171496008.4211\n",
      "Epoch 937/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 131119079.6904 - val_loss: 1156029259.7895\n",
      "Epoch 938/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 140176713.5550 - val_loss: 1151405129.5439\n",
      "Epoch 939/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 153677749.9666 - val_loss: 1178992610.8070\n",
      "Epoch 940/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 143011773.0378 - val_loss: 1165719776.5614\n",
      "Epoch 941/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 146930418.7933 - val_loss: 1131485496.7018\n",
      "Epoch 942/2000\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 153000924.235 - 0s 78us/step - loss: 142869565.3544 - val_loss: 1131703336.4211\n",
      "Epoch 943/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 143541383.1627 - val_loss: 1189667974.1754\n",
      "Epoch 944/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 133108266.3641 - val_loss: 1120829390.5965\n",
      "Epoch 945/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 170793553.2383 - val_loss: 1190069783.0175\n",
      "Epoch 946/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 224449327.9156 - val_loss: 1206270185.5439\n",
      "Epoch 947/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 153322674.8285 - val_loss: 1149318362.9474\n",
      "Epoch 948/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 127187031.2401 - val_loss: 1170365685.8947\n",
      "Epoch 949/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 141747590.9657 - val_loss: 1143687725.4737\n",
      "Epoch 950/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 188599093.5233 - val_loss: 1234146687.4386\n",
      "Epoch 951/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 168499732.7986 - val_loss: 1150534598.7368\n",
      "Epoch 952/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 131340044.8267 - val_loss: 1138222929.4035\n",
      "Epoch 953/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 132332140.6931 - val_loss: 1149936150.4561\n",
      "Epoch 954/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 130507617.1011 - val_loss: 1147032876.3509\n",
      "Epoch 955/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 128789651.4055 - val_loss: 1209857667.3684\n",
      "Epoch 956/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 126488116.8127 - val_loss: 1169511202.2456\n",
      "Epoch 957/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 131368848.3518 - val_loss: 1200683178.1053\n",
      "Epoch 958/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 135256094.2551 - val_loss: 1226789418.1053\n",
      "Epoch 959/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 154603067.2858 - val_loss: 1202595699.0877\n",
      "Epoch 960/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 137252620.2076 - val_loss: 1179092440.7018\n",
      "Epoch 961/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 133323308.4679 - val_loss: 1188402679.5789\n",
      "Epoch 962/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 130664398.0299 - val_loss: 1194513455.1579\n",
      "Epoch 963/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 121288332.7001 - val_loss: 1162496281.2632\n",
      "Epoch 964/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 119929128.5840 - val_loss: 1160154863.1579\n",
      "Epoch 965/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 141480226.8004 - val_loss: 1364944930.2456\n",
      "Epoch 966/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 259459279.8382 - val_loss: 1145462589.7544\n",
      "Epoch 967/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 148239001.4565 - val_loss: 1261968023.0175\n",
      "Epoch 968/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 175793002.9763 - val_loss: 1175574420.2105\n",
      "Epoch 969/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 133155685.7696 - val_loss: 1299079765.3333\n",
      "Epoch 970/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 172138174.2832 - val_loss: 1119772130.2456\n",
      "Epoch 971/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 149024884.6508 - val_loss: 1127169733.6140\n",
      "Epoch 972/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 124422962.8355 - val_loss: 1197588491.2281\n",
      "Epoch 973/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 121954977.9420 - val_loss: 1260810518.4561\n",
      "Epoch 974/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 191395820.4679 - val_loss: 1233400846.5965\n",
      "Epoch 975/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 205461319.8663 - val_loss: 1373806621.7544\n",
      "Epoch 976/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 210016088.7106 - val_loss: 1117561353.5439\n",
      "Epoch 977/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 148809331.9543 - val_loss: 1151520160.5614\n",
      "Epoch 978/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 141358681.8786 - val_loss: 1190307339.7895\n",
      "Epoch 979/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 130835497.4705 - val_loss: 1181625963.2281\n",
      "Epoch 980/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 128272145.0695 - val_loss: 1138138419.6491\n",
      "Epoch 981/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 121284513.4705 - val_loss: 1259555542.4561\n",
      "Epoch 982/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 162740083.0185 - val_loss: 1187213514.6667\n",
      "Epoch 983/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 162255504.7880 - val_loss: 1185557993.5439\n",
      "Epoch 984/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 140444895.8734 - val_loss: 1194906430.3158\n",
      "Epoch 985/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 209884668.5805 - val_loss: 1216849519.7193\n",
      "Epoch 986/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 138133506.6033 - val_loss: 1219883884.3509\n",
      "Epoch 987/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 132600182.0440 - val_loss: 1202197253.0526\n",
      "Epoch 988/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 123415948.7353 - val_loss: 1199363722.1053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 989/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 138454554.2234 - val_loss: 1163176326.1754\n",
      "Epoch 990/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 129790457.8857 - val_loss: 1187486379.7895\n",
      "Epoch 991/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 155952913.7449 - val_loss: 1150329653.8947\n",
      "Epoch 992/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 135786058.8496 - val_loss: 1219846138.9474\n",
      "Epoch 993/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 126931650.6315 - val_loss: 1204141459.0877\n",
      "Epoch 994/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 127155523.8487 - val_loss: 1188128149.3333\n",
      "Epoch 995/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 128148643.1240 - val_loss: 1193640172.9123\n",
      "Epoch 996/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 131515166.8461 - val_loss: 1196041457.4035\n",
      "Epoch 997/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 115162329.7661 - val_loss: 1157394145.6842\n",
      "Epoch 998/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 107170130.2304 - val_loss: 1265573699.9298\n",
      "Epoch 999/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 131777485.8892 - val_loss: 1200790312.9825\n",
      "Epoch 1000/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 121837582.9024 - val_loss: 1268397707.2281\n",
      "Epoch 1001/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 143851816.7177 - val_loss: 1198361325.4737\n",
      "Epoch 1002/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 119220954.2938 - val_loss: 1198857000.4211\n",
      "Epoch 1003/2000\n",
      "1137/1137 [==============================] - 0s 106us/step - loss: 110278033.6464 - val_loss: 1162264149.3333\n",
      "Epoch 1004/2000\n",
      "1137/1137 [==============================] - 0s 114us/step - loss: 118931440.3518 - val_loss: 1243634962.5263\n",
      "Epoch 1005/2000\n",
      "1137/1137 [==============================] - 0s 106us/step - loss: 112500643.6025 - val_loss: 1208633873.9649\n",
      "Epoch 1006/2000\n",
      "1137/1137 [==============================] - 0s 102us/step - loss: 134937447.2190 - val_loss: 1320703655.2982\n",
      "Epoch 1007/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 163974813.4529 - val_loss: 1221512775.8596\n",
      "Epoch 1008/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 154985337.0695 - val_loss: 1340040926.3158\n",
      "Epoch 1009/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 134760267.8558 - val_loss: 1186361177.2632\n",
      "Epoch 1010/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 122595522.7089 - val_loss: 1276172737.6842\n",
      "Epoch 1011/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 169336708.1935 - val_loss: 1162869204.2105\n",
      "Epoch 1012/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 145542287.5989 - val_loss: 1248540623.7193\n",
      "Epoch 1013/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 144949566.5084 - val_loss: 1168806736.8421\n",
      "Epoch 1014/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 234044066.1390 - val_loss: 1223622905.2632\n",
      "Epoch 1015/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 210206808.5488 - val_loss: 1166038812.0702\n",
      "Epoch 1016/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 136795037.5022 - val_loss: 1179614806.4561\n",
      "Epoch 1017/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 133999454.3606 - val_loss: 1185824491.7895\n",
      "Epoch 1018/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 116809930.7863 - val_loss: 1224235783.8596\n",
      "Epoch 1019/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 160575160.8936 - val_loss: 1212597930.6667\n",
      "Epoch 1020/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 191853499.8909 - val_loss: 1144380997.0526\n",
      "Epoch 1021/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 136533022.5576 - val_loss: 1231068367.7193\n",
      "Epoch 1022/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 192043673.7379 - val_loss: 1145799471.1579\n",
      "Epoch 1023/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 150276687.3808 - val_loss: 1144317102.5965\n",
      "Epoch 1024/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 128651314.5400 - val_loss: 1184337446.1754\n",
      "Epoch 1025/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 129623876.0880 - val_loss: 1184810368.5614\n",
      "Epoch 1026/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 130452600.4011 - val_loss: 1165701294.5965\n",
      "Epoch 1027/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 106535708.1583 - val_loss: 1224415021.4737\n",
      "Epoch 1028/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 178253622.9376 - val_loss: 1280287969.1228\n",
      "Epoch 1029/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 144178556.0457 - val_loss: 1176009176.7018\n",
      "Epoch 1030/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 166592011.9191 - val_loss: 1185921095.2982\n",
      "Epoch 1031/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 118525917.7168 - val_loss: 1180302554.3860\n",
      "Epoch 1032/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 137912392.2322 - val_loss: 1215091028.7719\n",
      "Epoch 1033/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 134833479.2049 - val_loss: 1244074737.9649\n",
      "Epoch 1034/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 128780220.5523 - val_loss: 1158964555.7895\n",
      "Epoch 1035/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 111524150.6631 - val_loss: 1167319991.5789\n",
      "Epoch 1036/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 115071227.8698 - val_loss: 1295587523.9298\n",
      "Epoch 1037/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 126972212.4257 - val_loss: 1149609766.1754\n",
      "Epoch 1038/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 114168454.1284 - val_loss: 1233879717.0526\n",
      "Epoch 1039/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 126783484.8408 - val_loss: 1356244624.8421\n",
      "Epoch 1040/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 172638865.7133 - val_loss: 1243319770.9474\n",
      "Epoch 1041/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 134478465.7731 - val_loss: 1246357415.8596\n",
      "Epoch 1042/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 150851722.0475 - val_loss: 1320692281.2632\n",
      "Epoch 1043/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 132232678.6174 - val_loss: 1209218998.4561\n",
      "Epoch 1044/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 108528357.1715 - val_loss: 1196762852.4912\n",
      "Epoch 1045/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 113378897.8294 - val_loss: 1272802375.2982\n",
      "Epoch 1046/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 113489887.5145 - val_loss: 1233881665.6842\n",
      "Epoch 1047/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 110866652.6719 - val_loss: 1194493113.2632\n",
      "Epoch 1048/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 119692257.2313 - val_loss: 1200768321.1228\n",
      "Epoch 1049/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 121464378.8707 - val_loss: 1173242687.4386\n",
      "Epoch 1050/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 120777914.0827 - val_loss: 1232339028.7719\n",
      "Epoch 1051/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 275275103.2964 - val_loss: 1253862940.0702\n",
      "Epoch 1052/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 192661589.2067 - val_loss: 1224722194.5263\n",
      "Epoch 1053/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 138172629.3826 - val_loss: 1215514222.5965\n",
      "Epoch 1054/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 83us/step - loss: 149833672.9428 - val_loss: 1203022294.4561\n",
      "Epoch 1055/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 127168302.4239 - val_loss: 1200062409.5439\n",
      "Epoch 1056/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 144250457.2665 - val_loss: 1210454021.0526\n",
      "Epoch 1057/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 137719605.0660 - val_loss: 1214433210.3860\n",
      "Epoch 1058/2000\n",
      "1137/1137 [==============================] - 0s 103us/step - loss: 139362654.5224 - val_loss: 1167512934.1754\n",
      "Epoch 1059/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 135771040.7740 - val_loss: 1201387864.7018\n",
      "Epoch 1060/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 121845673.6007 - val_loss: 1216227806.8772\n",
      "Epoch 1061/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 116733844.3694 - val_loss: 1176732641.6842\n",
      "Epoch 1062/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 146933368.0070 - val_loss: 1211205982.3158\n",
      "Epoch 1063/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 139858048.5629 - val_loss: 1184099280.2807\n",
      "Epoch 1064/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 181445396.0598 - val_loss: 1174451809.1228\n",
      "Epoch 1065/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 137807895.4230 - val_loss: 1148219472.2807\n",
      "Epoch 1066/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 116558435.5673 - val_loss: 1284524764.0702\n",
      "Epoch 1067/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 120496277.7872 - val_loss: 1169314526.8772\n",
      "Epoch 1068/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 103410267.3632 - val_loss: 1173927435.7895\n",
      "Epoch 1069/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 132939408.8725 - val_loss: 1199803571.0877\n",
      "Epoch 1070/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 118704882.0862 - val_loss: 1224408907.2281\n",
      "Epoch 1071/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 136057541.1926 - val_loss: 1236936198.7368\n",
      "Epoch 1072/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 125476505.8575 - val_loss: 1173678650.9474\n",
      "Epoch 1073/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 105455383.2964 - val_loss: 1208648789.3333\n",
      "Epoch 1074/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 223253577.6887 - val_loss: 1153425171.6491\n",
      "Epoch 1075/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 139023927.7115 - val_loss: 1159782349.4737\n",
      "Epoch 1076/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 130150669.8751 - val_loss: 1246036788.2105\n",
      "Epoch 1077/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 129636755.8417 - val_loss: 1195956707.9298\n",
      "Epoch 1078/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 112015401.6113 - val_loss: 1149381468.6316\n",
      "Epoch 1079/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 101475799.8804 - val_loss: 1206375654.7368\n",
      "Epoch 1080/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 109155240.6544 - val_loss: 1248112624.8421\n",
      "Epoch 1081/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 157389317.4318 - val_loss: 1250877524.2105\n",
      "Epoch 1082/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 163762574.9868 - val_loss: 1137869425.4035\n",
      "Epoch 1083/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 119311387.0255 - val_loss: 1195043096.1404\n",
      "Epoch 1084/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 105261003.8980 - val_loss: 1170587389.1930\n",
      "Epoch 1085/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 105539156.2498 - val_loss: 1224597888.5614\n",
      "Epoch 1086/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 128180631.3738 - val_loss: 1159648629.8947\n",
      "Epoch 1087/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 109649108.0317 - val_loss: 1186543000.7018\n",
      "Epoch 1088/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 107435917.4318 - val_loss: 1223267214.0351\n",
      "Epoch 1089/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 116820323.1381 - val_loss: 1130515573.8947\n",
      "Epoch 1090/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 114997522.7018 - val_loss: 1233772522.1053\n",
      "Epoch 1091/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 109583767.2049 - val_loss: 1173966810.3860\n",
      "Epoch 1092/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 103750809.1539 - val_loss: 1179055324.0702\n",
      "Epoch 1093/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 134920449.5479 - val_loss: 1432907008.0000\n",
      "Epoch 1094/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 259490157.6077 - val_loss: 1173378173.1930\n",
      "Epoch 1095/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 260480117.8821 - val_loss: 1534255744.0000\n",
      "Epoch 1096/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 323202508.8056 - val_loss: 1224604596.2105\n",
      "Epoch 1097/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 180412421.7555 - val_loss: 1146073216.0000\n",
      "Epoch 1098/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 121883485.6007 - val_loss: 1236685340.0702\n",
      "Epoch 1099/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 106513920.1548 - val_loss: 1215189960.4211\n",
      "Epoch 1100/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 118007133.8470 - val_loss: 1226306997.3333\n",
      "Epoch 1101/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 104391860.4186 - val_loss: 1182122081.1228\n",
      "Epoch 1102/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 121559932.6227 - val_loss: 1186180768.5614\n",
      "Epoch 1103/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 161366776.2040 - val_loss: 1181989998.5965\n",
      "Epoch 1104/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 113840091.5954 - val_loss: 1183332775.2982\n",
      "Epoch 1105/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 113013277.8751 - val_loss: 1153530472.4211\n",
      "Epoch 1106/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 104742494.2058 - val_loss: 1207835582.3158\n",
      "Epoch 1107/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 135855382.9235 - val_loss: 1258474943.4386\n",
      "Epoch 1108/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 134173011.5321 - val_loss: 1213309785.8246\n",
      "Epoch 1109/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 109780001.9631 - val_loss: 1162103568.2807\n",
      "Epoch 1110/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 118998319.9296 - val_loss: 1179512858.3860\n",
      "Epoch 1111/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 119733083.8417 - val_loss: 1200754393.8246\n",
      "Epoch 1112/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 115577758.9938 - val_loss: 1203102968.1404\n",
      "Epoch 1113/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 121530476.9675 - val_loss: 1196898754.8070\n",
      "Epoch 1114/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 102309759.7995 - val_loss: 1174966675.6491\n",
      "Epoch 1115/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 107121338.5893 - val_loss: 1195096209.9649\n",
      "Epoch 1116/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 97780939.9402 - val_loss: 1200193809.4035\n",
      "Epoch 1117/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 104211349.7344 - val_loss: 1216247965.7544\n",
      "Epoch 1118/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 107319661.6640 - val_loss: 1192143248.2807\n",
      "Epoch 1119/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 89us/step - loss: 123574139.4406 - val_loss: 1210356005.6140\n",
      "Epoch 1120/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 109650800.3518 - val_loss: 1234840897.1228\n",
      "Epoch 1121/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 125674689.2806 - val_loss: 1188760769.6842\n",
      "Epoch 1122/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 108956348.9041 - val_loss: 1166966172.6316\n",
      "Epoch 1123/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 153263342.4943 - val_loss: 1435609155.3684\n",
      "Epoch 1124/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 301903384.4573 - val_loss: 1539169003.7895\n",
      "Epoch 1125/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 198327759.2753 - val_loss: 1201289190.1754\n",
      "Epoch 1126/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 151068276.2709 - val_loss: 1230372691.6491\n",
      "Epoch 1127/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 136136946.6737 - val_loss: 1258072602.9474\n",
      "Epoch 1128/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 117240508.6860 - val_loss: 1248411393.1228\n",
      "Epoch 1129/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 106639487.8593 - val_loss: 1199608608.0000\n",
      "Epoch 1130/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 102271171.1170 - val_loss: 1202736766.3158\n",
      "Epoch 1131/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 111478232.9710 - val_loss: 1255101222.7368\n",
      "Epoch 1132/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 127048222.6772 - val_loss: 1212410481.9649\n",
      "Epoch 1133/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 197158351.7467 - val_loss: 1267007500.3509\n",
      "Epoch 1134/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 119312101.7696 - val_loss: 1143403087.1579\n",
      "Epoch 1135/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 100959954.8707 - val_loss: 1204655379.6491\n",
      "Epoch 1136/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 101721156.2498 - val_loss: 1219596456.4211\n",
      "Epoch 1137/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 115985941.7977 - val_loss: 1210115461.6140\n",
      "Epoch 1138/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 148835895.3597 - val_loss: 1173553445.6140\n",
      "Epoch 1139/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 128550254.3676 - val_loss: 1251022361.2632\n",
      "Epoch 1140/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 95264169.7379 - val_loss: 1216853340.6316\n",
      "Epoch 1141/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 102288980.0950 - val_loss: 1230442146.2456\n",
      "Epoch 1142/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 95759506.7652 - val_loss: 1241809875.6491\n",
      "Epoch 1143/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 93220809.2735 - val_loss: 1219013524.7719\n",
      "Epoch 1144/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 135478950.3184 - val_loss: 1231044720.8421\n",
      "Epoch 1145/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 127342572.7493 - val_loss: 1229636460.9123\n",
      "Epoch 1146/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 108593247.0220 - val_loss: 1210334313.5439\n",
      "Epoch 1147/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 121906024.6614 - val_loss: 1340043320.1404\n",
      "Epoch 1148/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 129612753.5972 - val_loss: 1292521282.8070\n",
      "Epoch 1149/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 130494428.6438 - val_loss: 1265655199.4386\n",
      "Epoch 1150/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 157734588.5875 - val_loss: 1188603510.4561\n",
      "Epoch 1151/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 128024272.5910 - val_loss: 1251637482.6667\n",
      "Epoch 1152/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 112834046.3184 - val_loss: 1149646253.4737\n",
      "Epoch 1153/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 93380696.1337 - val_loss: 1219859101.1930\n",
      "Epoch 1154/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 97173705.2032 - val_loss: 1217023289.2632\n",
      "Epoch 1155/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 107419538.4063 - val_loss: 1253311123.0877\n",
      "Epoch 1156/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 122175826.5330 - val_loss: 1209164467.6491\n",
      "Epoch 1157/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 94907599.4371 - val_loss: 1250777793.1228\n",
      "Epoch 1158/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 126144976.6332 - val_loss: 1466239765.3333\n",
      "Epoch 1159/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 190009803.9894 - val_loss: 1200667976.4211\n",
      "Epoch 1160/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 162795443.8980 - val_loss: 1274098891.2281\n",
      "Epoch 1161/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 133774197.4952 - val_loss: 1216616184.1404\n",
      "Epoch 1162/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 125438621.0449 - val_loss: 1281933504.0000\n",
      "Epoch 1163/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 162640485.3052 - val_loss: 1335121350.7368\n",
      "Epoch 1164/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 138683402.5963 - val_loss: 1219790489.2632\n",
      "Epoch 1165/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 114141166.1249 - val_loss: 1205693155.3684\n",
      "Epoch 1166/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 115960250.7933 - val_loss: 1193533891.9298\n",
      "Epoch 1167/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 108638834.8989 - val_loss: 1270945080.7018\n",
      "Epoch 1168/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 99961561.5866 - val_loss: 1181665328.8421\n",
      "Epoch 1169/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 97909580.1161 - val_loss: 1229436894.3158\n",
      "Epoch 1170/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 108451944.0457 - val_loss: 1246476893.7544\n",
      "Epoch 1171/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 98301652.6016 - val_loss: 1214916947.6491\n",
      "Epoch 1172/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 97682528.2392 - val_loss: 1205964723.0877\n",
      "Epoch 1173/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 106863062.5998 - val_loss: 1275333558.4561\n",
      "Epoch 1174/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 113644708.1091 - val_loss: 1208860848.8421\n",
      "Epoch 1175/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 102912655.9296 - val_loss: 1226896476.6316\n",
      "Epoch 1176/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 102050341.7907 - val_loss: 1299576384.0000\n",
      "Epoch 1177/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 110851368.5418 - val_loss: 1230290347.7895\n",
      "Epoch 1178/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 102871249.4811 - val_loss: 1249022363.5088\n",
      "Epoch 1179/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 99335631.0923 - val_loss: 1271113772.9123\n",
      "Epoch 1180/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 95202243.0888 - val_loss: 1189839311.7193\n",
      "Epoch 1181/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 124928540.1266 - val_loss: 1289359280.2807\n",
      "Epoch 1182/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 117483315.8382 - val_loss: 1304254038.4561\n",
      "Epoch 1183/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 137222339.4758 - val_loss: 1307338519.5789\n",
      "Epoch 1184/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 101899401.6887 - val_loss: 1247613911.0175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1185/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 119790287.8382 - val_loss: 1204813414.1754\n",
      "Epoch 1186/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 134448445.1856 - val_loss: 1230740974.0351\n",
      "Epoch 1187/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 117424778.9692 - val_loss: 1343745547.2281\n",
      "Epoch 1188/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 126931369.1539 - val_loss: 1312153237.8947\n",
      "Epoch 1189/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 132451095.8171 - val_loss: 1216045658.9474\n",
      "Epoch 1190/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 95520916.2498 - val_loss: 1212425006.0351\n",
      "Epoch 1191/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 95474898.1179 - val_loss: 1211452047.7193\n",
      "Epoch 1192/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 130825467.9191 - val_loss: 1271444257.6842\n",
      "Epoch 1193/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 100603734.9938 - val_loss: 1284038744.7018\n",
      "Epoch 1194/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 107979697.3298 - val_loss: 1216804497.9649\n",
      "Epoch 1195/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 98281473.0906 - val_loss: 1307696609.6842\n",
      "Epoch 1196/2000\n",
      "1137/1137 [==============================] - ETA: 0s - loss: 92053089.647 - 0s 75us/step - loss: 92954683.1170 - val_loss: 1254325607.8596\n",
      "Epoch 1197/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 110492172.1724 - val_loss: 1201847918.0351\n",
      "Epoch 1198/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 123017957.1082 - val_loss: 1215126542.0351\n",
      "Epoch 1199/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 111847637.9033 - val_loss: 1248910144.0000\n",
      "Epoch 1200/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 122479527.3456 - val_loss: 1182793515.7895\n",
      "Epoch 1201/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 96196021.1574 - val_loss: 1191855522.8070\n",
      "Epoch 1202/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 126651686.6631 - val_loss: 1248766950.1754\n",
      "Epoch 1203/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 119376474.8144 - val_loss: 1241371560.4211\n",
      "Epoch 1204/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 100463109.6429 - val_loss: 1275851019.2281\n",
      "Epoch 1205/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 104215092.4749 - val_loss: 1210195903.4386\n",
      "Epoch 1206/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 90089007.5004 - val_loss: 1170949059.9298\n",
      "Epoch 1207/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 115404753.3861 - val_loss: 1242052411.5088\n",
      "Epoch 1208/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 127245909.6711 - val_loss: 1211372794.3860\n",
      "Epoch 1209/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 133927470.3113 - val_loss: 1228291239.2982\n",
      "Epoch 1210/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 110982486.2762 - val_loss: 1207179622.1754\n",
      "Epoch 1211/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 109211010.5259 - val_loss: 1237724308.2105\n",
      "Epoch 1212/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 106494808.5629 - val_loss: 1239155575.0175\n",
      "Epoch 1213/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 110944341.8716 - val_loss: 1236787142.7368\n",
      "Epoch 1214/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 91053776.7458 - val_loss: 1242770640.8421\n",
      "Epoch 1215/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 84427147.5743 - val_loss: 1231222485.8947\n",
      "Epoch 1216/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 98652340.2533 - val_loss: 1317260701.1930\n",
      "Epoch 1217/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 103292985.0836 - val_loss: 1197902688.0000\n",
      "Epoch 1218/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 96645948.9815 - val_loss: 1250941607.2982\n",
      "Epoch 1219/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 107054000.9639 - val_loss: 1267964248.7018\n",
      "Epoch 1220/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 110086730.0334 - val_loss: 1223516061.1930\n",
      "Epoch 1221/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 111154503.8839 - val_loss: 1281612248.1404\n",
      "Epoch 1222/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 133272177.9912 - val_loss: 1105565931.7895\n",
      "Epoch 1223/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 95846577.3157 - val_loss: 1229843937.6842\n",
      "Epoch 1224/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 91783630.9376 - val_loss: 1179361280.5614\n",
      "Epoch 1225/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 106621755.4195 - val_loss: 1222929060.4912\n",
      "Epoch 1226/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 109775515.0185 - val_loss: 1205756740.4912\n",
      "Epoch 1227/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 117609604.2709 - val_loss: 1227230443.2281\n",
      "Epoch 1228/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 84948292.5734 - val_loss: 1256970406.7368\n",
      "Epoch 1229/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 89604231.4512 - val_loss: 1237957308.6316\n",
      "Epoch 1230/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 96687940.5242 - val_loss: 1193008684.3509\n",
      "Epoch 1231/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 98878529.3439 - val_loss: 1194967365.0526\n",
      "Epoch 1232/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 83238671.5145 - val_loss: 1287951614.3158\n",
      "Epoch 1233/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 87027655.8804 - val_loss: 1269926744.1404\n",
      "Epoch 1234/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 83275863.2260 - val_loss: 1327836328.4211\n",
      "Epoch 1235/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 112306385.9279 - val_loss: 1232705528.7018\n",
      "Epoch 1236/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 91321339.5462 - val_loss: 1206533917.1930\n",
      "Epoch 1237/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 87774576.3659 - val_loss: 1256258855.2982\n",
      "Epoch 1238/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 86288123.0818 - val_loss: 1247165460.2105\n",
      "Epoch 1239/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 91802976.9147 - val_loss: 1287271739.5088\n",
      "Epoch 1240/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 103291674.0193 - val_loss: 1296170282.6667\n",
      "Epoch 1241/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 91713927.1416 - val_loss: 1266457813.8947\n",
      "Epoch 1242/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 116363389.9877 - val_loss: 1306757593.2632\n",
      "Epoch 1243/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 128793406.9516 - val_loss: 1232725889.1228\n",
      "Epoch 1244/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 84554477.4107 - val_loss: 1204194351.1579\n",
      "Epoch 1245/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 92137122.5189 - val_loss: 1227225053.1930\n",
      "Epoch 1246/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 92040816.5770 - val_loss: 1314539380.2105\n",
      "Epoch 1247/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 106147524.1231 - val_loss: 1246316796.0702\n",
      "Epoch 1248/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 104062393.9912 - val_loss: 1211195902.3158\n",
      "Epoch 1249/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 99365469.5303 - val_loss: 1233792768.0000\n",
      "Epoch 1250/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 93us/step - loss: 86860858.9270 - val_loss: 1253925273.8246\n",
      "Epoch 1251/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 82403843.4195 - val_loss: 1265130080.5614\n",
      "Epoch 1252/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 84121167.4653 - val_loss: 1207651964.6316\n",
      "Epoch 1253/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 98899080.7071 - val_loss: 1355356596.7719\n",
      "Epoch 1254/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 98594332.8619 - val_loss: 1250929127.2982\n",
      "Epoch 1255/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 85254478.1179 - val_loss: 1232399512.7018\n",
      "Epoch 1256/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 77812730.9200 - val_loss: 1209150151.8596\n",
      "Epoch 1257/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 79422201.2524 - val_loss: 1204358039.0175\n",
      "Epoch 1258/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 77631136.7177 - val_loss: 1276166806.4561\n",
      "Epoch 1259/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 82677957.8329 - val_loss: 1285073641.5439\n",
      "Epoch 1260/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 85923998.3958 - val_loss: 1242036771.3684\n",
      "Epoch 1261/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 89864927.0677 - val_loss: 1291224234.6667\n",
      "Epoch 1262/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 80347661.9103 - val_loss: 1255419600.2807\n",
      "Epoch 1263/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 89215777.0273 - val_loss: 1336449052.6316\n",
      "Epoch 1264/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 83966038.2058 - val_loss: 1202662784.5614\n",
      "Epoch 1265/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 95212856.3729 - val_loss: 1365913095.2982\n",
      "Epoch 1266/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 211060288.0352 - val_loss: 1403529804.9123\n",
      "Epoch 1267/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 183609014.0510 - val_loss: 1262396661.8947\n",
      "Epoch 1268/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 168482026.3149 - val_loss: 1198712725.3333\n",
      "Epoch 1269/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 112125643.5884 - val_loss: 1191343742.3158\n",
      "Epoch 1270/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 98399857.0554 - val_loss: 1275568760.7018\n",
      "Epoch 1271/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 94241762.8989 - val_loss: 1282062768.2807\n",
      "Epoch 1272/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 96512998.9692 - val_loss: 1259697294.5965\n",
      "Epoch 1273/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 106836034.0123 - val_loss: 1309147047.8596\n",
      "Epoch 1274/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 99806452.0246 - val_loss: 1217128072.9825\n",
      "Epoch 1275/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 109281938.4345 - val_loss: 1221866287.1579\n",
      "Epoch 1276/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 95090589.0660 - val_loss: 1267086084.4912\n",
      "Epoch 1277/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 79268808.9569 - val_loss: 1229172460.9123\n",
      "Epoch 1278/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 89709696.7599 - val_loss: 1248624533.3333\n",
      "Epoch 1279/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 104102125.8751 - val_loss: 1296065886.3158\n",
      "Epoch 1280/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 99065452.7142 - val_loss: 1220115122.5263\n",
      "Epoch 1281/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 88907519.4442 - val_loss: 1231766525.7544\n",
      "Epoch 1282/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 95474493.2841 - val_loss: 1275915331.3684\n",
      "Epoch 1283/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 102271398.7757 - val_loss: 1241448583.8596\n",
      "Epoch 1284/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 93774406.7124 - val_loss: 1150822066.5263\n",
      "Epoch 1285/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 93524109.6570 - val_loss: 1256026196.2105\n",
      "Epoch 1286/2000\n",
      "1137/1137 [==============================] - 0s 100us/step - loss: 173580295.2542 - val_loss: 1533179332.4912\n",
      "Epoch 1287/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 250968162.7546 - val_loss: 1215159651.9298\n",
      "Epoch 1288/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 120497180.9323 - val_loss: 1234638060.9123\n",
      "Epoch 1289/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 149083606.8672 - val_loss: 1197459201.1228\n",
      "Epoch 1290/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 100367774.3395 - val_loss: 1295566053.0526\n",
      "Epoch 1291/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 85322863.9226 - val_loss: 1206023441.9649\n",
      "Epoch 1292/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 95235510.2410 - val_loss: 1384729864.9825\n",
      "Epoch 1293/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 108418909.8470 - val_loss: 1247182296.7018\n",
      "Epoch 1294/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 84794668.6931 - val_loss: 1243178959.7193\n",
      "Epoch 1295/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 75949729.3439 - val_loss: 1266433667.3684\n",
      "Epoch 1296/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 90577414.1706 - val_loss: 1225289758.8772\n",
      "Epoch 1297/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 85135180.3835 - val_loss: 1208554673.9649\n",
      "Epoch 1298/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 75431363.8347 - val_loss: 1224919237.6140\n",
      "Epoch 1299/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 73970703.9507 - val_loss: 1224967541.8947\n",
      "Epoch 1300/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 73271117.3404 - val_loss: 1209115301.6140\n",
      "Epoch 1301/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 90584206.3571 - val_loss: 1401948678.7368\n",
      "Epoch 1302/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 130028648.5840 - val_loss: 1231766929.4035\n",
      "Epoch 1303/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 110389483.3421 - val_loss: 1228026526.3158\n",
      "Epoch 1304/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 99628157.8892 - val_loss: 1225862660.4912\n",
      "Epoch 1305/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 112903518.6350 - val_loss: 1233141487.7193\n",
      "Epoch 1306/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 97566184.8936 - val_loss: 1286112977.9649\n",
      "Epoch 1307/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 88906688.4327 - val_loss: 1226773431.0175\n",
      "Epoch 1308/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 77480208.2955 - val_loss: 1268126255.1579\n",
      "Epoch 1309/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 155716617.0906 - val_loss: 1375062891.7895\n",
      "Epoch 1310/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 185729927.6482 - val_loss: 1236911571.0877\n",
      "Epoch 1311/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 144621528.0915 - val_loss: 1230733187.3684\n",
      "Epoch 1312/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 215935212.4116 - val_loss: 1373374780.6316\n",
      "Epoch 1313/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 198632595.8980 - val_loss: 1198282160.2807\n",
      "Epoch 1314/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 105217336.3870 - val_loss: 1196833808.8421\n",
      "Epoch 1315/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 91386799.2823 - val_loss: 1195050017.6842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1316/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 89302627.2929 - val_loss: 1259721534.8772\n",
      "Epoch 1317/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 89543922.4415 - val_loss: 1186220029.7544\n",
      "Epoch 1318/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 77609205.1082 - val_loss: 1213286333.7544\n",
      "Epoch 1319/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 75586231.3949 - val_loss: 1224696754.5263\n",
      "Epoch 1320/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 83567885.9912 - val_loss: 1235639040.0000\n",
      "Epoch 1321/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 84426424.1759 - val_loss: 1231056020.2105\n",
      "Epoch 1322/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 89517877.2419 - val_loss: 1192064651.2281\n",
      "Epoch 1323/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 96334812.7493 - val_loss: 1229773246.8772\n",
      "Epoch 1324/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 80483972.7300 - val_loss: 1216800224.5614\n",
      "Epoch 1325/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 74594172.6227 - val_loss: 1202420046.5965\n",
      "Epoch 1326/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 82801664.1478 - val_loss: 1200774123.7895\n",
      "Epoch 1327/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 86200968.9217 - val_loss: 1217557222.1754\n",
      "Epoch 1328/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 84616303.0079 - val_loss: 1263963543.5789\n",
      "Epoch 1329/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 86470501.9947 - val_loss: 1291289680.2807\n",
      "Epoch 1330/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 121902717.6570 - val_loss: 1310090842.9474\n",
      "Epoch 1331/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 141692620.5805 - val_loss: 1184525726.3158\n",
      "Epoch 1332/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 109108793.2313 - val_loss: 1237719180.3509\n",
      "Epoch 1333/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 93530898.0967 - val_loss: 1250284104.9825\n",
      "Epoch 1334/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 87204013.0730 - val_loss: 1352085167.1579\n",
      "Epoch 1335/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 136188207.5004 - val_loss: 1346786571.2281\n",
      "Epoch 1336/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 84797484.5418 - val_loss: 1228553784.1404\n",
      "Epoch 1337/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 82586613.5409 - val_loss: 1273470009.2632\n",
      "Epoch 1338/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 80729920.5136 - val_loss: 1169261859.3684\n",
      "Epoch 1339/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 88273851.8065 - val_loss: 1244128565.8947\n",
      "Epoch 1340/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 81039652.9182 - val_loss: 1254161892.4912\n",
      "Epoch 1341/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 70984376.5453 - val_loss: 1233856398.5965\n",
      "Epoch 1342/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 72208945.4318 - val_loss: 1221681321.5439\n",
      "Epoch 1343/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 85439028.4257 - val_loss: 1210684938.1053\n",
      "Epoch 1344/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 118084886.6772 - val_loss: 1227963868.0702\n",
      "Epoch 1345/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 143949908.0668 - val_loss: 1201180244.7719\n",
      "Epoch 1346/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 97216747.2718 - val_loss: 1245459933.1930\n",
      "Epoch 1347/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 88070972.1724 - val_loss: 1231663333.0526\n",
      "Epoch 1348/2000\n",
      "1137/1137 [==============================] - 0s 63us/step - loss: 80042991.7291 - val_loss: 1245968894.8772\n",
      "Epoch 1349/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 83359066.1460 - val_loss: 1241486773.3333\n",
      "Epoch 1350/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 80755200.0352 - val_loss: 1245137275.5088\n",
      "Epoch 1351/2000\n",
      "1137/1137 [==============================] - 0s 60us/step - loss: 81371481.7625 - val_loss: 1213350868.7719\n",
      "Epoch 1352/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 72512434.0193 - val_loss: 1276227934.3158\n",
      "Epoch 1353/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 73434415.3773 - val_loss: 1226796266.6667\n",
      "Epoch 1354/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 94106354.8004 - val_loss: 1208407699.0877\n",
      "Epoch 1355/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 111760704.8303 - val_loss: 1280744128.0000\n",
      "Epoch 1356/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 156101893.7977 - val_loss: 1312660590.0351\n",
      "Epoch 1357/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 98826451.4512 - val_loss: 1220311974.1754\n",
      "Epoch 1358/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 86303473.0765 - val_loss: 1251384971.2281\n",
      "Epoch 1359/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 125571357.2348 - val_loss: 1323216688.2807\n",
      "Epoch 1360/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 117228243.4195 - val_loss: 1360906714.9474\n",
      "Epoch 1361/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 143452030.5506 - val_loss: 1329177113.8246\n",
      "Epoch 1362/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 103325373.0730 - val_loss: 1228186961.9649\n",
      "Epoch 1363/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 76030340.2955 - val_loss: 1227537156.4912\n",
      "Epoch 1364/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 76478270.7828 - val_loss: 1236534736.8421\n",
      "Epoch 1365/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 75467280.2533 - val_loss: 1229998687.4386\n",
      "Epoch 1366/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 92694920.9499 - val_loss: 1268875104.5614\n",
      "Epoch 1367/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 96801203.4828 - val_loss: 1226674560.0000\n",
      "Epoch 1368/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 91846283.0044 - val_loss: 1227562094.0351\n",
      "Epoch 1369/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 72666588.5488 - val_loss: 1191706698.1053\n",
      "Epoch 1370/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 83704261.9666 - val_loss: 1250878731.2281\n",
      "Epoch 1371/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 85889226.8144 - val_loss: 1240075051.7895\n",
      "Epoch 1372/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 75437806.8215 - val_loss: 1211889309.7544\n",
      "Epoch 1373/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 79651495.0923 - val_loss: 1252094432.5614\n",
      "Epoch 1374/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 73263588.6790 - val_loss: 1197550312.4211\n",
      "Epoch 1375/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 82425688.1091 - val_loss: 1266630634.6667\n",
      "Epoch 1376/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 69054443.9226 - val_loss: 1204286848.0000\n",
      "Epoch 1377/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 69661913.8223 - val_loss: 1231726068.7719\n",
      "Epoch 1378/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 72198410.8320 - val_loss: 1207393391.1579\n",
      "Epoch 1379/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 69210739.5462 - val_loss: 1256222040.7018\n",
      "Epoch 1380/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 93188779.5251 - val_loss: 1294843231.4386\n",
      "Epoch 1381/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 77871454.5400 - val_loss: 1212629589.3333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1382/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 75780222.2551 - val_loss: 1243840090.9474\n",
      "Epoch 1383/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 76213064.8830 - val_loss: 1248687842.8070\n",
      "Epoch 1384/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 70627003.5391 - val_loss: 1223309996.9123\n",
      "Epoch 1385/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 72962692.9041 - val_loss: 1229347210.1053\n",
      "Epoch 1386/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 74889657.4529 - val_loss: 1198197346.8070\n",
      "Epoch 1387/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 107490774.0792 - val_loss: 1196198872.7018\n",
      "Epoch 1388/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 150394435.6869 - val_loss: 1179328878.0351\n",
      "Epoch 1389/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 89844976.3940 - val_loss: 1307866076.0702\n",
      "Epoch 1390/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 98385456.1126 - val_loss: 1245363944.4211\n",
      "Epoch 1391/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 75099547.3914 - val_loss: 1207437252.4912\n",
      "Epoch 1392/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 68727840.7669 - val_loss: 1302366472.9825\n",
      "Epoch 1393/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 110785310.6350 - val_loss: 1263252006.1754\n",
      "Epoch 1394/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 72620934.5928 - val_loss: 1231661234.5263\n",
      "Epoch 1395/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 80414816.7599 - val_loss: 1263075394.2456\n",
      "Epoch 1396/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 83648568.2674 - val_loss: 1273238471.8596\n",
      "Epoch 1397/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 75793581.3052 - val_loss: 1193398250.6667\n",
      "Epoch 1398/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 69841351.9930 - val_loss: 1208426162.5263\n",
      "Epoch 1399/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 77705419.0748 - val_loss: 1190246347.2281\n",
      "Epoch 1400/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 90745424.5558 - val_loss: 1241686312.4211\n",
      "Epoch 1401/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 86933612.9006 - val_loss: 1269726422.4561\n",
      "Epoch 1402/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 87194486.1284 - val_loss: 1215043039.4386\n",
      "Epoch 1403/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 78475592.2392 - val_loss: 1213426829.4737\n",
      "Epoch 1404/2000\n",
      "1137/1137 [==============================] - 0s 103us/step - loss: 88633608.9921 - val_loss: 1176676750.0351\n",
      "Epoch 1405/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 67488537.5163 - val_loss: 1186602129.9649\n",
      "Epoch 1406/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 107032393.0484 - val_loss: 1235995782.7368\n",
      "Epoch 1407/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 90393447.5708 - val_loss: 1211145591.0175\n",
      "Epoch 1408/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 71923147.1029 - val_loss: 1218850198.4561\n",
      "Epoch 1409/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 72487006.1777 - val_loss: 1249476312.7018\n",
      "Epoch 1410/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 120120110.4908 - val_loss: 1204578991.1579\n",
      "Epoch 1411/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 126505865.2313 - val_loss: 1319879099.5088\n",
      "Epoch 1412/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 141211431.5215 - val_loss: 1256942232.7018\n",
      "Epoch 1413/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 120093404.5383 - val_loss: 1139904378.3860\n",
      "Epoch 1414/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 114197258.5998 - val_loss: 1236204716.9123\n",
      "Epoch 1415/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 105534963.5743 - val_loss: 1244663318.4561\n",
      "Epoch 1416/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 80052817.3650 - val_loss: 1205496231.2982\n",
      "Epoch 1417/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 72318115.6974 - val_loss: 1197689011.0877\n",
      "Epoch 1418/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 65920684.2111 - val_loss: 1157907554.8070\n",
      "Epoch 1419/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 72791974.2480 - val_loss: 1210301739.7895\n",
      "Epoch 1420/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 72999717.0449 - val_loss: 1333899805.1930\n",
      "Epoch 1421/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 96412295.3527 - val_loss: 1231988185.8246\n",
      "Epoch 1422/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 68390591.1205 - val_loss: 1214532340.7719\n",
      "Epoch 1423/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 66960437.9807 - val_loss: 1230469655.5789\n",
      "Epoch 1424/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 69102894.0862 - val_loss: 1226016343.5789\n",
      "Epoch 1425/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 75960801.1539 - val_loss: 1268504928.5614\n",
      "Epoch 1426/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 81071317.6570 - val_loss: 1317557842.5263\n",
      "Epoch 1427/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 167630710.2551 - val_loss: 1168245153.6842\n",
      "Epoch 1428/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 100381420.7071 - val_loss: 1299242977.6842\n",
      "Epoch 1429/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 81026873.1891 - val_loss: 1272690612.7719\n",
      "Epoch 1430/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 75855824.2463 - val_loss: 1197831583.4386\n",
      "Epoch 1431/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 75307346.4978 - val_loss: 1235404071.2982\n",
      "Epoch 1432/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 72575961.4213 - val_loss: 1212858644.2105\n",
      "Epoch 1433/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 81682161.4881 - val_loss: 1267425893.0526\n",
      "Epoch 1434/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 81533221.3544 - val_loss: 1202187434.6667\n",
      "Epoch 1435/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 73635108.7423 - val_loss: 1217678251.7895\n",
      "Epoch 1436/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 72272514.8355 - val_loss: 1236067787.2281\n",
      "Epoch 1437/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 84185613.4952 - val_loss: 1215397944.1404\n",
      "Epoch 1438/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 66342692.4116 - val_loss: 1270015287.0175\n",
      "Epoch 1439/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 69644403.0114 - val_loss: 1245440428.9123\n",
      "Epoch 1440/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 83623394.8918 - val_loss: 1221456332.3509\n",
      "Epoch 1441/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 82498577.4565 - val_loss: 1204190788.4912\n",
      "Epoch 1442/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 79767823.2014 - val_loss: 1238827724.3509\n",
      "Epoch 1443/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 68911897.1680 - val_loss: 1212328230.1754\n",
      "Epoch 1444/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 72354039.1451 - val_loss: 1198977657.2632\n",
      "Epoch 1445/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 84335684.7599 - val_loss: 1211485387.2281\n",
      "Epoch 1446/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 66142870.5717 - val_loss: 1228816875.7895\n",
      "Epoch 1447/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 59896735.2331 - val_loss: 1199905002.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1448/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 63739471.3703 - val_loss: 1216852106.1053\n",
      "Epoch 1449/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 67836331.8804 - val_loss: 1278234788.4912\n",
      "Epoch 1450/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 67002935.4617 - val_loss: 1248921121.6842\n",
      "Epoch 1451/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 77866309.6359 - val_loss: 1189154453.3333\n",
      "Epoch 1452/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 72996456.3237 - val_loss: 1207346618.3860\n",
      "Epoch 1453/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 124201324.0035 - val_loss: 1117461360.8421\n",
      "Epoch 1454/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 135672777.5761 - val_loss: 1285199519.4386\n",
      "Epoch 1455/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 132464238.3254 - val_loss: 1288025465.2632\n",
      "Epoch 1456/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 76195119.4582 - val_loss: 1202859083.2281\n",
      "Epoch 1457/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 73943539.6447 - val_loss: 1179779184.2807\n",
      "Epoch 1458/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 74817129.9912 - val_loss: 1244761870.5965\n",
      "Epoch 1459/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 69889158.5471 - val_loss: 1199608313.2632\n",
      "Epoch 1460/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 70740326.9903 - val_loss: 1233294110.3158\n",
      "Epoch 1461/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 81080815.8804 - val_loss: 1176132674.2456\n",
      "Epoch 1462/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 65944416.2445 - val_loss: 1202501720.7018\n",
      "Epoch 1463/2000\n",
      "1137/1137 [==============================] - 0s 105us/step - loss: 90479985.9455 - val_loss: 1246769485.4737\n",
      "Epoch 1464/2000\n",
      "1137/1137 [==============================] - 0s 104us/step - loss: 76377624.4996 - val_loss: 1324243618.8070\n",
      "Epoch 1465/2000\n",
      "1137/1137 [==============================] - 0s 101us/step - loss: 92391923.4055 - val_loss: 1284903640.7018\n",
      "Epoch 1466/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 101116001.4072 - val_loss: 1183132508.6316\n",
      "Epoch 1467/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 106849795.1170 - val_loss: 1286323064.1404\n",
      "Epoch 1468/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 89998070.6139 - val_loss: 1180885820.0702\n",
      "Epoch 1469/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 122244041.3861 - val_loss: 1453492429.4737\n",
      "Epoch 1470/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 127991872.3096 - val_loss: 1161422736.8421\n",
      "Epoch 1471/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 82859670.5541 - val_loss: 1144048474.3860\n",
      "Epoch 1472/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 162897589.7414 - val_loss: 1188110757.6140\n",
      "Epoch 1473/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 131691918.0369 - val_loss: 1233889267.6491\n",
      "Epoch 1474/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 111482539.3034 - val_loss: 1275725358.0351\n",
      "Epoch 1475/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 125068444.2216 - val_loss: 1187964651.7895\n",
      "Epoch 1476/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 105266821.2841 - val_loss: 1206949865.5439\n",
      "Epoch 1477/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 94552722.2656 - val_loss: 1233376631.0175\n",
      "Epoch 1478/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 63568896.2955 - val_loss: 1206583317.3333\n",
      "Epoch 1479/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 68083910.8795 - val_loss: 1173415518.3158\n",
      "Epoch 1480/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 59921101.1223 - val_loss: 1166755185.4035\n",
      "Epoch 1481/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 59415619.0044 - val_loss: 1185312998.1754\n",
      "Epoch 1482/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 62376371.5954 - val_loss: 1227718713.2632\n",
      "Epoch 1483/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 59997102.2410 - val_loss: 1193813294.0351\n",
      "Epoch 1484/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 68128537.7555 - val_loss: 1210741077.3333\n",
      "Epoch 1485/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 127499467.7502 - val_loss: 1227257227.7895\n",
      "Epoch 1486/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 187161551.3949 - val_loss: 1229087507.0877\n",
      "Epoch 1487/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 304697838.3817 - val_loss: 1246397200.8421\n",
      "Epoch 1488/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 169609835.6728 - val_loss: 1172846013.7544\n",
      "Epoch 1489/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 102971712.6825 - val_loss: 1176813237.8947\n",
      "Epoch 1490/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 76635738.3676 - val_loss: 1144334885.0526\n",
      "Epoch 1491/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 72480750.9094 - val_loss: 1144510922.1053\n",
      "Epoch 1492/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 63189042.4908 - val_loss: 1267542453.8947\n",
      "Epoch 1493/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 95960567.7537 - val_loss: 1174042202.9474\n",
      "Epoch 1494/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 66435941.5022 - val_loss: 1135860832.5614\n",
      "Epoch 1495/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 62504413.0906 - val_loss: 1183130545.4035\n",
      "Epoch 1496/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 61066460.8179 - val_loss: 1161078495.4386\n",
      "Epoch 1497/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 70631540.8971 - val_loss: 1195816458.1053\n",
      "Epoch 1498/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 77251021.5233 - val_loss: 1151547665.4035\n",
      "Epoch 1499/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 63621413.1821 - val_loss: 1166366279.8596\n",
      "Epoch 1500/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 69450864.7353 - val_loss: 1169324464.2807\n",
      "Epoch 1501/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 77796902.5576 - val_loss: 1156909218.8070\n",
      "Epoch 1502/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 58812877.8153 - val_loss: 1198854437.0526\n",
      "Epoch 1503/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 63521199.7960 - val_loss: 1141669104.2807\n",
      "Epoch 1504/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 61114076.1266 - val_loss: 1203139493.0526\n",
      "Epoch 1505/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 54389301.1926 - val_loss: 1163706274.8070\n",
      "Epoch 1506/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 66502684.9182 - val_loss: 1183001467.5088\n",
      "Epoch 1507/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 59327103.6763 - val_loss: 1193995095.5789\n",
      "Epoch 1508/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 54770092.1091 - val_loss: 1134693303.0175\n",
      "Epoch 1509/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 61073073.9138 - val_loss: 1181125465.8246\n",
      "Epoch 1510/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 65846700.8232 - val_loss: 1198446039.5789\n",
      "Epoch 1511/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 83174210.4380 - val_loss: 1132213440.0000\n",
      "Epoch 1512/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 61241361.8892 - val_loss: 1212985073.4035\n",
      "Epoch 1513/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 63754296.3905 - val_loss: 1184907297.6842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1514/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 57592050.0475 - val_loss: 1230627614.3158\n",
      "Epoch 1515/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 67623040.4081 - val_loss: 1264027535.7193\n",
      "Epoch 1516/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 78181040.2463 - val_loss: 1203160937.5439\n",
      "Epoch 1517/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 67043328.8865 - val_loss: 1221803182.0351\n",
      "Epoch 1518/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 63400818.8250 - val_loss: 1172265384.4211\n",
      "Epoch 1519/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 53446412.5418 - val_loss: 1180050588.0702\n",
      "Epoch 1520/2000\n",
      "1137/1137 [==============================] - 0s 118us/step - loss: 56796070.8813 - val_loss: 1183882999.0175\n",
      "Epoch 1521/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 57993466.0123 - val_loss: 1238720830.8772\n",
      "Epoch 1522/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 65855419.2999 - val_loss: 1204946363.5088\n",
      "Epoch 1523/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 57183151.9543 - val_loss: 1193962458.9474\n",
      "Epoch 1524/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 61460026.7617 - val_loss: 1230034462.3158\n",
      "Epoch 1525/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 68608839.6517 - val_loss: 1230374009.2632\n",
      "Epoch 1526/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 69841645.1645 - val_loss: 1236515404.3509\n",
      "Epoch 1527/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 98468838.2726 - val_loss: 1205823650.8070\n",
      "Epoch 1528/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 131044469.0308 - val_loss: 1200089859.3684\n",
      "Epoch 1529/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 81573959.2964 - val_loss: 1197613081.8246\n",
      "Epoch 1530/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 82443564.7704 - val_loss: 1155588279.0175\n",
      "Epoch 1531/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 66172458.9833 - val_loss: 1245566754.8070\n",
      "Epoch 1532/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 81333020.0950 - val_loss: 1191623797.8947\n",
      "Epoch 1533/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 62204995.1240 - val_loss: 1159830148.4912\n",
      "Epoch 1534/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 62557519.4934 - val_loss: 1178769352.9825\n",
      "Epoch 1535/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 57232169.8012 - val_loss: 1174096626.5263\n",
      "Epoch 1536/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 56991640.3448 - val_loss: 1210305013.8947\n",
      "Epoch 1537/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 80989141.2172 - val_loss: 1199628057.8246\n",
      "Epoch 1538/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 74461407.5391 - val_loss: 1183263401.5439\n",
      "Epoch 1539/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 82238820.5312 - val_loss: 1144243599.7193\n",
      "Epoch 1540/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 67719684.4679 - val_loss: 1213043110.1754\n",
      "Epoch 1541/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 66076858.7124 - val_loss: 1166494022.7368\n",
      "Epoch 1542/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 76583693.4389 - val_loss: 1190917789.1930\n",
      "Epoch 1543/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 62432256.3377 - val_loss: 1175103904.5614\n",
      "Epoch 1544/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 69161921.8857 - val_loss: 1161199964.0702\n",
      "Epoch 1545/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 77353321.6183 - val_loss: 1173595284.2105\n",
      "Epoch 1546/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 98172894.6772 - val_loss: 1242496555.7895\n",
      "Epoch 1547/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 87209287.4793 - val_loss: 1172828486.7368\n",
      "Epoch 1548/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 75890284.8830 - val_loss: 1263114501.6140\n",
      "Epoch 1549/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 99466378.4697 - val_loss: 1185470720.5614\n",
      "Epoch 1550/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 80408390.6350 - val_loss: 1164888419.9298\n",
      "Epoch 1551/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 62515757.3404 - val_loss: 1257213114.3860\n",
      "Epoch 1552/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 96659662.7370 - val_loss: 1196978774.4561\n",
      "Epoch 1553/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 95016946.3219 - val_loss: 1223708349.7544\n",
      "Epoch 1554/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 115172591.4864 - val_loss: 1076236107.7895\n",
      "Epoch 1555/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 89998504.0704 - val_loss: 1213438209.1228\n",
      "Epoch 1556/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 85113056.4257 - val_loss: 1204279054.5965\n",
      "Epoch 1557/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 72124674.4380 - val_loss: 1126302222.5965\n",
      "Epoch 1558/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 65380740.1091 - val_loss: 1121667740.0702\n",
      "Epoch 1559/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 58511013.6851 - val_loss: 1160622702.0351\n",
      "Epoch 1560/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 63478435.3914 - val_loss: 1154526128.2807\n",
      "Epoch 1561/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 64636036.6332 - val_loss: 1200022346.1053\n",
      "Epoch 1562/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 65993972.0176 - val_loss: 1152301563.5088\n",
      "Epoch 1563/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 68010829.8962 - val_loss: 1234620289.1228\n",
      "Epoch 1564/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 59441724.8619 - val_loss: 1185540770.8070\n",
      "Epoch 1565/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 63332960.0915 - val_loss: 1189527603.6491\n",
      "Epoch 1566/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 81980123.7924 - val_loss: 1162961816.7018\n",
      "Epoch 1567/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 74366356.6719 - val_loss: 1202832873.5439\n",
      "Epoch 1568/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 101130244.4996 - val_loss: 1185334100.2105\n",
      "Epoch 1569/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 64442235.9437 - val_loss: 1147507037.1930\n",
      "Epoch 1570/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 72171193.0589 - val_loss: 1237878697.5439\n",
      "Epoch 1571/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 104698384.4011 - val_loss: 1124707142.7368\n",
      "Epoch 1572/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 63910994.0405 - val_loss: 1204272725.3333\n",
      "Epoch 1573/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 57895236.3026 - val_loss: 1170680227.9298\n",
      "Epoch 1574/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 63509344.8338 - val_loss: 1207666725.0526\n",
      "Epoch 1575/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 77425664.2392 - val_loss: 1199490033.4035\n",
      "Epoch 1576/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 57188722.3008 - val_loss: 1198040307.6491\n",
      "Epoch 1577/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 52402743.5989 - val_loss: 1178829153.6842\n",
      "Epoch 1578/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 59832367.3245 - val_loss: 1170141109.8947\n",
      "Epoch 1579/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 65120140.7071 - val_loss: 1216136755.6491\n",
      "Epoch 1580/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 88us/step - loss: 64126073.9349 - val_loss: 1139411499.7895\n",
      "Epoch 1581/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 103700655.3421 - val_loss: 1271874758.7368\n",
      "Epoch 1582/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 96275774.0651 - val_loss: 1235074252.3509\n",
      "Epoch 1583/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 86246729.6288 - val_loss: 1193525841.9649\n",
      "Epoch 1584/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 68023365.2770 - val_loss: 1192844556.3509\n",
      "Epoch 1585/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 65888786.8707 - val_loss: 1188777089.1228\n",
      "Epoch 1586/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 66925713.7238 - val_loss: 1165101176.1404\n",
      "Epoch 1587/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 61025005.5620 - val_loss: 1124917561.2632\n",
      "Epoch 1588/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 51501933.1961 - val_loss: 1201618511.7193\n",
      "Epoch 1589/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 57707778.0018 - val_loss: 1199722122.1053\n",
      "Epoch 1590/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 75982141.3544 - val_loss: 1207013236.7719\n",
      "Epoch 1591/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 91881504.7986 - val_loss: 1169632814.0351\n",
      "Epoch 1592/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 103734759.9930 - val_loss: 1104253793.6842\n",
      "Epoch 1593/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 109089591.9085 - val_loss: 1344101981.1930\n",
      "Epoch 1594/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 131347454.5752 - val_loss: 1308567393.6842\n",
      "Epoch 1595/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 106579442.5330 - val_loss: 1172535498.1053\n",
      "Epoch 1596/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 80642956.2181 - val_loss: 1146142807.5789\n",
      "Epoch 1597/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 79939688.9639 - val_loss: 1199141367.0175\n",
      "Epoch 1598/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 84114236.8056 - val_loss: 1245916520.4211\n",
      "Epoch 1599/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 73702861.5092 - val_loss: 1198004313.8246\n",
      "Epoch 1600/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 79564881.8610 - val_loss: 1218620469.8947\n",
      "Epoch 1601/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 75049892.1196 - val_loss: 1115449082.3860\n",
      "Epoch 1602/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 84033471.7045 - val_loss: 1229580454.1754\n",
      "Epoch 1603/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 70768229.5901 - val_loss: 1225985969.4035\n",
      "Epoch 1604/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 73956271.7889 - val_loss: 1189452080.2807\n",
      "Epoch 1605/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 75748133.1856 - val_loss: 1215936125.7544\n",
      "Epoch 1606/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 64592133.8153 - val_loss: 1164587210.1053\n",
      "Epoch 1607/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 55261990.7898 - val_loss: 1208738899.0877\n",
      "Epoch 1608/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 67134791.8311 - val_loss: 1254259530.1053\n",
      "Epoch 1609/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 88250128.4573 - val_loss: 1192662652.6316\n",
      "Epoch 1610/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 62981295.4125 - val_loss: 1152256398.5965\n",
      "Epoch 1611/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 83299364.1478 - val_loss: 1178714144.5614\n",
      "Epoch 1612/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 65841152.8865 - val_loss: 1248795448.1404\n",
      "Epoch 1613/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 85192089.8610 - val_loss: 1263171635.6491\n",
      "Epoch 1614/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 90527068.5963 - val_loss: 1155369568.5614\n",
      "Epoch 1615/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 71747201.2559 - val_loss: 1198951099.5088\n",
      "Epoch 1616/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 73567971.3492 - val_loss: 1206011762.5263\n",
      "Epoch 1617/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 69895160.5558 - val_loss: 1147422713.2632\n",
      "Epoch 1618/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 63771675.2753 - val_loss: 1206996567.5789\n",
      "Epoch 1619/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 55781294.2973 - val_loss: 1148727236.4912\n",
      "Epoch 1620/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 55272126.7546 - val_loss: 1192494871.5789\n",
      "Epoch 1621/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 53809944.1900 - val_loss: 1165625930.1053\n",
      "Epoch 1622/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 77652464.7880 - val_loss: 1223498684.6316\n",
      "Epoch 1623/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 96936198.2480 - val_loss: 1209195116.9123\n",
      "Epoch 1624/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 119024330.1038 - val_loss: 1178371372.9123\n",
      "Epoch 1625/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 62281322.5013 - val_loss: 1180770374.7368\n",
      "Epoch 1626/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 55066644.9288 - val_loss: 1171288161.6842\n",
      "Epoch 1627/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 59677245.5585 - val_loss: 1270712333.4737\n",
      "Epoch 1628/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 77539103.9736 - val_loss: 1196310805.3333\n",
      "Epoch 1629/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 61330654.9411 - val_loss: 1148037450.1053\n",
      "Epoch 1630/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 50946925.4635 - val_loss: 1179458573.4737\n",
      "Epoch 1631/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 51933119.7889 - val_loss: 1211612344.1404\n",
      "Epoch 1632/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 59430313.8997 - val_loss: 1181574861.4737\n",
      "Epoch 1633/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 62339590.4785 - val_loss: 1190984589.4737\n",
      "Epoch 1634/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 53330552.5066 - val_loss: 1213371113.5439\n",
      "Epoch 1635/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 49259640.3659 - val_loss: 1160541899.2281\n",
      "Epoch 1636/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 70842379.5884 - val_loss: 1162091467.2281\n",
      "Epoch 1637/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 60456092.6368 - val_loss: 1178371080.9825\n",
      "Epoch 1638/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 52682531.5110 - val_loss: 1193840282.9474\n",
      "Epoch 1639/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 48196701.2876 - val_loss: 1201950492.0702\n",
      "Epoch 1640/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 55530599.1451 - val_loss: 1182512460.3509\n",
      "Epoch 1641/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 49338536.6596 - val_loss: 1144413062.7368\n",
      "Epoch 1642/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 78612487.7397 - val_loss: 1269919635.0877\n",
      "Epoch 1643/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 80362332.8338 - val_loss: 1165732660.7719\n",
      "Epoch 1644/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 80143917.0624 - val_loss: 1176327645.1930\n",
      "Epoch 1645/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 56100316.1794 - val_loss: 1152815809.1228\n",
      "Epoch 1646/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 83us/step - loss: 65137814.5787 - val_loss: 1173453347.9298\n",
      "Epoch 1647/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 56868136.1847 - val_loss: 1283585758.3158\n",
      "Epoch 1648/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 69340766.7194 - val_loss: 1152111612.6316\n",
      "Epoch 1649/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 58754304.5770 - val_loss: 1168908349.7544\n",
      "Epoch 1650/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 66578494.8813 - val_loss: 1247662282.1053\n",
      "Epoch 1651/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 62275594.4802 - val_loss: 1177533877.8947\n",
      "Epoch 1652/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 65648579.9824 - val_loss: 1227558433.6842\n",
      "Epoch 1653/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 55873865.0836 - val_loss: 1169915990.4561\n",
      "Epoch 1654/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 50807703.7485 - val_loss: 1172738327.5789\n",
      "Epoch 1655/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 81640587.8311 - val_loss: 1252578734.0351\n",
      "Epoch 1656/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 183138834.8144 - val_loss: 1190404281.2632\n",
      "Epoch 1657/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 204483970.4274 - val_loss: 1370928386.2456\n",
      "Epoch 1658/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 99476011.8769 - val_loss: 1130407374.0351\n",
      "Epoch 1659/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 71989000.5172 - val_loss: 1144427013.6140\n",
      "Epoch 1660/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 64987612.6086 - val_loss: 1245379482.9474\n",
      "Epoch 1661/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 94785806.3360 - val_loss: 1208722051.3684\n",
      "Epoch 1662/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 66767643.7924 - val_loss: 1183879347.6491\n",
      "Epoch 1663/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 50576580.8654 - val_loss: 1219950327.0175\n",
      "Epoch 1664/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 59496496.7916 - val_loss: 1161137308.0702\n",
      "Epoch 1665/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 70516202.8074 - val_loss: 1235333853.1930\n",
      "Epoch 1666/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 63369151.3105 - val_loss: 1197602720.5614\n",
      "Epoch 1667/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 51739072.6895 - val_loss: 1183140812.3509\n",
      "Epoch 1668/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 47897890.9059 - val_loss: 1192013435.5088\n",
      "Epoch 1669/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 61687154.3395 - val_loss: 1271480735.4386\n",
      "Epoch 1670/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 95813534.7898 - val_loss: 1234809335.0175\n",
      "Epoch 1671/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 98563767.2507 - val_loss: 1215204578.8070\n",
      "Epoch 1672/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 64255606.7687 - val_loss: 1196597294.0351\n",
      "Epoch 1673/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 87096595.2155 - val_loss: 1210705677.4737\n",
      "Epoch 1674/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 58786352.6332 - val_loss: 1177526252.9123\n",
      "Epoch 1675/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 79420867.1627 - val_loss: 1278906759.8596\n",
      "Epoch 1676/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 68566803.3527 - val_loss: 1151608903.8596\n",
      "Epoch 1677/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 55070225.0941 - val_loss: 1181783347.6491\n",
      "Epoch 1678/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 49283407.0501 - val_loss: 1230162750.8772\n",
      "Epoch 1679/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 55110691.3069 - val_loss: 1196141067.2281\n",
      "Epoch 1680/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 66674172.7845 - val_loss: 1206751838.3158\n",
      "Epoch 1681/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 57152504.2498 - val_loss: 1186370816.0000\n",
      "Epoch 1682/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 53188859.4688 - val_loss: 1207825002.6667\n",
      "Epoch 1683/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 58140303.8734 - val_loss: 1212351765.3333\n",
      "Epoch 1684/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 57075514.9868 - val_loss: 1206693139.0877\n",
      "Epoch 1685/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 65451560.4292 - val_loss: 1214526734.5965\n",
      "Epoch 1686/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 58332453.2102 - val_loss: 1232298986.6667\n",
      "Epoch 1687/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 47740738.0862 - val_loss: 1180192181.8947\n",
      "Epoch 1688/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 51464509.1398 - val_loss: 1170913130.6667\n",
      "Epoch 1689/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 47522143.2753 - val_loss: 1227664681.5439\n",
      "Epoch 1690/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 59849911.2964 - val_loss: 1161963658.1053\n",
      "Epoch 1691/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 56609183.7784 - val_loss: 1215591960.7018\n",
      "Epoch 1692/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 47889540.0563 - val_loss: 1209837221.0526\n",
      "Epoch 1693/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 46678798.1284 - val_loss: 1198563338.1053\n",
      "Epoch 1694/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 53794754.9305 - val_loss: 1189475566.0351\n",
      "Epoch 1695/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 71516570.7546 - val_loss: 1363883178.6667\n",
      "Epoch 1696/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 88785476.4433 - val_loss: 1164269730.8070\n",
      "Epoch 1697/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 58195451.8558 - val_loss: 1217195765.8947\n",
      "Epoch 1698/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 84684110.8848 - val_loss: 1262708915.6491\n",
      "Epoch 1699/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 51858105.6394 - val_loss: 1198014172.0702\n",
      "Epoch 1700/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 50999224.9604 - val_loss: 1215164389.0526\n",
      "Epoch 1701/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 69045728.7142 - val_loss: 1156810824.9825\n",
      "Epoch 1702/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 86470468.1337 - val_loss: 1280372076.9123\n",
      "Epoch 1703/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 82146554.6209 - val_loss: 1278554176.0000\n",
      "Epoch 1704/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 85703431.9578 - val_loss: 1167581149.1930\n",
      "Epoch 1705/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 59194547.3632 - val_loss: 1185057163.2281\n",
      "Epoch 1706/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 53503293.7555 - val_loss: 1189583023.1579\n",
      "Epoch 1707/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 52877402.0440 - val_loss: 1219457833.5439\n",
      "Epoch 1708/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 56326041.2208 - val_loss: 1193242481.4035\n",
      "Epoch 1709/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 56863658.6561 - val_loss: 1214655760.8421\n",
      "Epoch 1710/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 51805820.3272 - val_loss: 1190660990.8772\n",
      "Epoch 1711/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 53606376.5101 - val_loss: 1190487389.1930\n",
      "Epoch 1712/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 76us/step - loss: 54208138.4310 - val_loss: 1199028789.8947\n",
      "Epoch 1713/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 50004982.4415 - val_loss: 1187810700.3509\n",
      "Epoch 1714/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 70064469.4318 - val_loss: 1188896775.8596\n",
      "Epoch 1715/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 124231851.7608 - val_loss: 1229562632.9825\n",
      "Epoch 1716/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 94608177.2172 - val_loss: 1177966058.6667\n",
      "Epoch 1717/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 87193186.1812 - val_loss: 1279600741.0526\n",
      "Epoch 1718/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 58100966.2269 - val_loss: 1158847923.6491\n",
      "Epoch 1719/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 59459065.2067 - val_loss: 1229792866.8070\n",
      "Epoch 1720/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 59483654.0018 - val_loss: 1222584754.5263\n",
      "Epoch 1721/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 53375676.5629 - val_loss: 1168718964.7719\n",
      "Epoch 1722/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 48086001.5233 - val_loss: 1181088181.8947\n",
      "Epoch 1723/2000\n",
      "1137/1137 [==============================] - 0s 67us/step - loss: 48240230.0756 - val_loss: 1186924385.6842\n",
      "Epoch 1724/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 54303710.1284 - val_loss: 1234517697.1228\n",
      "Epoch 1725/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 48531382.1143 - val_loss: 1176851095.5789\n",
      "Epoch 1726/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 49037402.4099 - val_loss: 1146656277.3333\n",
      "Epoch 1727/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 58302871.1170 - val_loss: 1201451662.5965\n",
      "Epoch 1728/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 71177942.2480 - val_loss: 1281893660.0702\n",
      "Epoch 1729/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 68916517.4987 - val_loss: 1187851152.8421\n",
      "Epoch 1730/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 73250413.6218 - val_loss: 1188822752.5614\n",
      "Epoch 1731/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 52933219.2190 - val_loss: 1201704204.3509\n",
      "Epoch 1732/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 52425276.8232 - val_loss: 1237088846.5965\n",
      "Epoch 1733/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 70590500.3817 - val_loss: 1312287104.0000\n",
      "Epoch 1734/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 63581527.6517 - val_loss: 1195572753.9649\n",
      "Epoch 1735/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 49118702.6227 - val_loss: 1224273814.4561\n",
      "Epoch 1736/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 47134996.5981 - val_loss: 1178327301.6140\n",
      "Epoch 1737/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 59351073.0413 - val_loss: 1177854430.3158\n",
      "Epoch 1738/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 73203903.8276 - val_loss: 1224692910.0351\n",
      "Epoch 1739/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 51941370.4310 - val_loss: 1171022110.3158\n",
      "Epoch 1740/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 58118337.7062 - val_loss: 1208655604.7719\n",
      "Epoch 1741/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 70338383.0748 - val_loss: 1205210844.0702\n",
      "Epoch 1742/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 62593175.3808 - val_loss: 1173556976.2807\n",
      "Epoch 1743/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 68068879.4230 - val_loss: 1187910073.2632\n",
      "Epoch 1744/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 70212937.9402 - val_loss: 1218835884.9123\n",
      "Epoch 1745/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 139230686.6772 - val_loss: 1329708446.3158\n",
      "Epoch 1746/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 172652060.8549 - val_loss: 1179316165.6140\n",
      "Epoch 1747/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 108191887.0114 - val_loss: 1287649670.7368\n",
      "Epoch 1748/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 221579771.7995 - val_loss: 1221126931.0877\n",
      "Epoch 1749/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 184572988.5031 - val_loss: 1246331361.6842\n",
      "Epoch 1750/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 114303735.6588 - val_loss: 1201763835.5088\n",
      "Epoch 1751/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 69780386.3852 - val_loss: 1169202845.1930\n",
      "Epoch 1752/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 52704899.1029 - val_loss: 1215634831.7193\n",
      "Epoch 1753/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 54550475.9296 - val_loss: 1138377900.9123\n",
      "Epoch 1754/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 55148722.6350 - val_loss: 1210050911.4386\n",
      "Epoch 1755/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 46724835.5427 - val_loss: 1199249358.5965\n",
      "Epoch 1756/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 51175550.5154 - val_loss: 1188853300.7719\n",
      "Epoch 1757/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 49567350.1390 - val_loss: 1171147284.2105\n",
      "Epoch 1758/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 44007811.3034 - val_loss: 1157999088.2807\n",
      "Epoch 1759/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 46233448.8830 - val_loss: 1187838450.5263\n",
      "Epoch 1760/2000\n",
      "1137/1137 [==============================] - 0s 88us/step - loss: 55673313.1504 - val_loss: 1189064076.3509\n",
      "Epoch 1761/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 42693144.2515 - val_loss: 1187687717.0526\n",
      "Epoch 1762/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 47858132.8566 - val_loss: 1197390535.8596\n",
      "Epoch 1763/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 44762012.0106 - val_loss: 1193256653.4737\n",
      "Epoch 1764/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 49909800.8197 - val_loss: 1209774369.6842\n",
      "Epoch 1765/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 52690521.1856 - val_loss: 1176826835.0877\n",
      "Epoch 1766/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 49771680.4011 - val_loss: 1180512791.5789\n",
      "Epoch 1767/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 53995575.9384 - val_loss: 1154585177.8246\n",
      "Epoch 1768/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 49920702.2938 - val_loss: 1203139041.6842\n",
      "Epoch 1769/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 50368776.1266 - val_loss: 1179363939.9298\n",
      "Epoch 1770/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 47492060.7564 - val_loss: 1156746227.6491\n",
      "Epoch 1771/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 48318472.0070 - val_loss: 1201738416.2807\n",
      "Epoch 1772/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 47358476.5840 - val_loss: 1203972366.5965\n",
      "Epoch 1773/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 50390358.2656 - val_loss: 1191690021.0526\n",
      "Epoch 1774/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 53619283.3316 - val_loss: 1210480388.4912\n",
      "Epoch 1775/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 65228163.9754 - val_loss: 1279792538.9474\n",
      "Epoch 1776/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 91181140.8162 - val_loss: 1139271389.1930\n",
      "Epoch 1777/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 70445146.7687 - val_loss: 1167140122.9474\n",
      "Epoch 1778/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 75us/step - loss: 60847754.4732 - val_loss: 1203015558.7368\n",
      "Epoch 1779/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 68096050.9798 - val_loss: 1216668465.4035\n",
      "Epoch 1780/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 84843471.9648 - val_loss: 1234531760.2807\n",
      "Epoch 1781/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 84210465.4037 - val_loss: 1229070567.2982\n",
      "Epoch 1782/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 64052276.1442 - val_loss: 1184602101.8947\n",
      "Epoch 1783/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 48109936.0915 - val_loss: 1194042972.0702\n",
      "Epoch 1784/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 55346306.5611 - val_loss: 1207172744.9825\n",
      "Epoch 1785/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 69387544.3870 - val_loss: 1182141808.2807\n",
      "Epoch 1786/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 57537296.3412 - val_loss: 1184871813.6140\n",
      "Epoch 1787/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 94526265.4846 - val_loss: 1226257999.7193\n",
      "Epoch 1788/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 113995132.3061 - val_loss: 1103615444.2105\n",
      "Epoch 1789/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 129221438.5013 - val_loss: 1172321073.4035\n",
      "Epoch 1790/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 57235276.9358 - val_loss: 1171722901.3333\n",
      "Epoch 1791/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 46936020.3131 - val_loss: 1164666568.9825\n",
      "Epoch 1792/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 47232687.6552 - val_loss: 1226040705.1228\n",
      "Epoch 1793/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 49495788.9710 - val_loss: 1155100967.2982\n",
      "Epoch 1794/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 43645518.7335 - val_loss: 1199060922.3860\n",
      "Epoch 1795/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 44214110.2656 - val_loss: 1131888042.6667\n",
      "Epoch 1796/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 45278737.4952 - val_loss: 1215621409.6842\n",
      "Epoch 1797/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 47663069.2419 - val_loss: 1167109446.7368\n",
      "Epoch 1798/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 51820208.7493 - val_loss: 1180268368.8421\n",
      "Epoch 1799/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 60613768.0633 - val_loss: 1170204029.7544\n",
      "Epoch 1800/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 48975981.9244 - val_loss: 1182218712.7018\n",
      "Epoch 1801/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 49975956.1865 - val_loss: 1244313841.4035\n",
      "Epoch 1802/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 47090912.0809 - val_loss: 1195934810.9474\n",
      "Epoch 1803/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 47702317.4600 - val_loss: 1186006686.3158\n",
      "Epoch 1804/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 43647627.0044 - val_loss: 1196938824.9825\n",
      "Epoch 1805/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 54664355.7748 - val_loss: 1175716546.2456\n",
      "Epoch 1806/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 44259689.3580 - val_loss: 1176326750.3158\n",
      "Epoch 1807/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 54054032.0704 - val_loss: 1289213757.7544\n",
      "Epoch 1808/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 58078722.7194 - val_loss: 1229118825.5439\n",
      "Epoch 1809/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 54084279.1768 - val_loss: 1182872704.0000\n",
      "Epoch 1810/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 43557624.1161 - val_loss: 1175581738.6667\n",
      "Epoch 1811/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 51617352.6825 - val_loss: 1257915183.1579\n",
      "Epoch 1812/2000\n",
      "1137/1137 [==============================] - 0s 97us/step - loss: 78632700.1689 - val_loss: 1225452777.5439\n",
      "Epoch 1813/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 68442326.4099 - val_loss: 1226110183.2982\n",
      "Epoch 1814/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 59579120.0774 - val_loss: 1184638376.4211\n",
      "Epoch 1815/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 60527364.5629 - val_loss: 1269202908.0702\n",
      "Epoch 1816/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 77753875.5040 - val_loss: 1237824803.9298\n",
      "Epoch 1817/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 94403516.5312 - val_loss: 1324030096.8421\n",
      "Epoch 1818/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 118493656.0352 - val_loss: 1099068008.4211\n",
      "Epoch 1819/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 106515498.3852 - val_loss: 1222036066.8070\n",
      "Epoch 1820/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 88838226.6280 - val_loss: 1181482127.7193\n",
      "Epoch 1821/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 51765689.9279 - val_loss: 1144287870.8772\n",
      "Epoch 1822/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 56739653.7731 - val_loss: 1190360312.1404\n",
      "Epoch 1823/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 58385153.5409 - val_loss: 1177462955.7895\n",
      "Epoch 1824/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 46776634.0563 - val_loss: 1160376527.7193\n",
      "Epoch 1825/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 43174694.0827 - val_loss: 1209716338.5263\n",
      "Epoch 1826/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 47666603.3140 - val_loss: 1215786097.4035\n",
      "Epoch 1827/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 51178493.3228 - val_loss: 1217988000.5614\n",
      "Epoch 1828/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 52295783.5602 - val_loss: 1187290839.5789\n",
      "Epoch 1829/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 43015318.3993 - val_loss: 1168892305.9649\n",
      "Epoch 1830/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 46395039.7397 - val_loss: 1201999391.4386\n",
      "Epoch 1831/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 48041817.3967 - val_loss: 1173508893.1930\n",
      "Epoch 1832/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 63604558.9446 - val_loss: 1198993701.0526\n",
      "Epoch 1833/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 53869813.6113 - val_loss: 1217220599.0175\n",
      "Epoch 1834/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 72881698.4767 - val_loss: 1199858232.1404\n",
      "Epoch 1835/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 64153197.0730 - val_loss: 1304401816.7018\n",
      "Epoch 1836/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 93312734.8109 - val_loss: 1169631931.5088\n",
      "Epoch 1837/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 87410099.1029 - val_loss: 1170819395.3684\n",
      "Epoch 1838/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 101073602.5611 - val_loss: 1214398149.6140\n",
      "Epoch 1839/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 99597218.0967 - val_loss: 1174589016.7018\n",
      "Epoch 1840/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 60547474.0915 - val_loss: 1145117145.8246\n",
      "Epoch 1841/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 56547939.3755 - val_loss: 1184927927.0175\n",
      "Epoch 1842/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 55157828.7599 - val_loss: 1246189032.4211\n",
      "Epoch 1843/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 42307774.3500 - val_loss: 1164162013.1930\n",
      "Epoch 1844/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 78us/step - loss: 43837904.8478 - val_loss: 1172459468.3509\n",
      "Epoch 1845/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 46071466.8461 - val_loss: 1220612929.1228\n",
      "Epoch 1846/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 44567810.4837 - val_loss: 1183026649.8246\n",
      "Epoch 1847/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 41584523.1275 - val_loss: 1218851585.1228\n",
      "Epoch 1848/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 46271806.1741 - val_loss: 1168641644.9123\n",
      "Epoch 1849/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 87567525.4072 - val_loss: 1251199823.7193\n",
      "Epoch 1850/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 72480424.9464 - val_loss: 1189605356.9123\n",
      "Epoch 1851/2000\n",
      "1137/1137 [==============================] - 0s 87us/step - loss: 61228957.2032 - val_loss: 1192109600.5614\n",
      "Epoch 1852/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 58124950.8953 - val_loss: 1255551130.9474\n",
      "Epoch 1853/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 62975363.1592 - val_loss: 1176907567.1579\n",
      "Epoch 1854/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 117230760.7106 - val_loss: 1419787296.5614\n",
      "Epoch 1855/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 238292111.2682 - val_loss: 1214989911.5789\n",
      "Epoch 1856/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 178090347.4969 - val_loss: 1240717773.4737\n",
      "Epoch 1857/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 97743849.2383 - val_loss: 1252792364.9123\n",
      "Epoch 1858/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 59269971.0396 - val_loss: 1165833171.0877\n",
      "Epoch 1859/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 65805034.7089 - val_loss: 1239805721.8246\n",
      "Epoch 1860/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 72002031.5638 - val_loss: 1223347665.9649\n",
      "Epoch 1861/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 63460291.4723 - val_loss: 1193120012.3509\n",
      "Epoch 1862/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 49247811.3527 - val_loss: 1191173898.1053\n",
      "Epoch 1863/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 64825979.3914 - val_loss: 1278031329.6842\n",
      "Epoch 1864/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 95077094.8391 - val_loss: 1186592857.8246\n",
      "Epoch 1865/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 56093596.3061 - val_loss: 1257588321.6842\n",
      "Epoch 1866/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 76511696.9991 - val_loss: 1193977207.0175\n",
      "Epoch 1867/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 51757629.6288 - val_loss: 1216912472.7018\n",
      "Epoch 1868/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 80165389.5585 - val_loss: 1203960055.0175\n",
      "Epoch 1869/2000\n",
      "1137/1137 [==============================] - 0s 99us/step - loss: 49129620.0281 - val_loss: 1181664693.8947\n",
      "Epoch 1870/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 49017940.7828 - val_loss: 1146473267.6491\n",
      "Epoch 1871/2000\n",
      "1137/1137 [==============================] - 0s 94us/step - loss: 48839527.8909 - val_loss: 1209047633.9649\n",
      "Epoch 1872/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 51162983.0501 - val_loss: 1166238953.5439\n",
      "Epoch 1873/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 74604056.9639 - val_loss: 1183363896.1404\n",
      "Epoch 1874/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 71592300.9675 - val_loss: 1157838593.1228\n",
      "Epoch 1875/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 54575756.7353 - val_loss: 1182828578.8070\n",
      "Epoch 1876/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 41738058.7230 - val_loss: 1217067223.5789\n",
      "Epoch 1877/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 46622400.0985 - val_loss: 1210915550.3158\n",
      "Epoch 1878/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 43226433.5919 - val_loss: 1181394007.5789\n",
      "Epoch 1879/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 45288242.0862 - val_loss: 1185171945.5439\n",
      "Epoch 1880/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 46258735.8804 - val_loss: 1191977366.4561\n",
      "Epoch 1881/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 52627558.5752 - val_loss: 1238433666.2456\n",
      "Epoch 1882/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 51566562.9692 - val_loss: 1179116465.4035\n",
      "Epoch 1883/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 58436064.7106 - val_loss: 1237882990.0351\n",
      "Epoch 1884/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 43490712.7230 - val_loss: 1151475817.5439\n",
      "Epoch 1885/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 51199701.9244 - val_loss: 1231421025.6842\n",
      "Epoch 1886/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 54899620.1442 - val_loss: 1188122408.4211\n",
      "Epoch 1887/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 44177590.7757 - val_loss: 1172074160.2807\n",
      "Epoch 1888/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 46253889.8962 - val_loss: 1200049440.5614\n",
      "Epoch 1889/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 61518553.8646 - val_loss: 1167370809.2632\n",
      "Epoch 1890/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 44083059.9191 - val_loss: 1234483768.1404\n",
      "Epoch 1891/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 64683123.1909 - val_loss: 1160721195.7895\n",
      "Epoch 1892/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 43883342.1865 - val_loss: 1200541088.5614\n",
      "Epoch 1893/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 43367982.8074 - val_loss: 1178207834.9474\n",
      "Epoch 1894/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 58953068.7036 - val_loss: 1187653806.0351\n",
      "Epoch 1895/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 48113861.3650 - val_loss: 1168777903.1579\n",
      "Epoch 1896/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 39872106.7687 - val_loss: 1192956722.5263\n",
      "Epoch 1897/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 54016686.5259 - val_loss: 1212353073.4035\n",
      "Epoch 1898/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 44510487.3738 - val_loss: 1217104331.2281\n",
      "Epoch 1899/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 41776700.0176 - val_loss: 1182487620.4912\n",
      "Epoch 1900/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 47349950.8953 - val_loss: 1190717322.1053\n",
      "Epoch 1901/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 37545306.0985 - val_loss: 1204095190.4561\n",
      "Epoch 1902/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 36618165.6042 - val_loss: 1178289216.0000\n",
      "Epoch 1903/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 53056875.2049 - val_loss: 1187038342.7368\n",
      "Epoch 1904/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 44472442.4380 - val_loss: 1267781809.4035\n",
      "Epoch 1905/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 53465718.7546 - val_loss: 1179612793.2632\n",
      "Epoch 1906/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 42657742.8461 - val_loss: 1168875217.9649\n",
      "Epoch 1907/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 34974380.0915 - val_loss: 1219230805.3333\n",
      "Epoch 1908/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 48566007.2436 - val_loss: 1203733313.1228\n",
      "Epoch 1909/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 77215717.8434 - val_loss: 1235448284.0702\n",
      "Epoch 1910/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 90us/step - loss: 55787457.8540 - val_loss: 1182860630.4561\n",
      "Epoch 1911/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 56282441.9877 - val_loss: 1274454117.0526\n",
      "Epoch 1912/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 56080941.3685 - val_loss: 1232955851.2281\n",
      "Epoch 1913/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 68965922.8848 - val_loss: 1200681658.3860\n",
      "Epoch 1914/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 64864672.1302 - val_loss: 1215557245.7544\n",
      "Epoch 1915/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 52515032.8039 - val_loss: 1225449046.4561\n",
      "Epoch 1916/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 42444257.3263 - val_loss: 1205626113.1228\n",
      "Epoch 1917/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 47402034.9024 - val_loss: 1258527852.9123\n",
      "Epoch 1918/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 58400871.7045 - val_loss: 1231347408.8421\n",
      "Epoch 1919/2000\n",
      "1137/1137 [==============================] - 0s 89us/step - loss: 63640217.4142 - val_loss: 1251365562.3860\n",
      "Epoch 1920/2000\n",
      "1137/1137 [==============================] - 0s 86us/step - loss: 120579522.8777 - val_loss: 1251262602.1053\n",
      "Epoch 1921/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 135370310.6491 - val_loss: 1224466654.3158\n",
      "Epoch 1922/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 84365771.9138 - val_loss: 1238067654.7368\n",
      "Epoch 1923/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 50081105.6922 - val_loss: 1228200627.6491\n",
      "Epoch 1924/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 44962246.8267 - val_loss: 1204825742.5965\n",
      "Epoch 1925/2000\n",
      "1137/1137 [==============================] - 0s 96us/step - loss: 46837637.0343 - val_loss: 1184548009.5439\n",
      "Epoch 1926/2000\n",
      "1137/1137 [==============================] - 0s 95us/step - loss: 53645127.9648 - val_loss: 1213182856.9825\n",
      "Epoch 1927/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 44978554.6438 - val_loss: 1171396473.2632\n",
      "Epoch 1928/2000\n",
      "1137/1137 [==============================] - 0s 93us/step - loss: 59745918.7370 - val_loss: 1236503649.6842\n",
      "Epoch 1929/2000\n",
      "1137/1137 [==============================] - 0s 92us/step - loss: 63112494.2938 - val_loss: 1227006754.8070\n",
      "Epoch 1930/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 57652073.2946 - val_loss: 1197472465.9649\n",
      "Epoch 1931/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 40835459.8874 - val_loss: 1222561344.0000\n",
      "Epoch 1932/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 50570193.4494 - val_loss: 1175715753.5439\n",
      "Epoch 1933/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 46242483.6588 - val_loss: 1151315192.1404\n",
      "Epoch 1934/2000\n",
      "1137/1137 [==============================] - 0s 90us/step - loss: 40907176.7634 - val_loss: 1197333593.8246\n",
      "Epoch 1935/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 36049261.1064 - val_loss: 1152674196.2105\n",
      "Epoch 1936/2000\n",
      "1137/1137 [==============================] - 0s 82us/step - loss: 41831871.3281 - val_loss: 1150611197.7544\n",
      "Epoch 1937/2000\n",
      "1137/1137 [==============================] - 0s 80us/step - loss: 38338358.0897 - val_loss: 1211190656.0000\n",
      "Epoch 1938/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 37467402.8285 - val_loss: 1182017498.9474\n",
      "Epoch 1939/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 51909863.5708 - val_loss: 1187820995.3684\n",
      "Epoch 1940/2000\n",
      "1137/1137 [==============================] - 0s 85us/step - loss: 88262510.7054 - val_loss: 1284224580.4912\n",
      "Epoch 1941/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 170855734.4028 - val_loss: 1116219898.3860\n",
      "Epoch 1942/2000\n",
      "1137/1137 [==============================] - 0s 81us/step - loss: 66719859.4406 - val_loss: 1144821690.3860\n",
      "Epoch 1943/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 47359712.3307 - val_loss: 1196998642.5263\n",
      "Epoch 1944/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 44826799.9120 - val_loss: 1196810757.6140\n",
      "Epoch 1945/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 39590843.4371 - val_loss: 1199011170.8070\n",
      "Epoch 1946/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 43321886.3870 - val_loss: 1181947770.3860\n",
      "Epoch 1947/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 42301779.0994 - val_loss: 1228864627.6491\n",
      "Epoch 1948/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 43296591.6834 - val_loss: 1209682924.9123\n",
      "Epoch 1949/2000\n",
      "1137/1137 [==============================] - 0s 70us/step - loss: 47104425.9877 - val_loss: 1133820659.6491\n",
      "Epoch 1950/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 50650444.0528 - val_loss: 1250390967.0175\n",
      "Epoch 1951/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 76547267.8030 - val_loss: 1183312128.0000\n",
      "Epoch 1952/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 55451614.6491 - val_loss: 1211206775.0175\n",
      "Epoch 1953/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 61658608.8408 - val_loss: 1192051787.2281\n",
      "Epoch 1954/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 70030557.6887 - val_loss: 1179478294.4561\n",
      "Epoch 1955/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 43404747.7256 - val_loss: 1155250653.1930\n",
      "Epoch 1956/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 44127621.6922 - val_loss: 1190483132.6316\n",
      "Epoch 1957/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 51768967.0871 - val_loss: 1192511349.8947\n",
      "Epoch 1958/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 47974141.7098 - val_loss: 1191030108.0702\n",
      "Epoch 1959/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 39239087.9525 - val_loss: 1229143154.5263\n",
      "Epoch 1960/2000\n",
      "1137/1137 [==============================] - 0s 78us/step - loss: 44953514.2586 - val_loss: 1194632103.2982\n",
      "Epoch 1961/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 53043001.6675 - val_loss: 1200301914.9474\n",
      "Epoch 1962/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 47639619.3527 - val_loss: 1198585346.2456\n",
      "Epoch 1963/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 40790495.2542 - val_loss: 1182118600.9825\n",
      "Epoch 1964/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 39413083.9842 - val_loss: 1267896120.1404\n",
      "Epoch 1965/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 59339739.7713 - val_loss: 1189532154.3860\n",
      "Epoch 1966/2000\n",
      "1137/1137 [==============================] - 0s 91us/step - loss: 52337103.1979 - val_loss: 1181703630.5965\n",
      "Epoch 1967/2000\n",
      "1137/1137 [==============================] - 0s 83us/step - loss: 44860454.9727 - val_loss: 1167666410.6667\n",
      "Epoch 1968/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 65177959.9789 - val_loss: 1184568840.9825\n",
      "Epoch 1969/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 48388357.7485 - val_loss: 1196856862.3158\n",
      "Epoch 1970/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 53744600.1794 - val_loss: 1165158202.3860\n",
      "Epoch 1971/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 46364976.1302 - val_loss: 1159210536.4211\n",
      "Epoch 1972/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 38489733.2982 - val_loss: 1175084142.0351\n",
      "Epoch 1973/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 41694608.5945 - val_loss: 1144187643.5088\n",
      "Epoch 1974/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 51335677.9033 - val_loss: 1210724161.1228\n",
      "Epoch 1975/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 47375534.0915 - val_loss: 1166749534.3158\n",
      "Epoch 1976/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 0s 74us/step - loss: 39833621.6447 - val_loss: 1167594007.5789\n",
      "Epoch 1977/2000\n",
      "1137/1137 [==============================] - 0s 79us/step - loss: 54024969.1117 - val_loss: 1208264582.7368\n",
      "Epoch 1978/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 40464731.1328 - val_loss: 1179070202.3860\n",
      "Epoch 1979/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 43619609.5022 - val_loss: 1215580341.8947\n",
      "Epoch 1980/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 50089381.6974 - val_loss: 1184080249.2632\n",
      "Epoch 1981/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 41733009.0519 - val_loss: 1144654874.9474\n",
      "Epoch 1982/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 41716881.0836 - val_loss: 1154907298.8070\n",
      "Epoch 1983/2000\n",
      "1137/1137 [==============================] - 0s 68us/step - loss: 46302024.0686 - val_loss: 1182319150.0351\n",
      "Epoch 1984/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 39710816.8426 - val_loss: 1165021361.4035\n",
      "Epoch 1985/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 37541305.1574 - val_loss: 1180106210.8070\n",
      "Epoch 1986/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 38960309.7432 - val_loss: 1228003452.6316\n",
      "Epoch 1987/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 34934238.3325 - val_loss: 1191462075.5088\n",
      "Epoch 1988/2000\n",
      "1137/1137 [==============================] - 0s 98us/step - loss: 35058253.7133 - val_loss: 1161639897.8246\n",
      "Epoch 1989/2000\n",
      "1137/1137 [==============================] - 0s 84us/step - loss: 66991230.0792 - val_loss: 1193408757.8947\n",
      "Epoch 1990/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 49792352.0774 - val_loss: 1156037958.7368\n",
      "Epoch 1991/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 37284916.7740 - val_loss: 1167557711.7193\n",
      "Epoch 1992/2000\n",
      "1137/1137 [==============================] - 0s 74us/step - loss: 34824621.6464 - val_loss: 1158628120.7018\n",
      "Epoch 1993/2000\n",
      "1137/1137 [==============================] - 0s 71us/step - loss: 75239396.7740 - val_loss: 1187155959.0175\n",
      "Epoch 1994/2000\n",
      "1137/1137 [==============================] - 0s 75us/step - loss: 61381021.4846 - val_loss: 1229929875.0877\n",
      "Epoch 1995/2000\n",
      "1137/1137 [==============================] - 0s 69us/step - loss: 77277205.3650 - val_loss: 1147567548.6316\n",
      "Epoch 1996/2000\n",
      "1137/1137 [==============================] - 0s 77us/step - loss: 58480568.9393 - val_loss: 1180175502.5965\n",
      "Epoch 1997/2000\n",
      "1137/1137 [==============================] - 0s 73us/step - loss: 42958052.7159 - val_loss: 1190469227.7895\n",
      "Epoch 1998/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 44909837.9455 - val_loss: 1196735638.4561\n",
      "Epoch 1999/2000\n",
      "1137/1137 [==============================] - 0s 72us/step - loss: 37633440.4468 - val_loss: 1191689037.4737\n",
      "Epoch 2000/2000\n",
      "1137/1137 [==============================] - 0s 76us/step - loss: 64586521.9173 - val_loss: 1253972875.2281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5b7115c8>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.values,y_train.values,validation_split=0.20,batch_size=50,nb_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[227172.66]]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([856\t,854,\t0,\t3,\t706.0,\t0.0,\t1.0,\t0.0,\t150.0,\t0,\t0,\t2,\t548.0,\t2.0,\t1710,\t1,\t1,\t8450,\t65.0,\t0,\t60,\t196.0,\t0,\t2,\t61,\t5,\t7,\t0,\t0,\t8,\t856.0,\t0,\t2003,\t2003,\t2008,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t0,\t0,\t0,\t1,\t1,\t0,\t0,\t0,\t0,\t1,\t0])\n",
    "print(model.predict(test_data.reshape(1,174), batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       208500.0\n",
       "1       181500.0\n",
       "2       223500.0\n",
       "3       140000.0\n",
       "4       250000.0\n",
       "          ...   \n",
       "1454         NaN\n",
       "1455         NaN\n",
       "1456         NaN\n",
       "1457         NaN\n",
       "1458         NaN\n",
       "Name: SalePrice, Length: 2881, dtype: float64"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_pred=model.predict(df_Test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120948.26],\n",
       "       [142404.1 ],\n",
       "       [211874.83],\n",
       "       ...,\n",
       "       [202058.58],\n",
       "       [133930.08],\n",
       "       [208777.02]], dtype=float32)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(ann_pred)\n",
    "sub_df=pd.read_csv('sample_submission5.csv')\n",
    "datasets=pd.concat([sub_df['Id'],pred],axis=1)\n",
    "datasets.columns=['Id','SalePrice']\n",
    "datasets.to_csv('sample_submission5.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
